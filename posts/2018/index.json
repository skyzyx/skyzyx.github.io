{
    "data": {
        
        "section": "posts",
        
        
        "count": 7,
        "items": [
            
            {
    "kind": "page",
    "title": "Understanding Trust in Your Infrastructure",
    "description": "Trust should be earned, not given blindly.",
    "summary": {
        "content": "Only a tiny fraction of the code your application runs was written by you or your team. How do you know you can trust the code that was written by other people? Where would you even start? This piece is part of a larger series on Engineering for Site Reliability, specifically balancing stability against the edge of technology. What do I mean by \u0026ldquo;trust\u0026rdquo;? Movies and TV shows have given us a version of trust which essentially boils down to “Do you trust me?",
        "isTruncated": true
    },
    "published": "2018-12-27T07:30:30Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/understanding-trust-in-your-infrastructure/",
    "relativePermalink": "/posts/2018/understanding-trust-in-your-infrastructure/",
    "aliases": ["/2018/12/27/understanding-trust-in-your-infrastructure"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/trust-dial@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/broken-collarbone@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/teen-titans@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/wrong-way@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/etc-passwd@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/batman-slap-docker-dev@2x.jpg", "https://cdn.ryanparman.com/hugo/headers/understanding-trust/severed-wrists@2x.jpg"],
    "videos": [],
    "categories": ["Engineering for Site Reliability"],
    "tags": ["trust", "docker", "alpine linux", "centos", "ubuntu", "rhel", "php", "python", "nodejs", "golang", "nginx", "composer", "pip", "npm", "java", "cve", "security", "site reliability engineering"],
    "series": ["Engineering for Site Reliability"],
    "keywords": [],
    "meta": {
        "wordCount": 1608,
        "readingTime": "8 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20181227-understanding-trust-in-your-infrastructure.md",
        "logicalName": "20181227-understanding-trust-in-your-infrastructure.md",
        "translationBaseName": "20181227-understanding-trust-in-your-infrastructure",
        "baseFileName": "20181227-understanding-trust-in-your-infrastructure",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#what-do-i-mean-by-trust\"\u003eWhat do I mean by \u0026ldquo;trust\u0026rdquo;?\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#what-is-my-application\"\u003eWhat is my application?\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#reusable-layers-and-understanding-trust\"\u003eReusable layers, and understanding trust\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#an-unpublished-package-broke-the-internet\"\u003eAn unpublished package broke the internet\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#crashing-the-entire-stack\"\u003eCrashing the entire stack\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#devprod-parity\"\u003eDev/Prod parity\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#guidelines-for-trust\"\u003eGuidelines for trust\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nOnly a tiny fraction of the code your application runs was written by you or your team. How do you know you can trust the code that was written by other people? Where would you even start?\n\u003c/p\u003e\n\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/trust-dial@2x.webp\" alt=\"Trust dial\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/trust-dial@2x.jpg\" alt=\"Trust dial\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003eThis piece is part of a larger series on \u003ca href=\"/series/engineering-for-site-reliability/\"\u003eEngineering for Site Reliability\u003c/a\u003e, specifically \u003cem\u003ebalancing stability against the edge of technology\u003c/em\u003e.\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003ch2 id=\"what-do-i-mean-by-trust\"\u003eWhat do I mean by \u0026ldquo;trust\u0026rdquo;?\u003c/h2\u003e\n\u003cp\u003eMovies and TV shows have given us a version of trust which essentially boils down to “Do you trust me?” as they hold out their hand to another person. In the movies things generally work out in the end, even if they run into a little more trouble along the way. This is the kind of trust that teenagers, newly in-love, have with their new person.\u003c/p\u003e\n\u003cp\u003eThis is also the kind of trust that most engineers have in their software dependencies. \u003cstrong\u003eThis is not what trust is\u003c/strong\u003e, and is a \u003cstrong\u003ehigh-risk\u003c/strong\u003e way to build applications.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/teen-titans@2x.webp\" alt=\"Teen Titans\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/teen-titans@2x.jpg\" alt=\"Teen Titans\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eIf you\u0026rsquo;ve ever been spurned by an ex-lover, or have grown-up around shady people, you\u0026rsquo;ll likely have a different definition of trust. A marriage counselor may say something like \u0026ldquo;trust, but verify\u0026rdquo;. A person who has grown-up in a bad neighborhood or around shady people may have the perspective that \u003cem\u003etrust is earned, not given\u003c/em\u003e. \u003cstrong\u003eA certain amount of paranoia is a good thing\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, as with everything, you can also have too much paranoia. These are the teams who ship an application, and if it\u0026rsquo;s not broken, they don\u0026rsquo;t touch it. Their curse is that they fall so far behind the security and maintenance curves, that their applications become ticking time bombs — defeating the very purpose they think their paranoia addresses.\u003c/p\u003e\n\u003cp\u003eThe point that I\u0026rsquo;d like you to take away from this is that \u003cem\u003etrust is earned, not given\u003c/em\u003e. When you come from this perspective, you make better technical decisions.\u003c/p\u003e\n\u003ch2 id=\"what-is-my-application\"\u003eWhat is my application?\u003c/h2\u003e\n\u003cp\u003eDepending on the type of engineer you are (front-end, backend, ops), you may look at the applications you work on through different lenses.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSome see the client-side, browser code they\u0026rsquo;re writing.\u003c/li\u003e\n\u003cli\u003eSome see the Golang, Node.js, Python, or PHP code they\u0026rsquo;re writing.\u003c/li\u003e\n\u003cli\u003eSome see the package dependencies, and their package dependencies, and so on…\u003c/li\u003e\n\u003cli\u003eSome see code like the Docker runtime, OpenSSL, cURL, or the Linux kernel.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn truth, \u003cem\u003eall of these answers are correct\u003c/em\u003e. The best engineers know how important it is to look at the entire stack — from the application, to the runtime, to the hypervisor, to the kernel.\u003c/p\u003e\n\u003ch2 id=\"reusable-layers-and-understanding-trust\"\u003eReusable layers, and understanding trust\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s a common (and extremely sensible) pattern to re-use and build atop existing technology layers. By leveraging this powerful foundation, we can build bigger, better, and more powerful appications and services! But we also need to understand how core concepts like \u003cem\u003etrust\u003c/em\u003e work between all of these layers.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/wrong-way@2x.webp\" alt=\"Wrong way sign\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/wrong-way@2x.jpg\" alt=\"Wrong way sign\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eLet me give a few examples of anti-patterns that are also very commonplace in many organizations (mostly due to ignorance, as opposed to malice):\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e I\u0026rsquo;m speaking from a context of applications which run on popular cloud infrastructure services like AWS, GCP, or Azure, and have sane processes in place like actively-supported system images (e.g., AMIs).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eFetching application dependencies \u003cem\u003elive\u003c/em\u003e from upstream sources (e.g., the internet is ephemeral; is your app?).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRunning package manager updates when spinning-up a new machine (e.g., modifying the underlying system image at boot-time; \u003ccode\u003eyum -y update\u003c/code\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRunning package manager updates when deploying to Production (e.g., picking up potentially untested software without a testing stage in-between).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAdding new package manager repositories from random places on the internet (e.g., taking candy from strangers).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRelying exclusively on a single \u003cem\u003eavailability zone\u003c/em\u003e or \u003cem\u003eregion\u003c/em\u003e from their cloud infrastructure provider.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e“These aren’t anti-patterns,” you say. “They’re just how development is done.”\u003c/p\u003e\n\u003cp\u003eThank you for your thoughts, hypothetical reader. But consider the following:\u003c/p\u003e\n\u003ch3 id=\"an-unpublished-package-broke-the-internet\"\u003eAn unpublished package broke the internet\u003c/h3\u003e\n\u003cp\u003eIn case you forgot, in early 2016, \u003cem\u003eone package\u003c/em\u003e \u003ca href=\"https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm\"\u003ebroke the entire Node.js ecosystem\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/broken-collarbone@2x.webp\" alt=\"Broken collarbone\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/broken-collarbone@2x.jpg\" alt=\"Broken collarbone\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eDavid Haney writes in his piece “\u003ca href=\"https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/\"\u003eNPM \u0026amp; left-pad: Have We Forgotten How To Program?\u003c/a\u003e”:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOkay developers, time to have a serious talk. As you are probably already aware, this week React, Babel, and a bunch of other high-profile packages on NPM broke. The reason they broke is rather astounding:\u003c/p\u003e\n\u003cp\u003eA simple NPM package called \u003cem\u003eleft-pad\u003c/em\u003e that was a dependency of their code.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eleft-pad\u003c/em\u003e, at the time of writing this, has 11 stars on GitHub. The entire package is 11 simple lines that implement a basic left-pad string function. […]\u003c/p\u003e\n\u003cp\u003eWhat concerns me here is that \u003cem\u003eso many packages and projects\u003c/em\u003e took on a \u003cstrong\u003edependency\u003c/strong\u003e for a simple left padding string function, rather than their developers taking 2 minutes to write such a basic function themselves.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eEach and every application team which was hit by this issue, and allowed it to impact a Production-facing deployment, \u003cstrong\u003efailed to understand trust\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn this case, they should have implemented a \u003cem\u003epackage caching system\u003c/em\u003e, which can fetch a dependency on the first request, then cache that version for all subsequent requests. That way, if there is an issue with an upstream source, you will not be impacted.\u003c/p\u003e\n\u003ch3 id=\"crashing-the-entire-stack\"\u003eCrashing the entire stack\u003c/h3\u003e\n\u003cp\u003eI was working at Amazon Web Services back in 2010 when \u003ca href=\"https://aws.amazon.com/elasticbeanstalk/\"\u003eAWS Elastic Beanstalk\u003c/a\u003e was still in development. The team was working to build an easy-to-use solution around the idea of \u0026ldquo;application containers\u0026rdquo; (back before Docker was spun-out from \u003ca href=\"https://www.crunchbase.com/organization/dotcloud\"\u003edotCloud\u003c/a\u003e, an early PaaS provider). At the time I was helping them add PHP + Apache support to Elastic Beanstalk in time for launch, as I was the \u003cem\u003ede-facto\u003c/em\u003e “PHP guy” in AWS.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/etc-passwd@2x.webp\" alt=\"/etc/passwd\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/etc-passwd@2x.jpg\" alt=\"/etc/passwd\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eDevelopment was running on a pre-release version of what would become \u003ca href=\"https://aws.amazon.com/amazon-linux-ami/\"\u003eAmazon Linux\u003c/a\u003e. The original configuration was designed to run \u003ccode\u003eyum -y update\u003c/code\u003e on boot, which essentially means \u003cem\u003epick up the latest versions of all installed packages\u003c/em\u003e. While the team was thinking about system security (and avoiding outdated packages), everything broke on the day that the Amazon Linux team published a new version of Apache with backwards-incompatible changes. The development team \u003cstrong\u003efailed to understand trust\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eFortunately, it was a little before the public launch, and so only a few internal beta customers and developers were impacted. But watching that incident was the day that I learned that you don\u0026rsquo;t \u003cem\u003earbitrarily\u003c/em\u003e install all system updates. You should do that in your development environment instead, work out the issues, then roll something out to Production that has been tested and works as expected.\u003c/p\u003e\n\u003ch3 id=\"devprod-parity\"\u003eDev/Prod parity\u003c/h3\u003e\n\u003cp\u003eIf you\u0026rsquo;ve never heard of the \u003ca href=\"https://www.12factor.net\"\u003e12-factor app methodology\u003c/a\u003e, you are absolutely missing out. One of the chapters is entitled “\u003ca href=\"https://www.12factor.net/dev-prod-parity\"\u003eDev/prod parity\u003c/a\u003e”, which essentially boils down to \u003cem\u003ekeeping development, staging, and production as similar as possible\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eOne thing that I\u0026rsquo;ve seen bite a team is that they were deploying an application by pushing the source code from Git to the production instances, then resolving their packages \u003cem\u003edirectly on the instance\u003c/em\u003e. (To be fair, this was back in the days when \u003ca href=\"https://capistranorb.com\"\u003eCapistrano\u003c/a\u003e was hot, and we\u0026rsquo;ve come a long way since then.)\u003c/p\u003e\n\u003cp\u003eBut even in the world of Docker and \u003cem\u003econtinuous integration\u003c/em\u003e, I still see similar things happen. A team will build a Docker image for their dev app in their CI pipeline, push it to their Docker repository, then deploy it to dev. Then they build the image again when deploying to staging. Then again when they deploy to Prod. \u003cstrong\u003eThis is the same problem!\u003c/strong\u003e The dependencies are not being tested appropriately in the earlier environments before progressing to the production environment.\u003c/p\u003e\n\u003cp\u003eWith Docker, some teams have figured out how to make the exact same mistakes even faster! Those teams have \u003cstrong\u003efailed to understand trust\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eInstead, you should build the production-ready Docker image \u003cem\u003eonce\u003c/em\u003e, then promote that same image up to each environment as the requisite confidence is built.\u003c/p\u003e\n\u003cp\u003e“But how do I include my development dependencies inside my Docker container?”\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/batman-slap-docker-dev@2x.webp\" alt=\"Batman slaps Robin\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/batman-slap-docker-dev@2x.jpg\" alt=\"Batman slaps Robin\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eDocker images that are built should be the exact same bytes, regardless of the environment. Your dev build should write out logs in the same way as your Production app would (although perhaps to a local location). You should be able to define things like environment variables that are read by the Docker daemon at container launch. Or by defining a local volume to mount containing configuration information. But the insides of the Docker image should always be completely identical between environments.\u003c/p\u003e\n\u003ch2 id=\"guidelines-for-trust\"\u003eGuidelines for trust\u003c/h2\u003e\n\u003cp\u003eWhen you\u0026rsquo;re provisioning software onto a machine that will run in Production, you don\u0026rsquo;t want to be running software from \u003cem\u003eanywhere\u003c/em\u003e. You need to know that you can \u003cem\u003etrust\u003c/em\u003e the source of the software before you ship it into Production.\u003c/p\u003e\n\u003cp\u003eIn my case, I tend to work on teams which run servers with a blend of RedHat Enterprise Linux (RHEL), CentOS, and Amazon Linux. Containers are commonly Ubuntu, Debian, or Alpine. I work with applications written in nearly every major programming language. These are my criteria for determining whether or not to trust a package or Docker image.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePackages are maintained by CentOS, RedHat, Amazon, Ubuntu, Debian, Alpine, etc., directly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePackages are maintained by the vendor of the software directly (e.g., Docker, Amazon, PHP, Node Foundation, Angular, Kubernetes).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePackages are maintained by a reputable third-party source (as few of these as possible; e.g., \u003ca href=\"https://github.com/nodesource/distributions\"\u003eNodeSource\u003c/a\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePackages are maintained by us. That is, we compile them from source ourselves (into \u003ccode\u003e.rpm\u003c/code\u003e, \u003ccode\u003e.deb\u003c/code\u003e, or \u003ccode\u003e.apk\u003c/code\u003e packages), or we write the software packages ourselves (e.g., \u003ccode\u003ecomposer\u003c/code\u003e, \u003ccode\u003epip\u003c/code\u003e, \u003ccode\u003enpm\u003c/code\u003e, \u003ccode\u003edep\u003c/code\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYour criteria may look different, and that\u0026rsquo;s OK. Some engineering teams are better at this, while others are still maturing.\u003c/p\u003e\n\u003cp\u003eIf you don\u0026rsquo;t have criteria, and generally just install software from anywhere, I have two pieces of advice.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eStop it.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOur criteria has been very good to us. Feel free to borrow ours.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
        "plain":"Only a tiny fraction of the code your application runs was written by you or your team. How do you know you can trust the code that was written by other people? Where would you even start?   This piece is part of a larger series on Engineering for Site Reliability, specifically balancing stability against the edge of technology.\n What do I mean by \u0026ldquo;trust\u0026rdquo;? Movies and TV shows have given us a version of trust which essentially boils down to “Do you trust me?” as they hold out their hand to another person. In the movies things generally work out in the end, even if they run into a little more trouble along the way. This is the kind of trust that teenagers, newly in-love, have with their new person.\nThis is also the kind of trust that most engineers have in their software dependencies. This is not what trust is, and is a high-risk way to build applications.\n  If you\u0026rsquo;ve ever been spurned by an ex-lover, or have grown-up around shady people, you\u0026rsquo;ll likely have a different definition of trust. A marriage counselor may say something like \u0026ldquo;trust, but verify\u0026rdquo;. A person who has grown-up in a bad neighborhood or around shady people may have the perspective that trust is earned, not given. A certain amount of paranoia is a good thing.\nHowever, as with everything, you can also have too much paranoia. These are the teams who ship an application, and if it\u0026rsquo;s not broken, they don\u0026rsquo;t touch it. Their curse is that they fall so far behind the security and maintenance curves, that their applications become ticking time bombs — defeating the very purpose they think their paranoia addresses.\nThe point that I\u0026rsquo;d like you to take away from this is that trust is earned, not given. When you come from this perspective, you make better technical decisions.\nWhat is my application? Depending on the type of engineer you are (front-end, backend, ops), you may look at the applications you work on through different lenses.\n Some see the client-side, browser code they\u0026rsquo;re writing. Some see the Golang, Node.js, Python, or PHP code they\u0026rsquo;re writing. Some see the package dependencies, and their package dependencies, and so on… Some see code like the Docker runtime, OpenSSL, cURL, or the Linux kernel.  In truth, all of these answers are correct. The best engineers know how important it is to look at the entire stack — from the application, to the runtime, to the hypervisor, to the kernel.\nReusable layers, and understanding trust It\u0026rsquo;s a common (and extremely sensible) pattern to re-use and build atop existing technology layers. By leveraging this powerful foundation, we can build bigger, better, and more powerful appications and services! But we also need to understand how core concepts like trust work between all of these layers.\n  Let me give a few examples of anti-patterns that are also very commonplace in many organizations (mostly due to ignorance, as opposed to malice):\n NOTE: I\u0026rsquo;m speaking from a context of applications which run on popular cloud infrastructure services like AWS, GCP, or Azure, and have sane processes in place like actively-supported system images (e.g., AMIs).\n   Fetching application dependencies live from upstream sources (e.g., the internet is ephemeral; is your app?).\n  Running package manager updates when spinning-up a new machine (e.g., modifying the underlying system image at boot-time; yum -y update).\n  Running package manager updates when deploying to Production (e.g., picking up potentially untested software without a testing stage in-between).\n  Adding new package manager repositories from random places on the internet (e.g., taking candy from strangers).\n  Relying exclusively on a single availability zone or region from their cloud infrastructure provider.\n  “These aren’t anti-patterns,” you say. “They’re just how development is done.”\nThank you for your thoughts, hypothetical reader. But consider the following:\nAn unpublished package broke the internet In case you forgot, in early 2016, one package broke the entire Node.js ecosystem.\n  David Haney writes in his piece “NPM \u0026amp; left-pad: Have We Forgotten How To Program?”:\n Okay developers, time to have a serious talk. As you are probably already aware, this week React, Babel, and a bunch of other high-profile packages on NPM broke. The reason they broke is rather astounding:\nA simple NPM package called left-pad that was a dependency of their code.\nleft-pad, at the time of writing this, has 11 stars on GitHub. The entire package is 11 simple lines that implement a basic left-pad string function. […]\nWhat concerns me here is that so many packages and projects took on a dependency for a simple left padding string function, rather than their developers taking 2 minutes to write such a basic function themselves.\n Each and every application team which was hit by this issue, and allowed it to impact a Production-facing deployment, failed to understand trust.\nIn this case, they should have implemented a package caching system, which can fetch a dependency on the first request, then cache that version for all subsequent requests. That way, if there is an issue with an upstream source, you will not be impacted.\nCrashing the entire stack I was working at Amazon Web Services back in 2010 when AWS Elastic Beanstalk was still in development. The team was working to build an easy-to-use solution around the idea of \u0026ldquo;application containers\u0026rdquo; (back before Docker was spun-out from dotCloud, an early PaaS provider). At the time I was helping them add PHP + Apache support to Elastic Beanstalk in time for launch, as I was the de-facto “PHP guy” in AWS.\n  Development was running on a pre-release version of what would become Amazon Linux. The original configuration was designed to run yum -y update on boot, which essentially means pick up the latest versions of all installed packages. While the team was thinking about system security (and avoiding outdated packages), everything broke on the day that the Amazon Linux team published a new version of Apache with backwards-incompatible changes. The development team failed to understand trust.\nFortunately, it was a little before the public launch, and so only a few internal beta customers and developers were impacted. But watching that incident was the day that I learned that you don\u0026rsquo;t arbitrarily install all system updates. You should do that in your development environment instead, work out the issues, then roll something out to Production that has been tested and works as expected.\nDev/Prod parity If you\u0026rsquo;ve never heard of the 12-factor app methodology, you are absolutely missing out. One of the chapters is entitled “Dev/prod parity”, which essentially boils down to keeping development, staging, and production as similar as possible.\nOne thing that I\u0026rsquo;ve seen bite a team is that they were deploying an application by pushing the source code from Git to the production instances, then resolving their packages directly on the instance. (To be fair, this was back in the days when Capistrano was hot, and we\u0026rsquo;ve come a long way since then.)\nBut even in the world of Docker and continuous integration, I still see similar things happen. A team will build a Docker image for their dev app in their CI pipeline, push it to their Docker repository, then deploy it to dev. Then they build the image again when deploying to staging. Then again when they deploy to Prod. This is the same problem! The dependencies are not being tested appropriately in the earlier environments before progressing to the production environment.\nWith Docker, some teams have figured out how to make the exact same mistakes even faster! Those teams have failed to understand trust.\nInstead, you should build the production-ready Docker image once, then promote that same image up to each environment as the requisite confidence is built.\n“But how do I include my development dependencies inside my Docker container?”\n  Docker images that are built should be the exact same bytes, regardless of the environment. Your dev build should write out logs in the same way as your Production app would (although perhaps to a local location). You should be able to define things like environment variables that are read by the Docker daemon at container launch. Or by defining a local volume to mount containing configuration information. But the insides of the Docker image should always be completely identical between environments.\nGuidelines for trust When you\u0026rsquo;re provisioning software onto a machine that will run in Production, you don\u0026rsquo;t want to be running software from anywhere. You need to know that you can trust the source of the software before you ship it into Production.\nIn my case, I tend to work on teams which run servers with a blend of RedHat Enterprise Linux (RHEL), CentOS, and Amazon Linux. Containers are commonly Ubuntu, Debian, or Alpine. I work with applications written in nearly every major programming language. These are my criteria for determining whether or not to trust a package or Docker image.\n  Packages are maintained by CentOS, RedHat, Amazon, Ubuntu, Debian, Alpine, etc., directly.\n  Packages are maintained by the vendor of the software directly (e.g., Docker, Amazon, PHP, Node Foundation, Angular, Kubernetes).\n  Packages are maintained by a reputable third-party source (as few of these as possible; e.g., NodeSource).\n  Packages are maintained by us. That is, we compile them from source ourselves (into .rpm, .deb, or .apk packages), or we write the software packages ourselves (e.g., composer, pip, npm, dep).\n  Your criteria may look different, and that\u0026rsquo;s OK. Some engineering teams are better at this, while others are still maturing.\nIf you don\u0026rsquo;t have criteria, and generally just install software from anywhere, I have two pieces of advice.\n  Stop it.\n  Our criteria has been very good to us. Feel free to borrow ours.\n  ",
        "source":"\n{{\u003cdescription\u003e}}\nOnly a tiny fraction of the code your application runs was written by you or your team. How do you know you can trust the code that was written by other people? Where would you even start?\n{{\u003c/description\u003e}}\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/trust-dial@2x.jpg\" alt=\"Trust dial\"\u003e}}\n\n{{% aside %}}\nThis piece is part of a larger series on [Engineering for Site Reliability](/series/engineering-for-site-reliability/), specifically _balancing stability against the edge of technology_.\n{{% /aside %}}\n\n## What do I mean by \"trust\"?\n\nMovies and TV shows have given us a version of trust which essentially boils down to “Do you trust me?” as they hold out their hand to another person. In the movies things generally work out in the end, even if they run into a little more trouble along the way. This is the kind of trust that teenagers, newly in-love, have with their new person.\n\nThis is also the kind of trust that most engineers have in their software dependencies. **This is not what trust is**, and is a **high-risk** way to build applications.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/teen-titans@2x.jpg\" alt=\"Teen Titans\" \u003e}}\n\nIf you've ever been spurned by an ex-lover, or have grown-up around shady people, you'll likely have a different definition of trust. A marriage counselor may say something like \"trust, but verify\". A person who has grown-up in a bad neighborhood or around shady people may have the perspective that _trust is earned, not given_. **A certain amount of paranoia is a good thing**.\n\nHowever, as with everything, you can also have too much paranoia. These are the teams who ship an application, and if it's not broken, they don't touch it. Their curse is that they fall so far behind the security and maintenance curves, that their applications become ticking time bombs — defeating the very purpose they think their paranoia addresses.\n\nThe point that I'd like you to take away from this is that _trust is earned, not given_. When you come from this perspective, you make better technical decisions.\n\n## What is my application?\n\nDepending on the type of engineer you are (front-end, backend, ops), you may look at the applications you work on through different lenses.\n\n* Some see the client-side, browser code they're writing.\n* Some see the Golang, Node.js, Python, or PHP code they're writing. \n* Some see the package dependencies, and their package dependencies, and so on…\n* Some see code like the Docker runtime, OpenSSL, cURL, or the Linux kernel.\n\nIn truth, _all of these answers are correct_. The best engineers know how important it is to look at the entire stack — from the application, to the runtime, to the hypervisor, to the kernel.\n\n## Reusable layers, and understanding trust\n\nIt's a common (and extremely sensible) pattern to re-use and build atop existing technology layers. By leveraging this powerful foundation, we can build bigger, better, and more powerful appications and services! But we also need to understand how core concepts like _trust_ work between all of these layers.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/wrong-way@2x.jpg\" alt=\"Wrong way sign\"\u003e}}\n\nLet me give a few examples of anti-patterns that are also very commonplace in many organizations (mostly due to ignorance, as opposed to malice):\n\n\u003e **NOTE:** I'm speaking from a context of applications which run on popular cloud infrastructure services like AWS, GCP, or Azure, and have sane processes in place like actively-supported system images (e.g., AMIs).\n\n* Fetching application dependencies _live_ from upstream sources (e.g., the internet is ephemeral; is your app?).\n\n* Running package manager updates when spinning-up a new machine (e.g., modifying the underlying system image at boot-time; `yum -y update`).\n\n* Running package manager updates when deploying to Production (e.g., picking up potentially untested software without a testing stage in-between).\n\n* Adding new package manager repositories from random places on the internet (e.g., taking candy from strangers).\n\n* Relying exclusively on a single _availability zone_ or _region_ from their cloud infrastructure provider.\n\n“These aren’t anti-patterns,” you say. “They’re just how development is done.”\n\nThank you for your thoughts, hypothetical reader. But consider the following:\n\n### An unpublished package broke the internet\n\nIn case you forgot, in early 2016, _one package_ [broke the entire Node.js ecosystem]({{% wayback \"https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm\" %}}).\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/broken-collarbone@2x.jpg\" alt=\"Broken collarbone\"\u003e}}\n\nDavid Haney writes in his piece “[NPM \u0026 left-pad: Have We Forgotten How To Program?]({{% wayback \"https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/\" %}})”:\n\n\u003e Okay developers, time to have a serious talk. As you are probably already aware, this week React, Babel, and a bunch of other high-profile packages on NPM broke. The reason they broke is rather astounding:\n\u003e\n\u003e A simple NPM package called _left-pad_ that was a dependency of their code.\n\u003e\n\u003e _left-pad_, at the time of writing this, has 11 stars on GitHub. The entire package is 11 simple lines that implement a basic left-pad string function. […]\n\u003e\n\u003e What concerns me here is that _so many packages and projects_ took on a **dependency** for a simple left padding string function, rather than their developers taking 2 minutes to write such a basic function themselves.\n\nEach and every application team which was hit by this issue, and allowed it to impact a Production-facing deployment, **failed to understand trust**.\n\nIn this case, they should have implemented a _package caching system_, which can fetch a dependency on the first request, then cache that version for all subsequent requests. That way, if there is an issue with an upstream source, you will not be impacted.\n\n### Crashing the entire stack\n\nI was working at Amazon Web Services back in 2010 when [AWS Elastic Beanstalk]({{% wayback \"https://aws.amazon.com/elasticbeanstalk/\" %}}) was still in development. The team was working to build an easy-to-use solution around the idea of \"application containers\" (back before Docker was spun-out from [dotCloud]({{% wayback \"https://www.crunchbase.com/organization/dotcloud\" %}}), an early PaaS provider). At the time I was helping them add PHP + Apache support to Elastic Beanstalk in time for launch, as I was the _de-facto_ “PHP guy” in AWS.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/etc-passwd@2x.jpg\" alt=\"/etc/passwd\"\u003e}}\n\nDevelopment was running on a pre-release version of what would become [Amazon Linux]({{% wayback \"https://aws.amazon.com/amazon-linux-ami/\" %}}). The original configuration was designed to run `yum -y update` on boot, which essentially means _pick up the latest versions of all installed packages_. While the team was thinking about system security (and avoiding outdated packages), everything broke on the day that the Amazon Linux team published a new version of Apache with backwards-incompatible changes. The development team **failed to understand trust**.\n\nFortunately, it was a little before the public launch, and so only a few internal beta customers and developers were impacted. But watching that incident was the day that I learned that you don't _arbitrarily_ install all system updates. You should do that in your development environment instead, work out the issues, then roll something out to Production that has been tested and works as expected.\n\n### Dev/Prod parity\n\nIf you've never heard of the [12-factor app methodology]({{% wayback \"https://www.12factor.net\" %}}), you are absolutely missing out. One of the chapters is entitled “[Dev/prod parity]({{% wayback \"https://www.12factor.net/dev-prod-parity\" %}})”, which essentially boils down to _keeping development, staging, and production as similar as possible_.\n\nOne thing that I've seen bite a team is that they were deploying an application by pushing the source code from Git to the production instances, then resolving their packages _directly on the instance_. (To be fair, this was back in the days when [Capistrano]({{% wayback \"https://capistranorb.com\" %}}) was hot, and we've come a long way since then.)\n\nBut even in the world of Docker and _continuous integration_, I still see similar things happen. A team will build a Docker image for their dev app in their CI pipeline, push it to their Docker repository, then deploy it to dev. Then they build the image again when deploying to staging. Then again when they deploy to Prod. **This is the same problem!** The dependencies are not being tested appropriately in the earlier environments before progressing to the production environment.\n\nWith Docker, some teams have figured out how to make the exact same mistakes even faster! Those teams have **failed to understand trust**.\n\nInstead, you should build the production-ready Docker image _once_, then promote that same image up to each environment as the requisite confidence is built.\n\n“But how do I include my development dependencies inside my Docker container?”\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/batman-slap-docker-dev@2x.jpg\" alt=\"Batman slaps Robin\"\u003e}}\n\nDocker images that are built should be the exact same bytes, regardless of the environment. Your dev build should write out logs in the same way as your Production app would (although perhaps to a local location). You should be able to define things like environment variables that are read by the Docker daemon at container launch. Or by defining a local volume to mount containing configuration information. But the insides of the Docker image should always be completely identical between environments.\n\n## Guidelines for trust\n\nWhen you're provisioning software onto a machine that will run in Production, you don't want to be running software from _anywhere_. You need to know that you can _trust_ the source of the software before you ship it into Production.\n\nIn my case, I tend to work on teams which run servers with a blend of RedHat Enterprise Linux (RHEL), CentOS, and Amazon Linux. Containers are commonly Ubuntu, Debian, or Alpine. I work with applications written in nearly every major programming language. These are my criteria for determining whether or not to trust a package or Docker image.\n\n1. Packages are maintained by CentOS, RedHat, Amazon, Ubuntu, Debian, Alpine, etc., directly.\n\n1. Packages are maintained by the vendor of the software directly (e.g., Docker, Amazon, PHP, Node Foundation, Angular, Kubernetes).\n\n1. Packages are maintained by a reputable third-party source (as few of these as possible; e.g., [NodeSource](https://github.com/nodesource/distributions)).\n\n1. Packages are maintained by us. That is, we compile them from source ourselves (into `.rpm`, `.deb`, or `.apk` packages), or we write the software packages ourselves (e.g., `composer`, `pip`, `npm`, `dep`).\n\nYour criteria may look different, and that's OK. Some engineering teams are better at this, while others are still maturing.\n\nIf you don't have criteria, and generally just install software from anywhere, I have two pieces of advice.\n\n1. Stop it.\n\n1. Our criteria has been very good to us. Feel free to borrow ours.\n"},
    "links": {
        "prev": {"title": "The Hiring Process, Part I", "permalink": "https://ryanparman.com/posts/2018/the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered/"},
        "next": {"title": "Playlist: Best of P.O.D.", "permalink": "https://ryanparman.com/posts/2019/playlist-best-of-pod/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "The Hiring Process, Part I",
    "description": "What I Look For in a CV/Résumé (Remastered)",
    "summary": {
        "content": "Over my career, my job title has typically fallen into the baskets of: Front-End Web Developer, Software Engineer, or DevOps/SRE. I’ve done a lot of interviewing to try and find the right people to join the teams I’ve worked on, and I thought it might be helpful to share part of my process. NOTE: I want to start by stating that what is written here are my own thoughts, and not representative of any past or current employer. Remastered? This is an updated and revised edition of my original piece from 2011, “The Hiring Process, Part I: What I Look For in a CV/Résumé”.",
        "isTruncated": true
    },
    "published": "2018-11-14T23:26:24Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered/",
    "relativePermalink": "/posts/2018/the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered/",
    "aliases": ["/2018/11/14/the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/stewie-work@2x.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/remastered@2x.jpg"],
    "videos": ["https://cdn.ryanparman.com/hugo/posts/2018/great-skills.mp4"],
    "categories": ["Tech Industry"],
    "tags": ["engineer", "hiring", "resume", "interview", "email", "github", "devops", "sre", "skills", "napoleon dynamite", "spelling", "remastered"],
    "series": [],
    "keywords": [],
    "meta": {
        "wordCount": 2347,
        "readingTime": "12 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20181114-the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered.md",
        "logicalName": "20181114-the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered.md",
        "translationBaseName": "20181114-the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered",
        "baseFileName": "20181114-the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#remastered\"\u003eRemastered?\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#over-arching-ideology\"\u003eOver-arching Ideology\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#it-begins-with-a-stack-of-résumés\"\u003eIt begins with a stack of résumés…\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#your-name\"\u003eYour Name\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#address-and-phone-number-vs-your-blog-twitter-and-github\"\u003eAddress and Phone Number vs. Your Blog, Twitter and GitHub\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#objective\"\u003eObjective\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#elevator-pitch\"\u003eElevator Pitch\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#skills-and-software\"\u003eSkills and Software\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#summary-of-qualifications\"\u003eSummary of Qualifications\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#work-experience\"\u003eWork Experience\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#references-and-recommendations\"\u003eReferences and Recommendations\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#other-tips\"\u003eOther Tips\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nOver my career, my job title has typically fallen into the baskets of: Front-End Web Developer, Software Engineer, or DevOps/SRE. I’ve done a lot of interviewing to try and find the right people to join the teams I’ve worked on, and I thought it might be helpful to share part of my process.\n\u003c/p\u003e\n\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003e\u003cb\u003eNOTE:\u003c/b\u003e I want to start by stating that what is written here are my own thoughts, and not representative of any past or current employer.\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003ch2 id=\"remastered\"\u003eRemastered?\u003c/h2\u003e\n\u003cp\u003eThis is an updated and revised edition of my original piece from 2011, “\u003ca href=\"/posts/2011/the-hiring-process-part-i-what-i-look-for-in-a-cvresume/\"\u003eThe Hiring Process, Part I: What I Look For in a CV/Résumé\u003c/a\u003e”.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/remastered@2x.webp\" alt=\"Remastered edition\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/remastered@2x.jpg\" alt=\"Remastered edition\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"over-arching-ideology\"\u003eOver-arching Ideology\u003c/h2\u003e\n\u003cp\u003eThere\u0026rsquo;s a dynamite quote from the book \u003ca href=\"https://medium.theuxblog.com/the-laws-of-simplicity-ed6fa92c7bc6\"\u003eThe Laws of Simplicity\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSimplicity is about subtracting the obvious, and adding the meaningful.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI first read this book in 2006, and it has changed my perspective on so many things. But if there\u0026rsquo;s one single idea you take from this piece, it should be to \u003cstrong\u003esubstract the obvious and add the meaningful\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"it-begins-with-a-stack-of-résumés\"\u003eIt begins with a stack of résumés…\u003c/h2\u003e\n\u003cp\u003eTypically the process starts with a recruiter sending over a stack of résumés. There\u0026rsquo;s always some amount of time it takes to teach the recruiters how to identify what I\u0026rsquo;m looking for. My job is to make a determination as to whether or not this person is anything like what we\u0026rsquo;re looking to add to the team.\u003c/p\u003e\n\u003cp\u003eNow, I’m going to start by saying that my approach to résumés is greatly influenced by \u0026ldquo;\u003ca href=\"https://web.archive.org/web/20181114232624/http://www.randsinrepose.com/archives/2007/02/25/a_glimpse_and_a_hook.html\"\u003eA Glimpse and a Hook\u003c/a\u003e\u0026rdquo; and the follow-up, \u0026ldquo;\u003ca href=\"https://web.archive.org/web/20181114232624/http://www.randsinrepose.com/archives/2007/12/06/a_brief_glimpse.html\"\u003eA Brief Glimpse\u003c/a\u003e\u0026rdquo;. Like the author, I generally update my résumé every 6–9 months whether I’m looking for a new gig or not.\u003c/p\u003e\n\u003ch2 id=\"your-name\"\u003eYour Name\u003c/h2\u003e\n\u003cp\u003eThere was a time where if you’ve done something notable on the HTML/CSS/JavaScript/PHP front, I’ve probably heard of you. However, in the intervening years since I transitioned from a Front-End Engineer → Application Engineer → Site Reliability Engineer, I\u0026rsquo;ve gotten busier, my kids have gotten older, and I quite frankly have other things to think about.\u003c/p\u003e\n\u003cp\u003eSo, I \u003cem\u003emay\u003c/em\u003e have heard of you, but don\u0026rsquo;t be offended or think I\u0026rsquo;m out-of-the-know if I haven\u0026rsquo;t.\u003c/p\u003e\n\u003ch2 id=\"address-and-phone-number-vs-your-blog-twitter-and-github\"\u003eAddress and Phone Number vs. Your Blog, Twitter and GitHub\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eAddress:\u003c/strong\u003e I don’t need to know where you live. I don’t care. I’m not going to mail you a letter.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEmail:\u003c/strong\u003e An email address is good \u003cem\u003eat the very least\u003c/em\u003e. Providing a contact phone number will probably save us a step, but isn’t strictly required.\u003c/p\u003e\n\u003cp\u003eHowever, I am keenly interested in where you live \u003cem\u003eon the Internet\u003c/em\u003e. I want to see your personal website, your blog, your GitHub/GitLab/BitBucket account, your Twitter account. I like to know what you think about (Twitter), write about (Blog), and the kinds of problems you like to solve (GitHub).\u003c/p\u003e\n\u003cp\u003eOpen Source or Free Software work is definitely a strong signal in your favor. Contributions to existing projects, or the open-sourcing of your own work — in non-trivial projects — is a very valuable skill to have. It shows me that you can play nicely inside an existing codebase. The fork-edit-PR workflow is an important part of how my team works, and having a background in that will make you more effective.\u003c/p\u003e\n\u003ch2 id=\"objective\"\u003eObjective\u003c/h2\u003e\n\u003cp\u003eIn my experience, this is near-universally worthless. It doesn’t tell me anything about you, and it’s usually either a mish-mash of bogus keywords or something so mind-numbingly obvious that you don’t need to waste the words on a page to tell me.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“To secure a position with a well established organization with a stable environment that will lead to a lasting relationship in the field of technology.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWell, no shit.\u003c/p\u003e\n\u003ch2 id=\"elevator-pitch\"\u003eElevator Pitch\u003c/h2\u003e\n\u003cp\u003eYour \u003ca href=\"https://web.archive.org/web/20181114232624/http://en.wikipedia.org/wiki/Elevator_pitch\"\u003eelevator pitch\u003c/a\u003e should summarize your value and experience in the length of a paragraph.\u003c/p\u003e\n\u003cp\u003eAssume that this is the \u003cstrong\u003eonly part\u003c/strong\u003e of your résumé I’m going to read. What’s going to get my attention? I see a zillion résumés; what’s going to make me spend time with yours? Should it tell me every company you’ve worked for? No. You are not your job. Should it tell me about the kinds of problems you like to solve and the things you’re passionate about? Absolutely.\u003c/p\u003e\n\u003cp\u003eIn all seriousness, the one résumé I’ve come across that has stuck out in my mind the most is \u003ca href=\"https://web.archive.org/web/20181114232624/http://noahstokes.com/hot-as-the-sun.php\"\u003eNoah Stokes\u003c/a\u003e’ résumé. Now I’m certainly not recommending you make yours look like this, but think about the memorability factor. As a matter of fact, as I was writing this, I couldn’t remember Noah’s name. But I sure as heck remembered “ajax the shit out of your site”. A quick search, and his site came up and I remembered his name. Be that guy.\u003c/p\u003e\n\u003cp\u003eSome people have adoped the \u003cstrong\u003e1-pager\u003c/strong\u003e style of résumé. I first came across this concept through my colleague \u003ca href=\"https://www.linkedin.com/in/cdybenko/\"\u003eChristen Dybenko\u003c/a\u003e, who in-turn got the idea from a post by \u003ca href=\"https://web.archive.org/web/20181114232624/http://davidseah.com/blog/2008/07/powerpoint-resume-layout-tips/\"\u003eDavid Seah\u003c/a\u003e. This idea has also been made popular by sites like \u003ca href=\"https://web.archive.org/web/20181114232624/http://cvparade.com\"\u003eCV Parade\u003c/a\u003e. It’s separate from your résumé, and is essentially treated as a list of highlights about you, your career and what you’re looking for.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s great interviewers with a short attention span, but that isn\u0026rsquo;t me. I\u0026rsquo;m looking to understand what you \u003cem\u003eactually know\u003c/em\u003e. If you include it, that\u0026rsquo;s cool, but it\u0026rsquo;s not a replacement for a real résumé.\u003c/p\u003e\n\u003ch2 id=\"skills-and-software\"\u003eSkills and Software\u003c/h2\u003e\n\u003cp\u003eSkills are more important than software.\u003c/p\u003e\n\u003cp\u003eLet me say that again: \u003cstrong\u003eSkills are more important than software.\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"video/mp4\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/great-skills2.mp4\" alt=\"Girls only want boyfriends that have great skills!\" class=\"db fullimage\" decoding=\"async\"\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/great-skills2.webp\" alt=\"Girls only want boyfriends that have great skills!\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/great-skills2.gif\" alt=\"Girls only want boyfriends that have great skills!\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eI care more about you \u003cem\u003ebeing able to do something\u003c/em\u003e than I do about the tools you use to get there.\u003c/p\u003e\n\u003cp\u003eBut even more than that, I want to know the difference between what you’re good at and what you’re not so good at. I’ve seen a lot of résumés that do little more than list out every single programming language they’ve ever heard of.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Java, JavaScript, C#, ASP.net, PHP, Perl, Python, Ruby, Erlang and Haskell.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cem\u003eBullshit\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eI recommend breaking your skills down into groups. The names of the groups don’t matter as much as how they’re classified. On a scale of one to ten, here’s how I group things:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e1–3:\u003c/strong\u003e You have limited experience with these things — so I shouldn’t start firing a bunch of in-depth questions at you about them — but you’re interested in growing your skills in them over time, or they\u0026rsquo;re things that you\u0026rsquo;ve dabbled in and would like to dabble more.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e4–6:\u003c/strong\u003e You’ve got a solid handle on these things, can answer questions about them, and would have no problem working with them on a regular basis.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e7–9:\u003c/strong\u003e You have advanced abilities in these skills, teach other people how to do them, and can answer obscure or complex questions about them in an interview setting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e10:\u003c/strong\u003e You are an expert in your field. Not just in your company, but in your field. People on the internet know your name, and you get invited to join working groups or hold advisory positions about these topics. You could write (or have written) a book about the topic.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMost people should have their skills grouped into the first three groups. I personally give these groups the following names: Highly Proficient (7–9), Proficient (4–6), and Limited Experience Worth Mentioning (1–3). I’ve only interviewed an expert in their field once or twice in my career, so I’m certainly not expecting that out of you. But if I ask you how well you know a topic, and you say “10”, I’m going to drill you like a subject-matter expert.\u003c/p\u003e\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003eThis is a personal pet peeve of mine: Don’t list Microsoft Office apps on your résumé. If your résumé is so bare that you have to list Microsoft Office apps, Adobe Acrobat or your preferred web browser, you’ve got bigger problems.\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003ch2 id=\"summary-of-qualifications\"\u003eSummary of Qualifications\u003c/h2\u003e\n\u003cp\u003eI think \u003cem\u003eRands\u003c/em\u003e says it best in his aforementioned blog post:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e[…] Here’s a good example from an imaginary résumé: “Proven success in leading technical problem solving situations”. This line tells me nothing. Yes, I know you’re trying to tell me that you’re strategic, but there is no way you’re going to convince me that you’re strategic in a résumé. I’m going to learn that from a phone screen and from an interview.\u003c/p\u003e\n\u003cp\u003e[…] When you write “Established track record for delivering measurable results under tight schedules”, I am going to ask you what the hell you mean on the phone and if your answer isn’t instant and insightful, I’ll know your qualifications are designed to be buzzword compliant and don’t actually define your qualifications.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMost of the time, this section doesn’t tell me anything because it tends to be packed with buzz-words or overzealous/unbelievable claims. I don’t care about stuff that doesn’t tell me anything. If I can’t discern your qualifications from your Elevator Pitch or your skill set, then you’re already in trouble.\u003c/p\u003e\n\u003cp\u003eInstead, I’d like to get a sense of what it would be like to work with you — warts and all. This is important because I believe that it’s important to know who you are and who you aren’t. If you feed me a line like \u0026ldquo;I work \u003cem\u003etoo\u003c/em\u003e hard\u0026rdquo; or \u0026ldquo;My work is \u003cem\u003etoo\u003c/em\u003e perfect\u0026rdquo;, I’m going to move onto the next résumé. Why? Because you just told me that you’re inefficient and you don’t know how to prioritize effectively.\u003c/p\u003e\n\u003cp\u003eGive me a sense of what it’s like to work with you. What kinds of things can I expect?\u003c/p\u003e\n\u003ch2 id=\"work-experience\"\u003eWork Experience\u003c/h2\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/stewie-work@2x.webp\" alt=\"Stewie at work\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/stewie-work@2x.jpg\" alt=\"Stewie at work\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eThis is the meat and potatoes of your résumé. For most people this is just a bulleted list of things they did at the job. Again, I don’t care about that. There are a few specific things that I look at in this area:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eI want to see what you \u003cem\u003eaccomplished\u003c/em\u003e (or where your \u003cem\u003efocus\u003c/em\u003e was), what you \u003cem\u003elearned\u003c/em\u003e, and how you’ve \u003cem\u003egrown\u003c/em\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow long were you at the job? For a full-time gig, I’d expect to see at least a year. Anything shorter than that is a red flag.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the same time, if I see that you’ve been at the same company for ten years, that’s also a red flag. How could you possibly grow in your career if you’ve had the same job for a decade? (Yes, I know it’s possible, but there’s going to be a burden of proof on you.)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHave you done any open-source work? Yes, that should go here too. This is work experience. Just because you may or may not have received a paycheck for it doesn’t mean it wasn’t work. Having good experience here would likely offset any red flags triggered by the previous points.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you list an accomplishment like “raised revenues by 10%”, my very first question in the interview is going to be “how?” Be prepared to back up anything you claim on your résumé.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAre you passionate about what you do, and does that passion exude from your résumé (or blog, or Twitter, or open-source work)?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, I understand that some jobs just plain suck. I’ve had a couple of those myself. How did you make the best of them, and what were your take-aways?\u003c/p\u003e\n\u003ch2 id=\"references-and-recommendations\"\u003eReferences and Recommendations\u003c/h2\u003e\n\u003cp\u003eIn white-collar jobs, references have gone the way of the dodo bird.\u003c/p\u003e\n\u003cp\u003eThe thing that services like LinkedIn have made more prevalent are recommendations. Without having to pick up a telephone and ask questions, what do your co-workers, managers, teammates, and other folks have to say about you? A link to your LinkedIn profile (or something similar) will do in a pinch.\u003c/p\u003e\n\u003cp\u003eNow, I understand that the whole purpose of a LinkedIn recommendation is to show that person in the best possible light. In that way, a lot of recommendations are junk. Knowing that, I’m less concerned about positive or negative spin and more concerned with \u003cem\u003ewhat\u003c/em\u003e they write about you? What language do they use? How well does it seem like they know you, and what kinds of things do they bring up about you? Do I see a recurring theme throughout all of your recommendations? How well do they mesh with how you’ve described yourself?\u003c/p\u003e\n\u003ch2 id=\"other-tips\"\u003eOther Tips\u003c/h2\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/spelling@2x.webp\" alt=\"Spelling errors\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/spelling@2x.jpg\" alt=\"Spelling errors\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eHere are a few other things to think about:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHave some skin in the game:\u003c/strong\u003e I’ve gotten jobs before because people knew of my work. They knew \u003cem\u003emy reputation\u003c/em\u003e before they knew \u003cem\u003eme\u003c/em\u003e. A huge bonus is if you’re already doing the thing that I’m trying to hire for.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSound like a human being:\u003c/strong\u003e If you say “planned, designed, and coordinated engineers efforts for the development of a mission critical system”, you sound like — no, just no. Only write stuff that will actually tell me something about you. Tell me about your process. Tell me about the issues you had to address. What kind of issues did you face?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSpelling and Grammar:\u003c/strong\u003e If you misspell wrods (see what I did there?) or have generally poor grammar, I’m going to move on. You\u0026rsquo;re an adult, presumably you graduated from high school, and you should be able to speak and write effectively. This especially applies to the following areas:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eTheir\u003c/em\u003e, \u003cem\u003ethere\u003c/em\u003e, and \u003cem\u003ethey’re\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThen\u003c/em\u003e and \u003cem\u003ethan\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLose\u003c/em\u003e and \u003cem\u003eloose\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eExcept\u003c/em\u003e and \u003cem\u003eaccept\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eEffect\u003c/em\u003e and \u003cem\u003eaffect\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eAnd brand/product names: “I-phone” (instead of \u003cem\u003eiPhone\u003c/em\u003e) and \u0026ldquo;MAC\u0026rdquo; (instead of \u003cem\u003eMac\u003c/em\u003e, or \u003cem\u003eMacintosh\u003c/em\u003e, or \u003cem\u003emacOS\u003c/em\u003e) is very, very wrong.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you\u0026rsquo;re not going to take the time to get your writing right (especially on a résumé), what else are you not going to take the time to do?\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eNow, I know that I don’t speak for everybody. Some people take a very different approach to reviewing résumés, and that’s fine. It’s also possible that I’m not the kind of person that you’d like to work with/for, and that’s fine too.\u003c/p\u003e\n\u003cp\u003eI know that facets of my approach are used by people that I consider some of the best in the industry, and as such I’ve tried to learn from the best ideas I’ve come across to make the most informed decisions possible when determining whether or not to schedule the next step (i.e., the phone interview).\u003c/p\u003e\n\u003cp\u003eIf you’re involved in creating things for the web, take heed to my suggestions. Even if you don’t follow all of them, definitely make it a point to raise the quality of your résumé.\u003c/p\u003e\n",
        "plain":"Over my career, my job title has typically fallen into the baskets of: Front-End Web Developer, Software Engineer, or DevOps/SRE. I’ve done a lot of interviewing to try and find the right people to join the teams I’ve worked on, and I thought it might be helpful to share part of my process. NOTE: I want to start by stating that what is written here are my own thoughts, and not representative of any past or current employer.\n Remastered? This is an updated and revised edition of my original piece from 2011, “The Hiring Process, Part I: What I Look For in a CV/Résumé”.\n  Over-arching Ideology There\u0026rsquo;s a dynamite quote from the book The Laws of Simplicity:\n Simplicity is about subtracting the obvious, and adding the meaningful.\n I first read this book in 2006, and it has changed my perspective on so many things. But if there\u0026rsquo;s one single idea you take from this piece, it should be to substract the obvious and add the meaningful.\nIt begins with a stack of résumés… Typically the process starts with a recruiter sending over a stack of résumés. There\u0026rsquo;s always some amount of time it takes to teach the recruiters how to identify what I\u0026rsquo;m looking for. My job is to make a determination as to whether or not this person is anything like what we\u0026rsquo;re looking to add to the team.\nNow, I’m going to start by saying that my approach to résumés is greatly influenced by \u0026ldquo;A Glimpse and a Hook\u0026rdquo; and the follow-up, \u0026ldquo;A Brief Glimpse\u0026rdquo;. Like the author, I generally update my résumé every 6–9 months whether I’m looking for a new gig or not.\nYour Name There was a time where if you’ve done something notable on the HTML/CSS/JavaScript/PHP front, I’ve probably heard of you. However, in the intervening years since I transitioned from a Front-End Engineer → Application Engineer → Site Reliability Engineer, I\u0026rsquo;ve gotten busier, my kids have gotten older, and I quite frankly have other things to think about.\nSo, I may have heard of you, but don\u0026rsquo;t be offended or think I\u0026rsquo;m out-of-the-know if I haven\u0026rsquo;t.\nAddress and Phone Number vs. Your Blog, Twitter and GitHub Address: I don’t need to know where you live. I don’t care. I’m not going to mail you a letter.\nEmail: An email address is good at the very least. Providing a contact phone number will probably save us a step, but isn’t strictly required.\nHowever, I am keenly interested in where you live on the Internet. I want to see your personal website, your blog, your GitHub/GitLab/BitBucket account, your Twitter account. I like to know what you think about (Twitter), write about (Blog), and the kinds of problems you like to solve (GitHub).\nOpen Source or Free Software work is definitely a strong signal in your favor. Contributions to existing projects, or the open-sourcing of your own work — in non-trivial projects — is a very valuable skill to have. It shows me that you can play nicely inside an existing codebase. The fork-edit-PR workflow is an important part of how my team works, and having a background in that will make you more effective.\nObjective In my experience, this is near-universally worthless. It doesn’t tell me anything about you, and it’s usually either a mish-mash of bogus keywords or something so mind-numbingly obvious that you don’t need to waste the words on a page to tell me.\n “To secure a position with a well established organization with a stable environment that will lead to a lasting relationship in the field of technology.”\n Well, no shit.\nElevator Pitch Your elevator pitch should summarize your value and experience in the length of a paragraph.\nAssume that this is the only part of your résumé I’m going to read. What’s going to get my attention? I see a zillion résumés; what’s going to make me spend time with yours? Should it tell me every company you’ve worked for? No. You are not your job. Should it tell me about the kinds of problems you like to solve and the things you’re passionate about? Absolutely.\nIn all seriousness, the one résumé I’ve come across that has stuck out in my mind the most is Noah Stokes’ résumé. Now I’m certainly not recommending you make yours look like this, but think about the memorability factor. As a matter of fact, as I was writing this, I couldn’t remember Noah’s name. But I sure as heck remembered “ajax the shit out of your site”. A quick search, and his site came up and I remembered his name. Be that guy.\nSome people have adoped the 1-pager style of résumé. I first came across this concept through my colleague Christen Dybenko, who in-turn got the idea from a post by David Seah. This idea has also been made popular by sites like CV Parade. It’s separate from your résumé, and is essentially treated as a list of highlights about you, your career and what you’re looking for.\nIt\u0026rsquo;s great interviewers with a short attention span, but that isn\u0026rsquo;t me. I\u0026rsquo;m looking to understand what you actually know. If you include it, that\u0026rsquo;s cool, but it\u0026rsquo;s not a replacement for a real résumé.\nSkills and Software Skills are more important than software.\nLet me say that again: Skills are more important than software.\n  I care more about you being able to do something than I do about the tools you use to get there.\nBut even more than that, I want to know the difference between what you’re good at and what you’re not so good at. I’ve seen a lot of résumés that do little more than list out every single programming language they’ve ever heard of.\n “Java, JavaScript, C#, ASP.net, PHP, Perl, Python, Ruby, Erlang and Haskell.”\n Bullshit.\nI recommend breaking your skills down into groups. The names of the groups don’t matter as much as how they’re classified. On a scale of one to ten, here’s how I group things:\n  1–3: You have limited experience with these things — so I shouldn’t start firing a bunch of in-depth questions at you about them — but you’re interested in growing your skills in them over time, or they\u0026rsquo;re things that you\u0026rsquo;ve dabbled in and would like to dabble more.\n  4–6: You’ve got a solid handle on these things, can answer questions about them, and would have no problem working with them on a regular basis.\n  7–9: You have advanced abilities in these skills, teach other people how to do them, and can answer obscure or complex questions about them in an interview setting.\n  10: You are an expert in your field. Not just in your company, but in your field. People on the internet know your name, and you get invited to join working groups or hold advisory positions about these topics. You could write (or have written) a book about the topic.\n  Most people should have their skills grouped into the first three groups. I personally give these groups the following names: Highly Proficient (7–9), Proficient (4–6), and Limited Experience Worth Mentioning (1–3). I’ve only interviewed an expert in their field once or twice in my career, so I’m certainly not expecting that out of you. But if I ask you how well you know a topic, and you say “10”, I’m going to drill you like a subject-matter expert.\nThis is a personal pet peeve of mine: Don’t list Microsoft Office apps on your résumé. If your résumé is so bare that you have to list Microsoft Office apps, Adobe Acrobat or your preferred web browser, you’ve got bigger problems.\n Summary of Qualifications I think Rands says it best in his aforementioned blog post:\n […] Here’s a good example from an imaginary résumé: “Proven success in leading technical problem solving situations”. This line tells me nothing. Yes, I know you’re trying to tell me that you’re strategic, but there is no way you’re going to convince me that you’re strategic in a résumé. I’m going to learn that from a phone screen and from an interview.\n[…] When you write “Established track record for delivering measurable results under tight schedules”, I am going to ask you what the hell you mean on the phone and if your answer isn’t instant and insightful, I’ll know your qualifications are designed to be buzzword compliant and don’t actually define your qualifications.\n Most of the time, this section doesn’t tell me anything because it tends to be packed with buzz-words or overzealous/unbelievable claims. I don’t care about stuff that doesn’t tell me anything. If I can’t discern your qualifications from your Elevator Pitch or your skill set, then you’re already in trouble.\nInstead, I’d like to get a sense of what it would be like to work with you — warts and all. This is important because I believe that it’s important to know who you are and who you aren’t. If you feed me a line like \u0026ldquo;I work too hard\u0026rdquo; or \u0026ldquo;My work is too perfect\u0026rdquo;, I’m going to move onto the next résumé. Why? Because you just told me that you’re inefficient and you don’t know how to prioritize effectively.\nGive me a sense of what it’s like to work with you. What kinds of things can I expect?\nWork Experience   This is the meat and potatoes of your résumé. For most people this is just a bulleted list of things they did at the job. Again, I don’t care about that. There are a few specific things that I look at in this area:\n  I want to see what you accomplished (or where your focus was), what you learned, and how you’ve grown.\n  How long were you at the job? For a full-time gig, I’d expect to see at least a year. Anything shorter than that is a red flag.\n  At the same time, if I see that you’ve been at the same company for ten years, that’s also a red flag. How could you possibly grow in your career if you’ve had the same job for a decade? (Yes, I know it’s possible, but there’s going to be a burden of proof on you.)\n  Have you done any open-source work? Yes, that should go here too. This is work experience. Just because you may or may not have received a paycheck for it doesn’t mean it wasn’t work. Having good experience here would likely offset any red flags triggered by the previous points.\n  If you list an accomplishment like “raised revenues by 10%”, my very first question in the interview is going to be “how?” Be prepared to back up anything you claim on your résumé.\n  Are you passionate about what you do, and does that passion exude from your résumé (or blog, or Twitter, or open-source work)?\n  Now, I understand that some jobs just plain suck. I’ve had a couple of those myself. How did you make the best of them, and what were your take-aways?\nReferences and Recommendations In white-collar jobs, references have gone the way of the dodo bird.\nThe thing that services like LinkedIn have made more prevalent are recommendations. Without having to pick up a telephone and ask questions, what do your co-workers, managers, teammates, and other folks have to say about you? A link to your LinkedIn profile (or something similar) will do in a pinch.\nNow, I understand that the whole purpose of a LinkedIn recommendation is to show that person in the best possible light. In that way, a lot of recommendations are junk. Knowing that, I’m less concerned about positive or negative spin and more concerned with what they write about you? What language do they use? How well does it seem like they know you, and what kinds of things do they bring up about you? Do I see a recurring theme throughout all of your recommendations? How well do they mesh with how you’ve described yourself?\nOther Tips   Here are a few other things to think about:\nHave some skin in the game: I’ve gotten jobs before because people knew of my work. They knew my reputation before they knew me. A huge bonus is if you’re already doing the thing that I’m trying to hire for.\nSound like a human being: If you say “planned, designed, and coordinated engineers efforts for the development of a mission critical system”, you sound like — no, just no. Only write stuff that will actually tell me something about you. Tell me about your process. Tell me about the issues you had to address. What kind of issues did you face?\nSpelling and Grammar: If you misspell wrods (see what I did there?) or have generally poor grammar, I’m going to move on. You\u0026rsquo;re an adult, presumably you graduated from high school, and you should be able to speak and write effectively. This especially applies to the following areas:\n Their, there, and they’re Then and than Lose and loose Except and accept Effect and affect And brand/product names: “I-phone” (instead of iPhone) and \u0026ldquo;MAC\u0026rdquo; (instead of Mac, or Macintosh, or macOS) is very, very wrong.  If you\u0026rsquo;re not going to take the time to get your writing right (especially on a résumé), what else are you not going to take the time to do?\nConclusion Now, I know that I don’t speak for everybody. Some people take a very different approach to reviewing résumés, and that’s fine. It’s also possible that I’m not the kind of person that you’d like to work with/for, and that’s fine too.\nI know that facets of my approach are used by people that I consider some of the best in the industry, and as such I’ve tried to learn from the best ideas I’ve come across to make the most informed decisions possible when determining whether or not to schedule the next step (i.e., the phone interview).\nIf you’re involved in creating things for the web, take heed to my suggestions. Even if you don’t follow all of them, definitely make it a point to raise the quality of your résumé.\n",
        "source":"\n{{\u003cdescription\u003e}}\nOver my career, my job title has typically fallen into the baskets of: Front-End Web Developer, Software Engineer, or DevOps/SRE. I’ve done a lot of interviewing to try and find the right people to join the teams I’ve worked on, and I thought it might be helpful to share part of my process.\n{{\u003c/description\u003e}}\n\n{{% aside %}}\n\u003cp\u003e\u003cb\u003eNOTE:\u003c/b\u003e I want to start by stating that what is written here are my own thoughts, and not representative of any past or current employer.\u003c/p\u003e\n{{% /aside %}}\n\n## Remastered?\n\nThis is an updated and revised edition of my original piece from 2011, “[The Hiring Process, Part I: What I Look For in a CV/Résumé](/posts/2011/the-hiring-process-part-i-what-i-look-for-in-a-cvresume/)”.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/remastered@2x.jpg\" alt=\"Remastered edition\" \u003e}}\n\n## Over-arching Ideology\n\nThere's a dynamite quote from the book [The Laws of Simplicity](https://medium.theuxblog.com/the-laws-of-simplicity-ed6fa92c7bc6):\n\n\u003e Simplicity is about subtracting the obvious, and adding the meaningful.\n\nI first read this book in 2006, and it has changed my perspective on so many things. But if there's one single idea you take from this piece, it should be to **substract the obvious and add the meaningful**.\n\n## It begins with a stack of résumés…\n\nTypically the process starts with a recruiter sending over a stack of résumés. There's always some amount of time it takes to teach the recruiters how to identify what I'm looking for. My job is to make a determination as to whether or not this person is anything like what we're looking to add to the team.\n\nNow, I’m going to start by saying that my approach to résumés is greatly influenced by \"[A Glimpse and a Hook]({{% wayback \"http://www.randsinrepose.com/archives/2007/02/25/a_glimpse_and_a_hook.html\" %}})\" and the follow-up, \"[A Brief Glimpse]({{% wayback \"http://www.randsinrepose.com/archives/2007/12/06/a_brief_glimpse.html\" %}})\". Like the author, I generally update my résumé every 6–9 months whether I’m looking for a new gig or not.\n\n## Your Name\n\nThere was a time where if you’ve done something notable on the HTML/CSS/JavaScript/PHP front, I’ve probably heard of you. However, in the intervening years since I transitioned from a Front-End Engineer → Application Engineer → Site Reliability Engineer, I've gotten busier, my kids have gotten older, and I quite frankly have other things to think about.\n\nSo, I _may_ have heard of you, but don't be offended or think I'm out-of-the-know if I haven't.\n\n## Address and Phone Number vs. Your Blog, Twitter and GitHub\n\n**Address:** I don’t need to know where you live. I don’t care. I’m not going to mail you a letter.\n\n**Email:** An email address is good _at the very least_. Providing a contact phone number will probably save us a step, but isn’t strictly required.\n\nHowever, I am keenly interested in where you live _on the Internet_. I want to see your personal website, your blog, your GitHub/GitLab/BitBucket account, your Twitter account. I like to know what you think about (Twitter), write about (Blog), and the kinds of problems you like to solve (GitHub).\n\nOpen Source or Free Software work is definitely a strong signal in your favor. Contributions to existing projects, or the open-sourcing of your own work — in non-trivial projects — is a very valuable skill to have. It shows me that you can play nicely inside an existing codebase. The fork-edit-PR workflow is an important part of how my team works, and having a background in that will make you more effective.\n\n## Objective\n\nIn my experience, this is near-universally worthless. It doesn’t tell me anything about you, and it’s usually either a mish-mash of bogus keywords or something so mind-numbingly obvious that you don’t need to waste the words on a page to tell me.\n\n\u003e “To secure a position with a well established organization with a stable environment that will lead to a lasting relationship in the field of technology.”\n\nWell, no shit.\n\n## Elevator Pitch\n\nYour [elevator pitch]({{% wayback \"http://en.wikipedia.org/wiki/Elevator_pitch\" %}}) should summarize your value and experience in the length of a paragraph.\n\nAssume that this is the **only part** of your résumé I’m going to read. What’s going to get my attention? I see a zillion résumés; what’s going to make me spend time with yours? Should it tell me every company you’ve worked for? No. You are not your job. Should it tell me about the kinds of problems you like to solve and the things you’re passionate about? Absolutely.\n\nIn all seriousness, the one résumé I’ve come across that has stuck out in my mind the most is [Noah Stokes]({{% wayback \"http://noahstokes.com/hot-as-the-sun.php\" %}})’ résumé. Now I’m certainly not recommending you make yours look like this, but think about the memorability factor. As a matter of fact, as I was writing this, I couldn’t remember Noah’s name. But I sure as heck remembered “ajax the shit out of your site”. A quick search, and his site came up and I remembered his name. Be that guy.\n\nSome people have adoped the **1-pager** style of résumé. I first came across this concept through my colleague [Christen Dybenko](https://www.linkedin.com/in/cdybenko/), who in-turn got the idea from a post by [David Seah]({{% wayback \"http://davidseah.com/blog/2008/07/powerpoint-resume-layout-tips/\" %}}). This idea has also been made popular by sites like [CV Parade]({{% wayback \"http://cvparade.com\" %}}). It’s separate from your résumé, and is essentially treated as a list of highlights about you, your career and what you’re looking for.\n\nIt's great interviewers with a short attention span, but that isn't me. I'm looking to understand what you _actually know_. If you include it, that's cool, but it's not a replacement for a real résumé.\n\n## Skills and Software\n\nSkills are more important than software.\n\nLet me say that again: **Skills are more important than software.**\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/great-skills2.gif\" mp4=true alt=\"Girls only want boyfriends that have great skills!\"\u003e}}\n\nI care more about you _being able to do something_ than I do about the tools you use to get there.\n\nBut even more than that, I want to know the difference between what you’re good at and what you’re not so good at. I’ve seen a lot of résumés that do little more than list out every single programming language they’ve ever heard of.\n\n\u003e “Java, JavaScript, C\\#, ASP.net, PHP, Perl, Python, Ruby, Erlang and Haskell.”\n\n_Bullshit_.\n\nI recommend breaking your skills down into groups. The names of the groups don’t matter as much as how they’re classified. On a scale of one to ten, here’s how I group things:\n\n* **1–3:** You have limited experience with these things — so I shouldn’t start firing a bunch of in-depth questions at you about them — but you’re interested in growing your skills in them over time, or they're things that you've dabbled in and would like to dabble more.\n\n* **4–6:** You’ve got a solid handle on these things, can answer questions about them, and would have no problem working with them on a regular basis.\n\n* **7–9:** You have advanced abilities in these skills, teach other people how to do them, and can answer obscure or complex questions about them in an interview setting.\n\n* **10:** You are an expert in your field. Not just in your company, but in your field. People on the internet know your name, and you get invited to join working groups or hold advisory positions about these topics. You could write (or have written) a book about the topic.\n\nMost people should have their skills grouped into the first three groups. I personally give these groups the following names: Highly Proficient (7–9), Proficient (4–6), and Limited Experience Worth Mentioning (1–3). I’ve only interviewed an expert in their field once or twice in my career, so I’m certainly not expecting that out of you. But if I ask you how well you know a topic, and you say “10”, I’m going to drill you like a subject-matter expert.\n\n{{% aside %}}\n\u003cp\u003eThis is a personal pet peeve of mine: Don’t list Microsoft Office apps on your résumé. If your résumé is so bare that you have to list Microsoft Office apps, Adobe Acrobat or your preferred web browser, you’ve got bigger problems.\u003c/p\u003e\n{{% /aside %}}\n\n## Summary of Qualifications\n\nI think _Rands_ says it best in his aforementioned blog post:\n\n\u003e […] Here’s a good example from an imaginary résumé: “Proven success in leading technical problem solving situations”. This line tells me nothing. Yes, I know you’re trying to tell me that you’re strategic, but there is no way you’re going to convince me that you’re strategic in a résumé. I’m going to learn that from a phone screen and from an interview.\n\u003e\n\u003e […] When you write “Established track record for delivering measurable results under tight schedules”, I am going to ask you what the hell you mean on the phone and if your answer isn’t instant and insightful, I’ll know your qualifications are designed to be buzzword compliant and don’t actually define your qualifications.\n\nMost of the time, this section doesn’t tell me anything because it tends to be packed with buzz-words or overzealous/unbelievable claims. I don’t care about stuff that doesn’t tell me anything. If I can’t discern your qualifications from your Elevator Pitch or your skill set, then you’re already in trouble.\n\nInstead, I’d like to get a sense of what it would be like to work with you — warts and all. This is important because I believe that it’s important to know who you are and who you aren’t. If you feed me a line like \"I work _too_ hard\" or \"My work is _too_ perfect\", I’m going to move onto the next résumé. Why? Because you just told me that you’re inefficient and you don’t know how to prioritize effectively.\n\nGive me a sense of what it’s like to work with you. What kinds of things can I expect?\n\n## Work Experience\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/stewie-work@2x.jpg\" alt=\"Stewie at work\" \u003e}}\n\nThis is the meat and potatoes of your résumé. For most people this is just a bulleted list of things they did at the job. Again, I don’t care about that. There are a few specific things that I look at in this area:\n\n* I want to see what you _accomplished_ (or where your _focus_ was), what you _learned_, and how you’ve _grown_.\n\n* How long were you at the job? For a full-time gig, I’d expect to see at least a year. Anything shorter than that is a red flag.\n\n* At the same time, if I see that you’ve been at the same company for ten years, that’s also a red flag. How could you possibly grow in your career if you’ve had the same job for a decade? (Yes, I know it’s possible, but there’s going to be a burden of proof on you.)\n\n* Have you done any open-source work? Yes, that should go here too. This is work experience. Just because you may or may not have received a paycheck for it doesn’t mean it wasn’t work. Having good experience here would likely offset any red flags triggered by the previous points.\n\n* If you list an accomplishment like “raised revenues by 10%”, my very first question in the interview is going to be “how?” Be prepared to back up anything you claim on your résumé.\n\n* Are you passionate about what you do, and does that passion exude from your résumé (or blog, or Twitter, or open-source work)?\n\nNow, I understand that some jobs just plain suck. I’ve had a couple of those myself. How did you make the best of them, and what were your take-aways?\n\n## References and Recommendations\n\nIn white-collar jobs, references have gone the way of the dodo bird.\n\nThe thing that services like LinkedIn have made more prevalent are recommendations. Without having to pick up a telephone and ask questions, what do your co-workers, managers, teammates, and other folks have to say about you? A link to your LinkedIn profile (or something similar) will do in a pinch.\n\nNow, I understand that the whole purpose of a LinkedIn recommendation is to show that person in the best possible light. In that way, a lot of recommendations are junk. Knowing that, I’m less concerned about positive or negative spin and more concerned with _what_ they write about you? What language do they use? How well does it seem like they know you, and what kinds of things do they bring up about you? Do I see a recurring theme throughout all of your recommendations? How well do they mesh with how you’ve described yourself?\n\n## Other Tips\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/spelling@2x.jpg\" alt=\"Spelling errors\" \u003e}}\n\nHere are a few other things to think about:\n\n**Have some skin in the game:** I’ve gotten jobs before because people knew of my work. They knew _my reputation_ before they knew _me_. A huge bonus is if you’re already doing the thing that I’m trying to hire for.\n\n**Sound like a human being:** If you say “planned, designed, and coordinated engineers efforts for the development of a mission critical system”, you sound like — no, just no. Only write stuff that will actually tell me something about you. Tell me about your process. Tell me about the issues you had to address. What kind of issues did you face?\n\n**Spelling and Grammar:** If you misspell wrods (see what I did there?) or have generally poor grammar, I’m going to move on. You're an adult, presumably you graduated from high school, and you should be able to speak and write effectively. This especially applies to the following areas:\n\n* _Their_, _there_, and _they’re_\n* _Then_ and _than_\n* _Lose_ and _loose_\n* _Except_ and _accept_\n* _Effect_ and _affect_\n* And brand/product names: “I-phone” (instead of _iPhone_) and \"MAC\" (instead of _Mac_, or _Macintosh_, or _macOS_) is very, very wrong.\n\nIf you're not going to take the time to get your writing right (especially on a résumé), what else are you not going to take the time to do?\n\n## Conclusion\n\nNow, I know that I don’t speak for everybody. Some people take a very different approach to reviewing résumés, and that’s fine. It’s also possible that I’m not the kind of person that you’d like to work with/for, and that’s fine too.\n\nI know that facets of my approach are used by people that I consider some of the best in the industry, and as such I’ve tried to learn from the best ideas I’ve come across to make the most informed decisions possible when determining whether or not to schedule the next step (i.e., the phone interview).\n\nIf you’re involved in creating things for the web, take heed to my suggestions. Even if you don’t follow all of them, definitely make it a point to raise the quality of your résumé.\n"},
    "links": {
        "prev": {"title": "Serving Bandwidth-Friendly Video with HTTP Live Streaming (HLS)", "permalink": "https://ryanparman.com/posts/2018/serving-bandwidth-friendly-video-with-hls/"},
        "next": {"title": "Understanding Trust in Your Infrastructure", "permalink": "https://ryanparman.com/posts/2018/understanding-trust-in-your-infrastructure/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "Serving Bandwidth-Friendly Video with HTTP Live Streaming (HLS)",
    "description": "",
    "summary": {
        "content": "While YouTube is free (as in money) to use, the cost is paid in terms of privacy and advertising analytics. So I've decided to investigate self-hosting my video content. The Cost of YouTube With YouTube, you sacrifice privacy in favor of cost. YouTube is the very best at what they do (serve video to all resolutions and bandwidths), and they are backed by Google who is the very best at what they do (collect data in order to facilitate selling a primed audience to advertisers).",
        "isTruncated": true
    },
    "published": "2018-09-09T03:18:33Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/serving-bandwidth-friendly-video-with-hls/",
    "relativePermalink": "/posts/2018/serving-bandwidth-friendly-video-with-hls/",
    "aliases": ["/2018/09/09/serving-bandwidth-friendly-video-with-hls"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/adaptive-bitrate-streaming.png", "https://cdn.ryanparman.com/hugo/posts/2018/youtube-2017.png"],
    "videos": ["https://cdn.ryanparman.com/hls/hallelujah.fmp4/progressive.mp4"],
    "categories": ["Projects and Code"],
    "tags": ["youtube", "h.264", "hls", "mpeg", "dash", "streaming", "s3", "cloudfront", "safari", "chrome", "firefox", "edge"],
    "series": [],
    "keywords": [],
    "meta": {
        "wordCount": 1611,
        "readingTime": "8 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20180909-serving-bandwidth-friendly-video-with-hls.md",
        "logicalName": "20180909-serving-bandwidth-friendly-video-with-hls.md",
        "translationBaseName": "20180909-serving-bandwidth-friendly-video-with-hls",
        "baseFileName": "20180909-serving-bandwidth-friendly-video-with-hls",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#the-cost-of-youtube\"\u003eThe Cost of YouTube\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#self-hosting-video-content\"\u003eSelf-Hosting Video Content\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#adaptive-bitrate-streaming\"\u003eAdaptive Bitrate Streaming\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#http-live-streaming-hls\"\u003eHTTP Live Streaming (HLS)\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#sample-video\"\u003eSample Video\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#implementation\"\u003eImplementation\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#encoding-and-deploying-video\"\u003eEncoding and Deploying Video\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#the-client-side-code\"\u003eThe Client-Side Code\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#enabling-chrome-firefox-and-edge-using-hlsjs\"\u003eEnabling Chrome, Firefox, and Edge using hls.js\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#cors\"\u003eCORS\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nWhile YouTube is free (as in money) to use, the cost is paid in terms of privacy and advertising analytics. So I've decided to investigate self-hosting my video content.\n\u003c/p\u003e\n\n\u003ch2 id=\"the-cost-of-youtube\"\u003eThe Cost of YouTube\u003c/h2\u003e\n\u003cp\u003eWith YouTube, you sacrifice privacy in favor of cost. YouTube is the very best at what they do (serve video to all resolutions and bandwidths), and they are backed by Google who is the very best at what they do (collect data in order to facilitate selling a primed audience to advertisers).\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/youtube-2017.webp\" alt=\"\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/youtube-2017.png\" alt=\"\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eThere’s nothing inherently wrong with that. We live in a capitalistic society; there is money to be made; Google/YouTube is providing a service to advertisers; many consumers will (knowingly or unknowingly) give up their privacy in exchange for free-as-in-money services.\u003c/p\u003e\n\u003cp\u003eBut as I\u0026rsquo;ve gotten older and started to realize just \u003cem\u003ehow much\u003c/em\u003e data Google has on each and every one of us, I\u0026rsquo;ve started valuing my privacy a lot more. I\u0026rsquo;d like to provide an option for you to protect your privacy as well.\u003c/p\u003e\n\u003ch2 id=\"self-hosting-video-content\"\u003eSelf-Hosting Video Content\u003c/h2\u003e\n\u003cp\u003eEven with efficient video codecs, video can still cost a lot of money to serve.\u003c/p\u003e\n\u003cp\u003eMany websites provide \u003cem\u003ea video\u003c/em\u003e to their users, wherein this video is a single file, and the browser will begin loading and playing the video from start to finish. This means that even if the user only watches the first few seconds of a 5 minute video, it\u0026rsquo;s possible that the video is downloaded in its entirety — which is an unnecessary cost.\u003c/p\u003e\n\u003cp\u003eHowever, we can provide a \u003cem\u003ebetter user experience\u003c/em\u003e as well as \u003cem\u003ereduce hosting costs\u003c/em\u003e by leveraging the ability to serve bandwidth-adaptive chunks of video to players on-demand.\u003c/p\u003e\n\u003ch3 id=\"adaptive-bitrate-streaming\"\u003eAdaptive Bitrate Streaming\u003c/h3\u003e\n\u003cp\u003eThere are two major, semi-compatible approaches to \u003ca href=\"https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\"\u003eadaptive bitrate streaming\u003c/a\u003e over HTTP. One is called \u003ca href=\"https://web.archive.org/web/20180909031833/https://en.wikipedia.org/wiki/HTTP_Live_Streaming\"\u003eHTTP Live Streaming\u003c/a\u003e (“HLS”), and the other is called \u003ca href=\"https://web.archive.org/web/20180909031833/https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP\"\u003eDynamic Adaptive Streaming over HTTP\u003c/a\u003e (“MPEG-DASH”).\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/adaptive-bitrate-streaming.webp\" alt=\"\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/adaptive-bitrate-streaming.png\" alt=\"\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eFrom \u003ca href=\"https://web.archive.org/web/20180909031833/https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\"\u003eWikipedia\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAdaptive bitrate streaming is a technique used in streaming multimedia over computer networks. While in the past most video or audio streaming technologies utilized streaming protocols such as RTP with RTSP, today\u0026rsquo;s adaptive streaming technologies are almost exclusively based on HTTP and designed to work efficiently over large distributed HTTP networks such as the Internet.\u003c/p\u003e\n\u003cp\u003eIt works by detecting a user\u0026rsquo;s bandwidth and CPU capacity in real time and adjusting the quality of the media stream accordingly. It requires the use of an encoder which can encode a single source media (video or audio) at multiple bit rates. The player client switches between streaming the different encodings depending on available resources. \u0026ldquo;The result: very little buffering, fast start time and a good experience for both high-end and low-end connections.\u0026rdquo; […]\u003c/p\u003e\n\u003cp\u003eHTTP-based adaptive bitrate streaming technologies yield additional benefits over traditional server-driven adaptive bitrate streaming. First, since the streaming technology is built on top of HTTP, contrary to RTP-based adaptive streaming, the packets have no difficulties traversing firewall and NAT devices. Second, since HTTP streaming is purely client-driven, all adaptation logic resides at the client. This reduces the requirement of persistent connections between server and client application. Furthermore, the server is not required to maintain session state information on each client, increasing scalability. Finally, existing HTTP delivery infrastructure, such as HTTP caches and servers can be seamlessly adopted.\u003c/p\u003e\n\u003cp\u003eA scalable CDN is used to deliver media streaming to an Internet audience. The CDN receives the stream from the source at its Origin server, then replicates it to many or all of its Edge cache servers. The end-user requests the stream and is redirected to the \u0026ldquo;closest\u0026rdquo; Edge server. […] The use of HTTP-based adaptive streaming allows the Edge server to run a simple HTTP server software, whose licence cost is cheap or free, reducing software licensing cost, compared to costly media server licences (e.g. Adobe Flash Media Streaming Server). The CDN cost for HTTP streaming media is then similar to HTTP web caching CDN cost.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis means that we can use off-the-shelf services like \u003ca href=\"https://aws.amazon.com/s3\"\u003eAmazon S3\u003c/a\u003e and \u003ca href=\"https://aws.amazon.com/cloudfront\"\u003eAmazon CloudFront\u003c/a\u003e to serve video, which are relatively inexpensive and have large user-bases who can answer questions when you run into issues.\u003c/p\u003e\n\u003ch3 id=\"http-live-streaming-hls\"\u003eHTTP Live Streaming (HLS)\u003c/h3\u003e\n\u003cp\u003eAfter doing some research, I came across a blog post that was particularly helpful — “\u003ca href=\"https://web.archive.org/web/20180909031833/https://vincent.bernat.ch/en/blog/2018-self-hosted-videos\"\u003eSelf-hosted videos with HLS\u003c/a\u003e” by Vincent Bernat.\u003c/p\u003e\n\u003cp\u003eVincent writes:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo serve HLS videos, you need three kinds of files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe media segments (encoded with different bitrates/resolutions),\u003c/li\u003e\n\u003cli\u003ea media playlist for each variant, listing the media segments, and\u003c/li\u003e\n\u003cli\u003ea master playlist, listing the media playlists.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMedia segments can come in two formats:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMPEG-2 Transport Streams (TS), or\u003c/li\u003e\n\u003cli\u003eFragmented MP4.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFragmented MP4 media segments are supported since iOS 10. They are a bit more efficient and can be reused to serve the same content as MPEG-DASH (only the playlists are different). Also, they can be served from the same file with range requests. However, if you want to target older versions of iOS, you need to stick with MPEG-2 TS.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAt the time of this writing, iOS 12 will be out in a week or two. A quick search tells me that \u003ca href=\"https://web.archive.org/web/20180909031833/https://data.apteligent.com/ios/\"\u003eiOS 10 and newer make up 85% of all iOS users\u003c/a\u003e. This means that I can pretty safely use the \u003cem\u003eFragmented MP4\u003c/em\u003e method which, according to \u003ca href=\"https://web.archive.org/web/20180909031833/https://bitmovin.com/hls-news-wwdc-2016/\"\u003ethese\u003c/a\u003e \u003ca href=\"https://web.archive.org/web/20180909031833/http://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=111796\"\u003esources\u003c/a\u003e, is more compatible with MPEG-DASH for some cross-over implementations in the future.\u003c/p\u003e\n\u003ch2 id=\"sample-video\"\u003eSample Video\u003c/h2\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; overflow: hidden;\"\u003e\n    \u003cvideo poster=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/poster.jpg\" controls preload=\"none\" style=\"position: absolute; width: 100%; height: 100%;\"\u003e\n      \u003csource src=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/index.m3u8\" type=\"application/vnd.apple.mpegurl\"\u003e\n      \u003csource src=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/progressive.mp4\" type='video/mp4; codecs=\"avc1.4d401f, mp4a.40.2\"'\u003e\n    \u003c/video\u003e\n\u003c/div\u003e\u003cp class=\"tc black-60 db mb2\"\u003e\u003csmall\u003e\u003cb\u003eSource:\u003c/b\u003e \u003ca href=\"https://youtu.be/e1C9kpMV2e8\"\u003eHallelujah - Brooklyn Duo (Piano \u0026#43; Cello)\u003c/a\u003e\u003c/small\u003e\u003c/p\u003e\n\u003ch2 id=\"implementation\"\u003eImplementation\u003c/h2\u003e\n\u003ch3 id=\"encoding-and-deploying-video\"\u003eEncoding and Deploying Video\u003c/h3\u003e\n\u003cp\u003eVincent Bernat provides a tool on GitHub which greatly simplifies the process of creating the various video fragments called \u003ca href=\"https://web.archive.org/web/20180909031833/https://github.com/vincentbernat/video2hls\"\u003evideo2hls\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor \u003ca href=\"https://github.com/skyzyx/blog/blob/master/Makefile\"\u003ethis website\u003c/a\u003e, I have put together a workflow for creating and serving HLS video content.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eI use \u003cstrong\u003eH.264\u003c/strong\u003e video with \u003cstrong\u003eAAC\u003c/strong\u003e audio wrapped inside an \u003cstrong\u003eMP4\u003c/strong\u003e container, \u003cem\u003eexclusively\u003c/em\u003e. These are all defined as part of the MPEG-4 specification, and is the \u003ca href=\"https://caniuse.com/#search=H.264\"\u003ebest-supported grouping of codecs and containers across all browsers and devices\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHardware-level decoders are commonplace inside computers, phones, tablets, and set-top boxes like Xbox, PlayStation, and Apple TV.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI have a directory called \u003ccode\u003estreaming-video\u003c/code\u003e, which is separate from the images that I use and push to S3. Video files are large, and I don\u0026rsquo;t want to accidentally push partially-completed video data to my caching CDN before they\u0026rsquo;re ready.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI have a command which takes any video file inside the \u003ccode\u003estreaming-video\u003c/code\u003e folder, with a filename ending in \u003ccode\u003e-source.mp4\u003c/code\u003e, and passes it through \u003ccode\u003evideo2hls\u003c/code\u003e, creating a folder called \u003ccode\u003e{video}.fmp4\u003c/code\u003e which contains all of the video and playlist files I need across a large variety of bandwidths and resolutions.\u003c/p\u003e\n\u003cp\u003eIt will only do the work to create the directory and all of the fragmented files if the directory doesn\u0026rsquo;t already exist.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003efind ./streaming-video -type f -name \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;*-source.mp4\u0026#34;\u003c/span\u003e | xargs -I \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    bash -c \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;if [ ! -d \u0026#34;${1%-source.mp4}.fmp4\u0026#34; ]; then \\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e        video2hls --debug --output \u0026#34;${1%-source.mp4}.fmp4\u0026#34; --hls-type fmp4 \u0026#34;$1\u0026#34;; \\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e    fi;\u0026#39;\u003c/span\u003e _ \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI find all of the \u003ccode\u003e.m3u8\u003c/code\u003e playlist files and gzip them (since they\u0026rsquo;re just text). This is essentially an in-place rewrite of the files.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003efind ./streaming-video -type f -name \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;*.m3u8\u0026#34;\u003c/span\u003e | xargs -P \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e -I \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    bash -c \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;! gunzip -t $1 2\u0026gt;/dev/null \u0026amp;\u0026amp; gzip -v $1 \u0026amp;\u0026amp; mv -v $1.gz $1;\u0026#39;\u003c/span\u003e _ \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLastly, I push all of the files up to the \u003ccode\u003ehls\u003c/code\u003e folder in my S3 bucket using the \u003ca href=\"https://github.com/aws/aws-cli\"\u003eAWS Unified CLI Tools\u003c/a\u003e, setting the correct \u003ccode\u003eContent-Type\u003c/code\u003e and \u003ccode\u003eContent-Encoding\u003c/code\u003e headers.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"color:#75715e\"\u003e# The .m3u8 playlists that we gzipped\u003c/span\u003e\naws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --exclude \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.*\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --include \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.m3u8\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --acl\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003epublic-read \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --cache-control max-age\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e31536000,public \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --content-type \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;application/vnd.apple.mpegurl\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --content-encoding \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;gzip\u0026#39;\u003c/span\u003e\n\n\u003cspan style=\"color:#75715e\"\u003e# The video \u0026#34;posters\u0026#34;\u003c/span\u003e\naws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --exclude \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.*\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --include \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.jpg\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --acl\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003epublic-read \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --cache-control max-age\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e31536000,public \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --content-type \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;image/jpeg\u0026#39;\u003c/span\u003e\n\n\u003cspan style=\"color:#75715e\"\u003e# The fragmented MP4 files\u003c/span\u003e\naws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --exclude \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.*\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --include \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*.mp4\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --acl\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003epublic-read \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --cache-control max-age\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e31536000,public \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --content-type \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video/mp4\u0026#39;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"the-client-side-code\"\u003eThe Client-Side Code\u003c/h3\u003e\n\u003cp\u003eAfter pushing the content to our CDN, we can use the standard HTML5 \u003ccode\u003e\u0026lt;video\u0026gt;\u003c/code\u003e tag to tell browsers how to load the requested assets.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u0026lt;\u003cspan style=\"color:#f92672\"\u003evideo\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eposter\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/poster.jpg\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econtrols\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003epreload\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;none\u0026#34;\u003c/span\u003e\u0026gt;\n    \u0026lt;\u003cspan style=\"color:#f92672\"\u003esource\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/index.m3u8\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etype\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;application/vnd.apple.mpegurl\u0026#34;\u003c/span\u003e\u0026gt;\n    \u0026lt;\u003cspan style=\"color:#f92672\"\u003esource\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/progressive.mp4\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etype\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video/mp4; codecs=\u0026#34;avc1.4d401f, mp4a.40.2\u0026#34;\u0026#39;\u003c/span\u003e\u0026gt;\n\u0026lt;/\u003cspan style=\"color:#f92672\"\u003evideo\u003c/span\u003e\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eHere, we have a static \u003cem\u003eposter\u003c/em\u003e image that the \u003ccode\u003e\u0026lt;video\u0026gt;\u003c/code\u003e element loads by default.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNext, we have an HLS-compatible playlist file (\u003ccode\u003e.m3u8\u003c/code\u003e), which ultimately points to the correct \u003ccode\u003e.mp4\u003c/code\u003e files.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLastly, we have a standard \u003ccode\u003e.mp4\u003c/code\u003e fallback.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"enabling-chrome-firefox-and-edge-using-hlsjs\"\u003eEnabling Chrome, Firefox, and Edge using hls.js\u003c/h3\u003e\n\u003cp\u003eDailymotion has released a JavaScript library called \u003ca href=\"https://github.com/video-dev/hls.js\"\u003ehls.js\u003c/a\u003e which enables HLS playback on browsers like Chrome, Firefox, and Edge using Fragmented MP4 sources.\u003c/p\u003e\n\u003cp\u003eYou can load the script from the CDN:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u0026lt;\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.jsdelivr.net/npm/hls.js@latest\u0026#34;\u003c/span\u003e\u0026gt;\u0026lt;/\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter that, we have the implementation. Here, we start with a working \u003ccode\u003e\u0026lt;video\u0026gt;\u003c/code\u003e element, then use JavaScript to swap over to HLS.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-javascript\" data-lang=\"javascript\"\u003e(() =\u0026gt; {\n  \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;use strict\u0026#39;\u003c/span\u003e;\n\n  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eHls\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eisSupported\u003c/span\u003e()) {\n    \u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eselector\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;video source[type=\u0026#39;application/vnd.apple.mpegurl\u0026#39;]\u0026#34;\u003c/span\u003e,\n        \u003cspan style=\"color:#a6e22e\"\u003evideoSources\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e document.\u003cspan style=\"color:#a6e22e\"\u003equerySelectorAll\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eselector\u003c/span\u003e);\n\n    \u003cspan style=\"color:#a6e22e\"\u003evideoSources\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eforEach\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003evideoSource\u003c/span\u003e =\u0026gt; {\n      \u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003em3u8\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003evideoSource\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e,\n          \u003cspan style=\"color:#a6e22e\"\u003eonce\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efalse\u003c/span\u003e;\n\n      \u003cspan style=\"color:#75715e\"\u003e// Clone the video to remove any source\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e      \u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eoldVideo\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003evideoSource\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eparentNode\u003c/span\u003e,\n          \u003cspan style=\"color:#a6e22e\"\u003enewVideo\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eoldVideo\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ecloneNode\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003efalse\u003c/span\u003e);\n\n      \u003cspan style=\"color:#75715e\"\u003e// Replace video tag with our clone.\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e      \u003cspan style=\"color:#a6e22e\"\u003eoldVideo\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eparentNode\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ereplaceChild\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003enewVideo\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eoldVideo\u003c/span\u003e);\n\n      \u003cspan style=\"color:#75715e\"\u003e// On play, initialize hls.js, once.\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e      \u003cspan style=\"color:#a6e22e\"\u003enewVideo\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eaddEventListener\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;play\u0026#39;\u003c/span\u003e, () =\u0026gt; {\n        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eonce\u003c/span\u003e) {\n          \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e;\n        };\n        \u003cspan style=\"color:#a6e22e\"\u003eonce\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003etrue\u003c/span\u003e;\n\n        \u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ehls\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eHls\u003c/span\u003e({\n          \u003cspan style=\"color:#a6e22e\"\u003ecapLevelToPlayerSize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efalse\u003c/span\u003e\n        });\n        \u003cspan style=\"color:#a6e22e\"\u003ehls\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eattachMedia\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003enewVideo\u003c/span\u003e);\n        \u003cspan style=\"color:#a6e22e\"\u003ehls\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eloadSource\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003em3u8\u003c/span\u003e);\n        \u003cspan style=\"color:#a6e22e\"\u003ehls\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eon\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eHls\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eEvents\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eMANIFEST_PARSED\u003c/span\u003e, (\u003cspan style=\"color:#a6e22e\"\u003eevent\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003edata\u003c/span\u003e) =\u0026gt; {\n          \u003cspan style=\"color:#a6e22e\"\u003enewVideo\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eplay\u003c/span\u003e();\n        });\n      }, \u003cspan style=\"color:#66d9ef\"\u003efalse\u003c/span\u003e);\n    });\n  }\n})();\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"cors\"\u003eCORS\u003c/h3\u003e\n\u003cp\u003eIf you are serving the files from a third-party host (such as Amazon S3), you will need to enable \u003ca href=\"https://caniuse.com/#search=cors\"\u003eCORS\u003c/a\u003e support on your bucket.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan style=\"color:#75715e\"\u003e\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\u003c/span\u003e\n\u003cspan style=\"color:#f92672\"\u003e\u0026lt;CORSConfiguration\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003exmlns=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://s3.amazonaws.com/doc/2006-03-01/\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e\n  \u003cspan style=\"color:#f92672\"\u003e\u0026lt;CORSRule\u0026gt;\u003c/span\u003e\n    \u003cspan style=\"color:#f92672\"\u003e\u0026lt;AllowedHeader\u0026gt;\u003c/span\u003e*\u003cspan style=\"color:#f92672\"\u003e\u0026lt;/AllowedHeader\u0026gt;\u003c/span\u003e\n    \u003cspan style=\"color:#f92672\"\u003e\u0026lt;AllowedOrigin\u0026gt;\u003c/span\u003e*\u003cspan style=\"color:#f92672\"\u003e\u0026lt;/AllowedOrigin\u0026gt;\u003c/span\u003e\n    \u003cspan style=\"color:#f92672\"\u003e\u0026lt;AllowedMethod\u0026gt;\u003c/span\u003eGET\u003cspan style=\"color:#f92672\"\u003e\u0026lt;/AllowedMethod\u0026gt;\u003c/span\u003e\n    \u003cspan style=\"color:#f92672\"\u003e\u0026lt;AllowedMethod\u0026gt;\u003c/span\u003eHEAD\u003cspan style=\"color:#f92672\"\u003e\u0026lt;/AllowedMethod\u0026gt;\u003c/span\u003e\n  \u003cspan style=\"color:#f92672\"\u003e\u0026lt;/CORSRule\u0026gt;\u003c/span\u003e\n\u003cspan style=\"color:#f92672\"\u003e\u0026lt;/CORSConfiguration\u0026gt;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAdditionally, if you have a CDN cache in front of that S3 bucket (e.g., Amazon CloudFront), you\u0026rsquo;ll need to make sure that it is configured to allow the \u003ccode\u003eOrigin\u003c/code\u003e headers through and also respond to the HTTP \u003ccode\u003eOPTIONS\u003c/code\u003e verb.\u003c/p\u003e\n\u003cp\u003eYou can find more information about solving this problem with CloudFront at “\u003ca href=\"https://web.archive.org/web/20180909031833/https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html#header-caching-web-cors\"\u003eConfiguring CloudFront to Respect CORS Settings\u003c/a\u003e”.\u003c/p\u003e\n\u003cscript src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://ryanparman.com/js/stream-hls.min.270faa3c315322e584846228fe3da0897e67544a45ac2646b9050c886baa54b9.js\" integrity=\"sha256-Jw\u0026#43;qPDFTIuWEhGIo/j2giX5nVEpFrCZGuQUMiGuqVLk=\"\u003e\u003c/script\u003e\n\n",
        "plain":"While YouTube is free (as in money) to use, the cost is paid in terms of privacy and advertising analytics. So I've decided to investigate self-hosting my video content. The Cost of YouTube With YouTube, you sacrifice privacy in favor of cost. YouTube is the very best at what they do (serve video to all resolutions and bandwidths), and they are backed by Google who is the very best at what they do (collect data in order to facilitate selling a primed audience to advertisers).\n  There’s nothing inherently wrong with that. We live in a capitalistic society; there is money to be made; Google/YouTube is providing a service to advertisers; many consumers will (knowingly or unknowingly) give up their privacy in exchange for free-as-in-money services.\nBut as I\u0026rsquo;ve gotten older and started to realize just how much data Google has on each and every one of us, I\u0026rsquo;ve started valuing my privacy a lot more. I\u0026rsquo;d like to provide an option for you to protect your privacy as well.\nSelf-Hosting Video Content Even with efficient video codecs, video can still cost a lot of money to serve.\nMany websites provide a video to their users, wherein this video is a single file, and the browser will begin loading and playing the video from start to finish. This means that even if the user only watches the first few seconds of a 5 minute video, it\u0026rsquo;s possible that the video is downloaded in its entirety — which is an unnecessary cost.\nHowever, we can provide a better user experience as well as reduce hosting costs by leveraging the ability to serve bandwidth-adaptive chunks of video to players on-demand.\nAdaptive Bitrate Streaming There are two major, semi-compatible approaches to adaptive bitrate streaming over HTTP. One is called HTTP Live Streaming (“HLS”), and the other is called Dynamic Adaptive Streaming over HTTP (“MPEG-DASH”).\n  From Wikipedia:\n Adaptive bitrate streaming is a technique used in streaming multimedia over computer networks. While in the past most video or audio streaming technologies utilized streaming protocols such as RTP with RTSP, today\u0026rsquo;s adaptive streaming technologies are almost exclusively based on HTTP and designed to work efficiently over large distributed HTTP networks such as the Internet.\nIt works by detecting a user\u0026rsquo;s bandwidth and CPU capacity in real time and adjusting the quality of the media stream accordingly. It requires the use of an encoder which can encode a single source media (video or audio) at multiple bit rates. The player client switches between streaming the different encodings depending on available resources. \u0026ldquo;The result: very little buffering, fast start time and a good experience for both high-end and low-end connections.\u0026rdquo; […]\nHTTP-based adaptive bitrate streaming technologies yield additional benefits over traditional server-driven adaptive bitrate streaming. First, since the streaming technology is built on top of HTTP, contrary to RTP-based adaptive streaming, the packets have no difficulties traversing firewall and NAT devices. Second, since HTTP streaming is purely client-driven, all adaptation logic resides at the client. This reduces the requirement of persistent connections between server and client application. Furthermore, the server is not required to maintain session state information on each client, increasing scalability. Finally, existing HTTP delivery infrastructure, such as HTTP caches and servers can be seamlessly adopted.\nA scalable CDN is used to deliver media streaming to an Internet audience. The CDN receives the stream from the source at its Origin server, then replicates it to many or all of its Edge cache servers. The end-user requests the stream and is redirected to the \u0026ldquo;closest\u0026rdquo; Edge server. […] The use of HTTP-based adaptive streaming allows the Edge server to run a simple HTTP server software, whose licence cost is cheap or free, reducing software licensing cost, compared to costly media server licences (e.g. Adobe Flash Media Streaming Server). The CDN cost for HTTP streaming media is then similar to HTTP web caching CDN cost.\n This means that we can use off-the-shelf services like Amazon S3 and Amazon CloudFront to serve video, which are relatively inexpensive and have large user-bases who can answer questions when you run into issues.\nHTTP Live Streaming (HLS) After doing some research, I came across a blog post that was particularly helpful — “Self-hosted videos with HLS” by Vincent Bernat.\nVincent writes:\n To serve HLS videos, you need three kinds of files:\n the media segments (encoded with different bitrates/resolutions), a media playlist for each variant, listing the media segments, and a master playlist, listing the media playlists.  Media segments can come in two formats:\n MPEG-2 Transport Streams (TS), or Fragmented MP4.  Fragmented MP4 media segments are supported since iOS 10. They are a bit more efficient and can be reused to serve the same content as MPEG-DASH (only the playlists are different). Also, they can be served from the same file with range requests. However, if you want to target older versions of iOS, you need to stick with MPEG-2 TS.\n At the time of this writing, iOS 12 will be out in a week or two. A quick search tells me that iOS 10 and newer make up 85% of all iOS users. This means that I can pretty safely use the Fragmented MP4 method which, according to these sources, is more compatible with MPEG-DASH for some cross-over implementations in the future.\nSample Video  Source: Hallelujah - Brooklyn Duo (Piano \u0026#43; Cello)\nImplementation Encoding and Deploying Video Vincent Bernat provides a tool on GitHub which greatly simplifies the process of creating the various video fragments called video2hls.\nFor this website, I have put together a workflow for creating and serving HLS video content.\n  I use H.264 video with AAC audio wrapped inside an MP4 container, exclusively. These are all defined as part of the MPEG-4 specification, and is the best-supported grouping of codecs and containers across all browsers and devices.\nHardware-level decoders are commonplace inside computers, phones, tablets, and set-top boxes like Xbox, PlayStation, and Apple TV.\n  I have a directory called streaming-video, which is separate from the images that I use and push to S3. Video files are large, and I don\u0026rsquo;t want to accidentally push partially-completed video data to my caching CDN before they\u0026rsquo;re ready.\n  I have a command which takes any video file inside the streaming-video folder, with a filename ending in -source.mp4, and passes it through video2hls, creating a folder called {video}.fmp4 which contains all of the video and playlist files I need across a large variety of bandwidths and resolutions.\nIt will only do the work to create the directory and all of the fragmented files if the directory doesn\u0026rsquo;t already exist.\nfind ./streaming-video -type f -name \u0026#34;*-source.mp4\u0026#34; | xargs -I {} \\  bash -c \u0026#39;if [ ! -d \u0026#34;${1%-source.mp4}.fmp4\u0026#34; ]; then \\ video2hls --debug --output \u0026#34;${1%-source.mp4}.fmp4\u0026#34; --hls-type fmp4 \u0026#34;$1\u0026#34;; \\ fi;\u0026#39; _ {} \\;   I find all of the .m3u8 playlist files and gzip them (since they\u0026rsquo;re just text). This is essentially an in-place rewrite of the files.\nfind ./streaming-video -type f -name \u0026#34;*.m3u8\u0026#34; | xargs -P 8 -I {} \\  bash -c \u0026#39;! gunzip -t $1 2\u0026gt;/dev/null \u0026amp;\u0026amp; gzip -v $1 \u0026amp;\u0026amp; mv -v $1.gz $1;\u0026#39; _ {} \\;   Lastly, I push all of the files up to the hls folder in my S3 bucket using the AWS Unified CLI Tools, setting the correct Content-Type and Content-Encoding headers.\n# The .m3u8 playlists that we gzipped aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\  --exclude \u0026#39;*.*\u0026#39; \\  --include \u0026#39;*.m3u8\u0026#39; \\  --acl=public-read \\  --cache-control max-age=31536000,public \\  --content-type \u0026#39;application/vnd.apple.mpegurl\u0026#39; \\  --content-encoding \u0026#39;gzip\u0026#39; # The video \u0026#34;posters\u0026#34; aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\  --exclude \u0026#39;*.*\u0026#39; \\  --include \u0026#39;*.jpg\u0026#39; \\  --acl=public-read \\  --cache-control max-age=31536000,public \\  --content-type \u0026#39;image/jpeg\u0026#39; # The fragmented MP4 files aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\  --exclude \u0026#39;*.*\u0026#39; \\  --include \u0026#39;*.mp4\u0026#39; \\  --acl=public-read \\  --cache-control max-age=31536000,public \\  --content-type \u0026#39;video/mp4\u0026#39;   The Client-Side Code After pushing the content to our CDN, we can use the standard HTML5 \u0026lt;video\u0026gt; tag to tell browsers how to load the requested assets.\n\u0026lt;video poster=\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/poster.jpg\u0026#34; controls preload=\u0026#34;none\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/index.m3u8\u0026#34; type=\u0026#34;application/vnd.apple.mpegurl\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;https://cdn.ryanparman.com/hls/hallelujah.fmp4/progressive.mp4\u0026#34; type=\u0026#39;video/mp4; codecs=\u0026#34;avc1.4d401f, mp4a.40.2\u0026#34;\u0026#39;\u0026gt; \u0026lt;/video\u0026gt;   Here, we have a static poster image that the \u0026lt;video\u0026gt; element loads by default.\n  Next, we have an HLS-compatible playlist file (.m3u8), which ultimately points to the correct .mp4 files.\n  Lastly, we have a standard .mp4 fallback.\n  Enabling Chrome, Firefox, and Edge using hls.js Dailymotion has released a JavaScript library called hls.js which enables HLS playback on browsers like Chrome, Firefox, and Edge using Fragmented MP4 sources.\nYou can load the script from the CDN:\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/hls.js@latest\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; After that, we have the implementation. Here, we start with a working \u0026lt;video\u0026gt; element, then use JavaScript to swap over to HLS.\n(() =\u0026gt; { \u0026#39;use strict\u0026#39;; if (Hls.isSupported()) { let selector = \u0026#34;video source[type=\u0026#39;application/vnd.apple.mpegurl\u0026#39;]\u0026#34;, videoSources = document.querySelectorAll(selector); videoSources.forEach(videoSource =\u0026gt; { let m3u8 = videoSource.src, once = false; // Clone the video to remove any source  let oldVideo = videoSource.parentNode, newVideo = oldVideo.cloneNode(false); // Replace video tag with our clone.  oldVideo.parentNode.replaceChild(newVideo, oldVideo); // On play, initialize hls.js, once.  newVideo.addEventListener(\u0026#39;play\u0026#39;, () =\u0026gt; { if (once) { return; }; once = true; var hls = new Hls({ capLevelToPlayerSize: false }); hls.attachMedia(newVideo); hls.loadSource(m3u8); hls.on(Hls.Events.MANIFEST_PARSED, (event, data) =\u0026gt; { newVideo.play(); }); }, false); }); } })(); CORS If you are serving the files from a third-party host (such as Amazon S3), you will need to enable CORS support on your bucket.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;CORSConfiguration xmlns=\u0026#34;http://s3.amazonaws.com/doc/2006-03-01/\u0026#34;\u0026gt; \u0026lt;CORSRule\u0026gt; \u0026lt;AllowedHeader\u0026gt;*\u0026lt;/AllowedHeader\u0026gt; \u0026lt;AllowedOrigin\u0026gt;*\u0026lt;/AllowedOrigin\u0026gt; \u0026lt;AllowedMethod\u0026gt;GET\u0026lt;/AllowedMethod\u0026gt; \u0026lt;AllowedMethod\u0026gt;HEAD\u0026lt;/AllowedMethod\u0026gt; \u0026lt;/CORSRule\u0026gt; \u0026lt;/CORSConfiguration\u0026gt; Additionally, if you have a CDN cache in front of that S3 bucket (e.g., Amazon CloudFront), you\u0026rsquo;ll need to make sure that it is configured to allow the Origin headers through and also respond to the HTTP OPTIONS verb.\nYou can find more information about solving this problem with CloudFront at “Configuring CloudFront to Respect CORS Settings”.\n  ",
        "source":"\n{{\u003cdescription\u003e}}\nWhile YouTube is free (as in money) to use, the cost is paid in terms of privacy and advertising analytics. So I've decided to investigate self-hosting my video content.\n{{\u003c/description\u003e}}\n\n## The Cost of YouTube\n\nWith YouTube, you sacrifice privacy in favor of cost. YouTube is the very best at what they do (serve video to all resolutions and bandwidths), and they are backed by Google who is the very best at what they do (collect data in order to facilitate selling a primed audience to advertisers).\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/youtube-2017.png\"\u003e}}\n\nThere’s nothing inherently wrong with that. We live in a capitalistic society; there is money to be made; Google/YouTube is providing a service to advertisers; many consumers will (knowingly or unknowingly) give up their privacy in exchange for free-as-in-money services.\n\nBut as I've gotten older and started to realize just _how much_ data Google has on each and every one of us, I've started valuing my privacy a lot more. I'd like to provide an option for you to protect your privacy as well.\n\n## Self-Hosting Video Content\n\nEven with efficient video codecs, video can still cost a lot of money to serve.\n\nMany websites provide _a video_ to their users, wherein this video is a single file, and the browser will begin loading and playing the video from start to finish. This means that even if the user only watches the first few seconds of a 5 minute video, it's possible that the video is downloaded in its entirety — which is an unnecessary cost.\n\nHowever, we can provide a _better user experience_ as well as _reduce hosting costs_ by leveraging the ability to serve bandwidth-adaptive chunks of video to players on-demand.\n\n### Adaptive Bitrate Streaming\n\nThere are two major, semi-compatible approaches to [adaptive bitrate streaming](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming) over HTTP. One is called [HTTP Live Streaming]({{\u003c wayback \"https://en.wikipedia.org/wiki/HTTP_Live_Streaming\" \u003e}}) (“HLS”), and the other is called [Dynamic Adaptive Streaming over HTTP]({{\u003c wayback \"https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP\" \u003e}}) (“MPEG-DASH”).\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/adaptive-bitrate-streaming.png\"\u003e}}\n\nFrom [Wikipedia]({{\u003c wayback \"https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\" \u003e}}):\n\n\u003e Adaptive bitrate streaming is a technique used in streaming multimedia over computer networks. While in the past most video or audio streaming technologies utilized streaming protocols such as RTP with RTSP, today's adaptive streaming technologies are almost exclusively based on HTTP and designed to work efficiently over large distributed HTTP networks such as the Internet.\n\u003e \n\u003e It works by detecting a user's bandwidth and CPU capacity in real time and adjusting the quality of the media stream accordingly. It requires the use of an encoder which can encode a single source media (video or audio) at multiple bit rates. The player client switches between streaming the different encodings depending on available resources. \"The result: very little buffering, fast start time and a good experience for both high-end and low-end connections.\" […]\n\u003e\n\u003e HTTP-based adaptive bitrate streaming technologies yield additional benefits over traditional server-driven adaptive bitrate streaming. First, since the streaming technology is built on top of HTTP, contrary to RTP-based adaptive streaming, the packets have no difficulties traversing firewall and NAT devices. Second, since HTTP streaming is purely client-driven, all adaptation logic resides at the client. This reduces the requirement of persistent connections between server and client application. Furthermore, the server is not required to maintain session state information on each client, increasing scalability. Finally, existing HTTP delivery infrastructure, such as HTTP caches and servers can be seamlessly adopted.\n\u003e\n\u003e A scalable CDN is used to deliver media streaming to an Internet audience. The CDN receives the stream from the source at its Origin server, then replicates it to many or all of its Edge cache servers. The end-user requests the stream and is redirected to the \"closest\" Edge server. […] The use of HTTP-based adaptive streaming allows the Edge server to run a simple HTTP server software, whose licence cost is cheap or free, reducing software licensing cost, compared to costly media server licences (e.g. Adobe Flash Media Streaming Server). The CDN cost for HTTP streaming media is then similar to HTTP web caching CDN cost.\n\nThis means that we can use off-the-shelf services like [Amazon S3](https://aws.amazon.com/s3) and [Amazon CloudFront](https://aws.amazon.com/cloudfront) to serve video, which are relatively inexpensive and have large user-bases who can answer questions when you run into issues.\n\n### HTTP Live Streaming (HLS)\n\nAfter doing some research, I came across a blog post that was particularly helpful — “[Self-hosted videos with HLS]({{\u003c wayback \"https://vincent.bernat.ch/en/blog/2018-self-hosted-videos\" \u003e}})” by Vincent Bernat.\n\nVincent writes:\n\n\u003e To serve HLS videos, you need three kinds of files:\n\u003e \n\u003e * the media segments (encoded with different bitrates/resolutions),\n\u003e * a media playlist for each variant, listing the media segments, and\n\u003e * a master playlist, listing the media playlists.\n\u003e \n\u003e Media segments can come in two formats:\n\u003e \n\u003e * MPEG-2 Transport Streams (TS), or\n\u003e * Fragmented MP4.\n\u003e \n\u003e Fragmented MP4 media segments are supported since iOS 10. They are a bit more efficient and can be reused to serve the same content as MPEG-DASH (only the playlists are different). Also, they can be served from the same file with range requests. However, if you want to target older versions of iOS, you need to stick with MPEG-2 TS.\n\nAt the time of this writing, iOS 12 will be out in a week or two. A quick search tells me that [iOS 10 and newer make up 85% of all iOS users]({{\u003c wayback \"https://data.apteligent.com/ios/\" \u003e}}). This means that I can pretty safely use the _Fragmented MP4_ method which, according to [these]({{\u003c wayback \"https://bitmovin.com/hls-news-wwdc-2016/\" \u003e}}) [sources]({{\u003c wayback \"http://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=111796\" \u003e}}), is more compatible with MPEG-DASH for some cross-over implementations in the future.\n\n## Sample Video\n\n{{\u003chls src=\"hallelujah\"\n       sourceUrl=\"https://youtu.be/e1C9kpMV2e8\"\n       sourceTitle=\"Hallelujah - Brooklyn Duo (Piano + Cello)\"\n\u003e}}\n\n## Implementation\n\n### Encoding and Deploying Video\n\nVincent Bernat provides a tool on GitHub which greatly simplifies the process of creating the various video fragments called [video2hls]({{\u003c wayback \"https://github.com/vincentbernat/video2hls\" \u003e}}).\n\nFor [this website](https://github.com/skyzyx/blog/blob/master/Makefile), I have put together a workflow for creating and serving HLS video content.\n\n1. I use **H.264** video with **AAC** audio wrapped inside an **MP4** container, _exclusively_. These are all defined as part of the MPEG-4 specification, and is the [best-supported grouping of codecs and containers across all browsers and devices](https://caniuse.com/#search=H.264).\n\n    Hardware-level decoders are commonplace inside computers, phones, tablets, and set-top boxes like Xbox, PlayStation, and Apple TV.\n\n1. I have a directory called `streaming-video`, which is separate from the images that I use and push to S3. Video files are large, and I don't want to accidentally push partially-completed video data to my caching CDN before they're ready.\n\n1. I have a command which takes any video file inside the `streaming-video` folder, with a filename ending in `-source.mp4`, and passes it through `video2hls`, creating a folder called `{video}.fmp4` which contains all of the video and playlist files I need across a large variety of bandwidths and resolutions.\n\n    It will only do the work to create the directory and all of the fragmented files if the directory doesn't already exist.\n\n    ```bash\n    find ./streaming-video -type f -name \"*-source.mp4\" | xargs -I {} \\\n        bash -c 'if [ ! -d \"${1%-source.mp4}.fmp4\" ]; then \\\n            video2hls --debug --output \"${1%-source.mp4}.fmp4\" --hls-type fmp4 \"$1\"; \\\n        fi;' _ {} \\;\n    ```\n\n1. I find all of the `.m3u8` playlist files and gzip them (since they're just text). This is essentially an in-place rewrite of the files.\n\n    ```bash\n    find ./streaming-video -type f -name \"*.m3u8\" | xargs -P 8 -I {} \\\n        bash -c '! gunzip -t $1 2\u003e/dev/null \u0026\u0026 gzip -v $1 \u0026\u0026 mv -v $1.gz $1;' _ {} \\;\n    ```\n\n1. Lastly, I push all of the files up to the `hls` folder in my S3 bucket using the [AWS Unified CLI Tools](https://github.com/aws/aws-cli), setting the correct `Content-Type` and `Content-Encoding` headers.\n\n    ```bash\n    # The .m3u8 playlists that we gzipped\n    aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\\n        --exclude '*.*' \\\n        --include '*.m3u8' \\\n        --acl=public-read \\\n        --cache-control max-age=31536000,public \\\n        --content-type 'application/vnd.apple.mpegurl' \\\n        --content-encoding 'gzip'\n\n    # The video \"posters\"\n    aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\\n        --exclude '*.*' \\\n        --include '*.jpg' \\\n        --acl=public-read \\\n        --cache-control max-age=31536000,public \\\n        --content-type 'image/jpeg'\n\n    # The fragmented MP4 files\n    aws s3 sync ./streaming-video s3://blog.ryanparman.com/hls \\\n        --exclude '*.*' \\\n        --include '*.mp4' \\\n        --acl=public-read \\\n        --cache-control max-age=31536000,public \\\n        --content-type 'video/mp4'\n    ```\n\n### The Client-Side Code\n\nAfter pushing the content to our CDN, we can use the standard HTML5 `\u003cvideo\u003e` tag to tell browsers how to load the requested assets.\n\n```html\n\u003cvideo poster=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/poster.jpg\" controls preload=\"none\"\u003e\n    \u003csource src=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/index.m3u8\" type=\"application/vnd.apple.mpegurl\"\u003e\n    \u003csource src=\"https://cdn.ryanparman.com/hls/hallelujah.fmp4/progressive.mp4\" type='video/mp4; codecs=\"avc1.4d401f, mp4a.40.2\"'\u003e\n\u003c/video\u003e\n```\n\n1. Here, we have a static _poster_ image that the `\u003cvideo\u003e` element loads by default.\n\n1. Next, we have an HLS-compatible playlist file (`.m3u8`), which ultimately points to the correct `.mp4` files.\n\n1. Lastly, we have a standard `.mp4` fallback.\n\n### Enabling Chrome, Firefox, and Edge using hls.js\n\nDailymotion has released a JavaScript library called [hls.js](https://github.com/video-dev/hls.js) which enables HLS playback on browsers like Chrome, Firefox, and Edge using Fragmented MP4 sources.\n\nYou can load the script from the CDN:\n\n```html\n\u003cscript src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\"\u003e\u003c/script\u003e\n```\n\nAfter that, we have the implementation. Here, we start with a working `\u003cvideo\u003e` element, then use JavaScript to swap over to HLS.\n\n```javascript\n(() =\u003e {\n  'use strict';\n\n  if (Hls.isSupported()) {\n    let selector = \"video source[type='application/vnd.apple.mpegurl']\",\n        videoSources = document.querySelectorAll(selector);\n\n    videoSources.forEach(videoSource =\u003e {\n      let m3u8 = videoSource.src,\n          once = false;\n\n      // Clone the video to remove any source\n      let oldVideo = videoSource.parentNode,\n          newVideo = oldVideo.cloneNode(false);\n\n      // Replace video tag with our clone.\n      oldVideo.parentNode.replaceChild(newVideo, oldVideo);\n\n      // On play, initialize hls.js, once.\n      newVideo.addEventListener('play', () =\u003e {\n        if (once) {\n          return;\n        };\n        once = true;\n\n        var hls = new Hls({\n          capLevelToPlayerSize: false\n        });\n        hls.attachMedia(newVideo);\n        hls.loadSource(m3u8);\n        hls.on(Hls.Events.MANIFEST_PARSED, (event, data) =\u003e {\n          newVideo.play();\n        });\n      }, false);\n    });\n  }\n})();\n```\n\n### CORS\n\nIf you are serving the files from a third-party host (such as Amazon S3), you will need to enable [CORS](https://caniuse.com/#search=cors) support on your bucket.\n\n```xml\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003cCORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"\u003e\n  \u003cCORSRule\u003e\n    \u003cAllowedHeader\u003e*\u003c/AllowedHeader\u003e\n    \u003cAllowedOrigin\u003e*\u003c/AllowedOrigin\u003e\n    \u003cAllowedMethod\u003eGET\u003c/AllowedMethod\u003e\n    \u003cAllowedMethod\u003eHEAD\u003c/AllowedMethod\u003e\n  \u003c/CORSRule\u003e\n\u003c/CORSConfiguration\u003e\n```\n\nAdditionally, if you have a CDN cache in front of that S3 bucket (e.g., Amazon CloudFront), you'll need to make sure that it is configured to allow the `Origin` headers through and also respond to the HTTP `OPTIONS` verb.\n\nYou can find more information about solving this problem with CloudFront at “[Configuring CloudFront to Respect CORS Settings]({{\u003c wayback \"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html#header-caching-web-cors\" \u003e}})”.\n\n\u003cscript src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\"\u003e\u003c/script\u003e\n{{\u003cscript src=\"/js/stream-hls.js\"\u003e}}\n"},
    "links": {
        "prev": {"title": "Clueless Recruiters, Issue #8", "permalink": "https://ryanparman.com/posts/2018/clueless-recruiters-issue-8/"},
        "next": {"title": "The Hiring Process, Part I", "permalink": "https://ryanparman.com/posts/2018/the-hiring-process-part-i-what-i-look-for-in-a-cv-resume-remastered/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "Clueless Recruiters, Issue #8",
    "description": "",
    "summary": {
        "content": "When recruiters have absolutely no understanding about the things that they're recruiting for! In this episode of Clueless Recruiters! (Cue theme music!) Explanation There are few things that technical people are more annoyed by than technical recruiters. A very large segment of the technical recruiting industry has made a bad name for the rest of their industry by relentlessly spamming technical professionals after having not done their homework. These people hock job openings the same way that sleazy salesmen hock used cars.",
        "isTruncated": true
    },
    "published": "2018-08-22T19:05:01Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/clueless-recruiters-issue-8/",
    "relativePermalink": "/posts/2018/clueless-recruiters-issue-8/",
    "aliases": ["/2018/08/22/clueless-recruiters-issue-8"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/office-space-waiter.jpg"],
    "videos": [],
    "categories": ["Clueless Recruiters"],
    "tags": ["amazon", "mobile"],
    "series": ["Clueless Recruiters"],
    "keywords": [],
    "meta": {
        "wordCount": 854,
        "readingTime": "5 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20180822-clueless-recruiters-issue-8.md",
        "logicalName": "20180822-clueless-recruiters-issue-8.md",
        "translationBaseName": "20180822-clueless-recruiters-issue-8",
        "baseFileName": "20180822-clueless-recruiters-issue-8",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#explanation\"\u003eExplanation\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#recruiter-schlock\"\u003eRecruiter Schlock\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#the-follow-up\"\u003eThe Follow-Up\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#epilogue\"\u003eEpilogue\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nWhen recruiters have absolutely no understanding about the things that they're recruiting for! In this episode of \u003cem\u003eClueless Recruiters\u003c/em\u003e! (Cue theme music!)\n\u003c/p\u003e\n\n\u003ch2 id=\"explanation\"\u003eExplanation\u003c/h2\u003e\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003eThere are few things that technical people are more annoyed by than technical recruiters. A very large segment of the technical recruiting industry has made a bad name for the rest of their industry by relentlessly spamming technical professionals after having \u003ca href=\"/2011/11/05/how-to-make-technical-professionals-not-hate-your-guts-a-guide-for-technical-recruiters/\"\u003enot done their homework\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThese people hock job openings the same way that sleazy salesmen hock used cars. These recruiting companies need to radically change how they do business with the technical crowd, and the \u003ca href=\"/categories/clueless-recruiters/\"\u003eClueless Recruiters\u003c/a\u003e series is an attempt to call out clueless technical recruiters who contact me for jobs that are clearly a terrible fit. Everything here is posted from \u003cem\u003ereal\u003c/em\u003e exchanges between myself and recruiters, entirely uncut. Enjoy!\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003ch2 id=\"recruiter-schlock\"\u003eRecruiter Schlock\u003c/h2\u003e\n\u003cp\u003eHere\u0026rsquo;s one I got a few weeks ago from a \u003cem\u003eclueless recruiter\u003c/em\u003e. For reference, here is my \u003ca href=\"http://ryanparman.com/resume/\"\u003erésumé\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNormally, I don\u0026rsquo;t say which companies these come from, but in this case, I\u0026rsquo;ll make an exception because I used to work there — Amazon.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRyan,\u003c/p\u003e\n\u003cp\u003eYes, I am fully aware you receive endless messages from recruiters (especially other Amazonians). I apologize for just adding to that noise.\u003c/p\u003e\n\u003cp\u003eYou’re probably reading a lot of spam from those individuals. And you’re probably not enjoying yourself. I’m writing this feeling like one of them, and I’m not enjoying myself, either. So, let me cut to the chase.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA recruiter, making an attempt to set themselves apart by sympathizing with me. OK, ok, that\u0026rsquo;s cool.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYour background rocks, you have exp. with mobile application development and you appear to have committed to repos on Github (like “swift4.1” for example – which it looks like you made a pull request!) that line up with our needs on [name of team and department] (I’ll get into that in a second).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cem\u003e*sigh*.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI have a single repository in GitHub where I have attempted to learn Swift by porting a small CLI application to it. Yes it uses Swift, but there is nothing mobile about it. It\u0026rsquo;s a CLI app for macOS.\u003c/p\u003e\n\u003cp\u003eAnd of course I made a pull request. I\u0026rsquo;m the \u003cem\u003esole author\u003c/em\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI won’t pretend that my message is different from the rest you’re receiving.  But, know that it is from a real person who researched your background and found it highly compelling.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou really didn\u0026rsquo;t. AT. ALL.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI recruit for our [name of team and department] (super cool stuff I’ll explain over the phone) and [name of team and department] (Android and iOS roles).\u003c/p\u003e\n\u003cp\u003eWe’re looking for leaders (both on the IC side and on the management side).\u003c/p\u003e\n\u003cp\u003eWe’re essentially building an entirely new way to communicate with other people a background like yours could be a huge factor in driving this new venture.\u003c/p\u003e\n\u003cp\u003eWould you be open to me giving you a bit of an overview of the cool stuff we’re doing and see if it piques your interest?\u003c/p\u003e\n\u003cp\u003eJust FYI, we’re seeking these roles in Sunnyvale, Santa Cruz, Santa Monica, Toronto and Seattle.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eOK. I\u0026rsquo;ve reached a point in my career where these kinds of conversations happen, sure.\u003c/p\u003e\n\u003cp\u003eBut this pitch is about as truthful as \u003cspan itemscope itemtype=\"http://schema.org/Person\"\u003e\u003cspan itemprop=\"name\"\u003eDonald Trump\u003c/span\u003e\u003c/span\u003e\n standing at a podium talking to a crowd of cheering deplorables. Actually, in my mind, I imagine this recruiter to be something like Brian, the chipper waiter from \u003ca href=\"https://letterboxd.com/film/office-space/\"\u003eOffice Space\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/office-space-waiter.webp\" alt=\"Brian, the waiter from Office Space\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/office-space-waiter.jpg\" alt=\"Brian, the waiter from Office Space\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eI ignored this email (as I often do) as simply not being worth the brain-space required to reply.\u003c/p\u003e\n\u003ch2 id=\"the-follow-up\"\u003eThe Follow-Up\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRyan,\u003c/p\u003e\n\u003cp\u003eI shot you an email a few weeks back regarding my interest in your background and how it could translate to our [redacted] team(s).\u003c/p\u003e\n\u003cp\u003eI really liked digging through your GitHub profile as well as LinkedIn. It was quite apparent you have deep experience with mobile application development (via repo “swift4.1”).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"pa2-ns pv4-l\"\u003e\n\t\u003camp-img src=\"https://cdn.ryanparman.com/hugo/posts/2018/finger-up.jpg\" layout=\"responsive\" width=\"\" height=\"\"\u003e\u003c/amp-img\u003e\n    \u003cp class=\"f6 gray tc db\"\u003eI… (pause…) Not worth it.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWould you have any interest in hearing about what we’re doing and the awesome stuff we’re building\u0026hellip;..and see if it piques your interest?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe’re building an entirely new way to communicate with other people and are in big need of individuals with such an awesome background like yourself. I’m searching for lead engineers (individual contributors) as well as managers for several teams.\u003c/p\u003e\n\u003cp\u003eIf you’re not interested, no worries and I’m happy you’re in a great place right now. Just let me know and I’ll not reach out again.\u003c/p\u003e\n\u003cp\u003eHowever, if you do have a little interest in learning about us, I would love to speak sometime.\u003c/p\u003e\n\u003cp\u003eThanks Ryan!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI’m never going back to Amazon ever again. Sorry.\u003c/p\u003e\n\u003ch2 id=\"epilogue\"\u003eEpilogue\u003c/h2\u003e\n\u003cp\u003eNow, I will be the first to admit that this particular recruiter was not nearly as bad as the other recruiters I\u0026rsquo;ve skewered in this series. This person seems fairly reasonable on the surface, but still made the \u003cem\u003ecardinal mistake\u003c/em\u003e of \u003cstrong\u003enot doing their homework\u003c/strong\u003e. At best, they did a quick search for a keyword — which was fairly obvious by the beginning of the note.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecruiters:\u003c/strong\u003e \u003cem\u003eDo your homework\u003c/em\u003e. I know that you have other pressures in your job with regard to filling open reqs within certain timeframes, but you still need to \u003cem\u003edo your homework\u003c/em\u003e.\u003c/p\u003e\n",
        "plain":"When recruiters have absolutely no understanding about the things that they're recruiting for! In this episode of Clueless Recruiters! (Cue theme music!) Explanation There are few things that technical people are more annoyed by than technical recruiters. A very large segment of the technical recruiting industry has made a bad name for the rest of their industry by relentlessly spamming technical professionals after having not done their homework.\nThese people hock job openings the same way that sleazy salesmen hock used cars. These recruiting companies need to radically change how they do business with the technical crowd, and the Clueless Recruiters series is an attempt to call out clueless technical recruiters who contact me for jobs that are clearly a terrible fit. Everything here is posted from real exchanges between myself and recruiters, entirely uncut. Enjoy!\n Recruiter Schlock Here\u0026rsquo;s one I got a few weeks ago from a clueless recruiter. For reference, here is my résumé.\nNormally, I don\u0026rsquo;t say which companies these come from, but in this case, I\u0026rsquo;ll make an exception because I used to work there — Amazon.\n Ryan,\nYes, I am fully aware you receive endless messages from recruiters (especially other Amazonians). I apologize for just adding to that noise.\nYou’re probably reading a lot of spam from those individuals. And you’re probably not enjoying yourself. I’m writing this feeling like one of them, and I’m not enjoying myself, either. So, let me cut to the chase.\n A recruiter, making an attempt to set themselves apart by sympathizing with me. OK, ok, that\u0026rsquo;s cool.\n Your background rocks, you have exp. with mobile application development and you appear to have committed to repos on Github (like “swift4.1” for example – which it looks like you made a pull request!) that line up with our needs on [name of team and department] (I’ll get into that in a second).\n *sigh*.\nI have a single repository in GitHub where I have attempted to learn Swift by porting a small CLI application to it. Yes it uses Swift, but there is nothing mobile about it. It\u0026rsquo;s a CLI app for macOS.\nAnd of course I made a pull request. I\u0026rsquo;m the sole author.\n I won’t pretend that my message is different from the rest you’re receiving. But, know that it is from a real person who researched your background and found it highly compelling.\n You really didn\u0026rsquo;t. AT. ALL.\n I recruit for our [name of team and department] (super cool stuff I’ll explain over the phone) and [name of team and department] (Android and iOS roles).\nWe’re looking for leaders (both on the IC side and on the management side).\nWe’re essentially building an entirely new way to communicate with other people a background like yours could be a huge factor in driving this new venture.\nWould you be open to me giving you a bit of an overview of the cool stuff we’re doing and see if it piques your interest?\nJust FYI, we’re seeking these roles in Sunnyvale, Santa Cruz, Santa Monica, Toronto and Seattle.\n OK. I\u0026rsquo;ve reached a point in my career where these kinds of conversations happen, sure.\nBut this pitch is about as truthful as Donald Trump standing at a podium talking to a crowd of cheering deplorables. Actually, in my mind, I imagine this recruiter to be something like Brian, the chipper waiter from Office Space.\n  I ignored this email (as I often do) as simply not being worth the brain-space required to reply.\nThe Follow-Up  Ryan,\nI shot you an email a few weeks back regarding my interest in your background and how it could translate to our [redacted] team(s).\nI really liked digging through your GitHub profile as well as LinkedIn. It was quite apparent you have deep experience with mobile application development (via repo “swift4.1”).\n  I… (pause…) Not worth it.\n  Would you have any interest in hearing about what we’re doing and the awesome stuff we’re building\u0026hellip;..and see if it piques your interest?\n  We’re building an entirely new way to communicate with other people and are in big need of individuals with such an awesome background like yourself. I’m searching for lead engineers (individual contributors) as well as managers for several teams.\nIf you’re not interested, no worries and I’m happy you’re in a great place right now. Just let me know and I’ll not reach out again.\nHowever, if you do have a little interest in learning about us, I would love to speak sometime.\nThanks Ryan!\n I’m never going back to Amazon ever again. Sorry.\nEpilogue Now, I will be the first to admit that this particular recruiter was not nearly as bad as the other recruiters I\u0026rsquo;ve skewered in this series. This person seems fairly reasonable on the surface, but still made the cardinal mistake of not doing their homework. At best, they did a quick search for a keyword — which was fairly obvious by the beginning of the note.\nRecruiters: Do your homework. I know that you have other pressures in your job with regard to filling open reqs within certain timeframes, but you still need to do your homework.\n",
        "source":"\n{{\u003cdescription\u003e}}\nWhen recruiters have absolutely no understanding about the things that they're recruiting for! In this episode of \u003cem\u003eClueless Recruiters\u003c/em\u003e! (Cue theme music!)\n{{\u003c/description\u003e}}\n\n## Explanation\n\n{{% aside %}}\nThere are few things that technical people are more annoyed by than technical recruiters. A very large segment of the technical recruiting industry has made a bad name for the rest of their industry by relentlessly spamming technical professionals after having [not done their homework](/2011/11/05/how-to-make-technical-professionals-not-hate-your-guts-a-guide-for-technical-recruiters/).\n\nThese people hock job openings the same way that sleazy salesmen hock used cars. These recruiting companies need to radically change how they do business with the technical crowd, and the [Clueless Recruiters](/categories/clueless-recruiters/) series is an attempt to call out clueless technical recruiters who contact me for jobs that are clearly a terrible fit. Everything here is posted from *real* exchanges between myself and recruiters, entirely uncut. Enjoy!\n{{% /aside %}}\n\n## Recruiter Schlock\n\nHere's one I got a few weeks ago from a *clueless recruiter*. For reference, here is my [résumé](http://ryanparman.com/resume/).\n\nNormally, I don't say which companies these come from, but in this case, I'll make an exception because I used to work there — Amazon.\n\n\u003e Ryan,\n\u003e\n\u003e Yes, I am fully aware you receive endless messages from recruiters (especially other Amazonians). I apologize for just adding to that noise.\n\u003e\n\u003e You’re probably reading a lot of spam from those individuals. And you’re probably not enjoying yourself. I’m writing this feeling like one of them, and I’m not enjoying myself, either. So, let me cut to the chase.\n\nA recruiter, making an attempt to set themselves apart by sympathizing with me. OK, ok, that's cool.\n\n\u003e Your background rocks, you have exp. with mobile application development and you appear to have committed to repos on Github (like “swift4.1” for example – which it looks like you made a pull request!) that line up with our needs on \\[name of team and department\\] (I’ll get into that in a second).\n\n_\\*sigh\\*._\n\nI have a single repository in GitHub where I have attempted to learn Swift by porting a small CLI application to it. Yes it uses Swift, but there is nothing mobile about it. It's a CLI app for macOS.\n\nAnd of course I made a pull request. I'm the _sole author_.\n\n\u003e I won’t pretend that my message is different from the rest you’re receiving.  But, know that it is from a real person who researched your background and found it highly compelling.\n\nYou really didn't. AT. ALL.\n\n\u003e I recruit for our \\[name of team and department\\] (super cool stuff I’ll explain over the phone) and \\[name of team and department\\] (Android and iOS roles).\n\u003e \n\u003e We’re looking for leaders (both on the IC side and on the management side).\n\u003e \n\u003e We’re essentially building an entirely new way to communicate with other people a background like yours could be a huge factor in driving this new venture.\n\u003e \n\u003e Would you be open to me giving you a bit of an overview of the cool stuff we’re doing and see if it piques your interest?\n\u003e\n\u003e Just FYI, we’re seeking these roles in Sunnyvale, Santa Cruz, Santa Monica, Toronto and Seattle.\n\nOK. I've reached a point in my career where these kinds of conversations happen, sure.\n\nBut this pitch is about as truthful as {{\u003cperson name=\"Donald Trump\"\u003e}} standing at a podium talking to a crowd of cheering deplorables. Actually, in my mind, I imagine this recruiter to be something like Brian, the chipper waiter from [Office Space](https://letterboxd.com/film/office-space/).\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/office-space-waiter.jpg\" alt=\"Brian, the waiter from Office Space\"\u003e}}\n\nI ignored this email (as I often do) as simply not being worth the brain-space required to reply.\n\n## The Follow-Up\n\n\u003e Ryan,\n\u003e \n\u003e I shot you an email a few weeks back regarding my interest in your background and how it could translate to our \\[redacted\\] team(s).\n\u003e \n\u003e I really liked digging through your GitHub profile as well as LinkedIn. It was quite apparent you have deep experience with mobile application development (via repo “swift4.1”).\n\n{{\u003cfullimage-noshadow src=\"https://cdn.ryanparman.com/hugo/posts/2018/finger-up.jpg\" alt=\"I… (pause…) Not worth it.\" figure=\"I… (pause…) Not worth it.\"\u003e}}\n\n\u003e Would you have any interest in hearing about what we’re doing and the awesome stuff we’re building.....and see if it piques your interest?\n\n\u003e We’re building an entirely new way to communicate with other people and are in big need of individuals with such an awesome background like yourself. I’m searching for lead engineers (individual contributors) as well as managers for several teams.\n\u003e \n\u003e If you’re not interested, no worries and I’m happy you’re in a great place right now. Just let me know and I’ll not reach out again.\n\u003e \n\u003e However, if you do have a little interest in learning about us, I would love to speak sometime.\n\u003e \n\u003e Thanks Ryan!\n\nI’m never going back to Amazon ever again. Sorry.\n\n## Epilogue\n\nNow, I will be the first to admit that this particular recruiter was not nearly as bad as the other recruiters I've skewered in this series. This person seems fairly reasonable on the surface, but still made the _cardinal mistake_ of **not doing their homework**. At best, they did a quick search for a keyword — which was fairly obvious by the beginning of the note.\n\n**Recruiters:** _Do your homework_. I know that you have other pressures in your job with regard to filling open reqs within certain timeframes, but you still need to _do your homework_.\n"},
    "links": {
        "prev": {"title": "Creating Smaller Docker Containers for Your Apps", "permalink": "https://ryanparman.com/posts/2018/creating-smaller-docker-containers-for-your-apps/"},
        "next": {"title": "Serving Bandwidth-Friendly Video with HTTP Live Streaming (HLS)", "permalink": "https://ryanparman.com/posts/2018/serving-bandwidth-friendly-video-with-hls/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "Creating Smaller Docker Containers for Your Apps",
    "description": "When it comes to Docker containers, the smaller, the better.",
    "summary": {
        "content": "When it comes to Docker containers, the smaller, the better. Smaller containers are easier to work with, deploy faster, and tend to have fewer security vulnerabilities. This piece is part of a larger series on Engineering for Site Reliability, specifically Docker. Big is Bad I worked at WePay during the transition from a monolithic application in the datacenter to a series of microservices running in the cloud. I spent a lot of time working on the Vagrant-based CentOS development environment for the monolith, and also started maintaining a custom CentOS base image in Google Cloud.",
        "isTruncated": true
    },
    "published": "2018-08-16T16:27:52Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/creating-smaller-docker-containers-for-your-apps/",
    "relativePermalink": "/posts/2018/creating-smaller-docker-containers-for-your-apps/",
    "aliases": ["/2018/08/16/creating-very-small-docker-containers-for-php-apps"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/docker-logo.jpg", "https://cdn.ryanparman.com/hugo/headers/docker/dockerconeu.jpg"],
    "videos": [],
    "categories": ["Engineering for Site Reliability"],
    "tags": ["docker", "alpine linux", "centos", "ubuntu", "php", "nginx", "composer", "pip", "npm", "java", "glibc", "musl", "cve"],
    "series": ["Engineering for Site Reliability"],
    "keywords": [],
    "meta": {
        "wordCount": 1980,
        "readingTime": "10 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20180816-creating-smaller-docker-containers-for-your-apps.md",
        "logicalName": "20180816-creating-smaller-docker-containers-for-your-apps.md",
        "translationBaseName": "20180816-creating-smaller-docker-containers-for-your-apps",
        "baseFileName": "20180816-creating-smaller-docker-containers-for-your-apps",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#big-is-bad\"\u003eBig is Bad\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#smaller-is-better\"\u003eSmaller is better\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#use-alpine-linux\"\u003eUse Alpine Linux\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#learn-to-love-the-layer-cache\"\u003eLearn to love the layer cache\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#installed-dependencies-should-be-runtime-only\"\u003eInstalled dependencies should be runtime-only\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#flattening-your-base-images\"\u003eFlattening your (base) images\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#reduced-security-vulnerabilities\"\u003eReduced security vulnerabilities\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nWhen it comes to Docker containers, the smaller, the better. Smaller containers are easier to work with, deploy faster, and tend to have fewer security vulnerabilities.\n\u003c/p\u003e\n\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/docker-logo.webp\" alt=\"Docker Logo\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/docker-logo.jpg\" alt=\"Docker Logo\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003eThis piece is part of a larger series on \u003ca href=\"/series/engineering-for-site-reliability/\"\u003eEngineering for Site Reliability\u003c/a\u003e, specifically \u003cem\u003eDocker\u003c/em\u003e.\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003ch2 id=\"big-is-bad\"\u003eBig is Bad\u003c/h2\u003e\n\u003cp\u003eI worked at \u003ca href=\"https://web.archive.org/web/20180816162752/https://wepay.com\"\u003eWePay\u003c/a\u003e during the transition from a monolithic application in the datacenter to a series of microservices running in the cloud. I spent a lot of time working on the Vagrant-based CentOS development environment for the monolith, and also started maintaining a custom CentOS base image in Google Cloud.\u003c/p\u003e\n\u003cp\u003eAs we were all learning about Docker, images, containers, and how it all worked together, the director of DevOps declared (unilaterally) that we should create Docker base images for the various languages we were using (PHP, Python, Java, Go), and they should all be built on a core CentOS 7 Docker image.\u003c/p\u003e\n\u003cp\u003eNow, many parts of this make sense:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eHaving a base disk image for our hosts that builds-in all of the shared functionality we needed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHaving a base Docker image that every application referenced with \u003ccode\u003eFROM\u003c/code\u003e in their \u003ccode\u003eDockerfiles\u003c/code\u003e which included shared patterns for logging and metrics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHaving an optimized image for specific languages made it easier for developers using those languages to rapidly spin-up new application containers.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut there were also some major drawbacks to this approach.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDevelopers wanted to run \u003ccode\u003ecomposer install\u003c/code\u003e and \u003ccode\u003epip install requirements.txt\u003c/code\u003e from \u003cem\u003einside the container\u003c/em\u003e. This often required \u003cem\u003edevelopment dependencies\u003c/em\u003e to be installed in the containers.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOne of our Java micro-service applications (CentOS 7 + Oracle Java + application code + development dependencies) clocked in at \u003cstrong\u003e1.8 GB\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOur build system was frequently buckling under the weight of caching and transferring large Docker images between its cluster and our Artifactory installation.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/fat-guy.webp\" alt=\"Fat Guy eating a donut and cheese-whiz\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/fat-guy.jpg\" alt=\"Fat Guy eating a donut and cheese-whiz\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eNow, some of this can be chalked up to learning a new technology. Some of these are growing pains that were incurred at the same time as chunking apart our monolithic PHP app into Java/Python/Golang microservices. Some of this was hubris by people who made unilateral decisions. But we\u0026rsquo;d made it to the cloud. We\u0026rsquo;d made it to microservices. And I\u0026rsquo;m sure that WePay\u0026rsquo;s development practices have improved greatly over the last couple of years since I left.\u003c/p\u003e\n\u003ch2 id=\"smaller-is-better\"\u003eSmaller is better\u003c/h2\u003e\n\u003cp\u003eIn my current gig, my team has gone all-in with Docker, the AWS cloud, Infrastructure-as-Code, CI/CD practices, and the SRE support model. I\u0026rsquo;ll spend some time talking about these other topics in a future post, but I do want to talk about some process magic that makes it nearly effortless to deploy to Production multiple times per day with exceptionally little stress.\u003c/p\u003e\n\u003ch3 id=\"use-alpine-linux\"\u003eUse Alpine Linux\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://web.archive.org/web/20180816162752/https://alpinelinux.org\"\u003eAlpine Linux\u003c/a\u003e is the \u003cstrong\u003e5 MB\u003c/strong\u003e successor to \u003ca href=\"https://web.archive.org/web/20180816162752/https://www.busybox.net\"\u003eBusybox\u003c/a\u003e, which provides a few additional tools to Busybox’s \u003cstrong\u003e2 MB\u003c/strong\u003e image size.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/alpine.webp\" alt=\"Alpine Linux size compared to other Docker images.\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/alpine.png\" alt=\"Alpine Linux size compared to other Docker images.\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eGenerally speaking, \u003cstrong\u003eyou should always use Alpine Linux\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eI use the word \u0026ldquo;generally\u0026rdquo; because there are certain exceptions to this (otherwise) strong recommendation. The most important of which is that while larger Linux distributions which use the GNU’s \u003ca href=\"https://web.archive.org/web/20180816162752/https://www.gnu.org/software/libc/\"\u003eglibc\u003c/a\u003e library for the C Standard Library implementation, Alpine, Busybox, and others use a different library called \u003ca href=\"https://web.archive.org/web/20180816162752/https://musl-libc.org\"\u003emusl\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can take a look at the \u003ca href=\"https://web.archive.org/web/20180816162752/https://wiki.musl-libc.org/functional-differences-from-glibc.html\"\u003edifferences between musl and glibc\u003c/a\u003e, but the part that matters to you is that there is \u003cem\u003esome\u003c/em\u003e software that exists which depends on the non-standard parts of glibc that haven\u0026rsquo;t been implemented in musl yet. What this means, practically speaking, are that things like the \u003ca href=\"https://web.archive.org/web/20180816162752/https://bugs.php.net/74982\"\u003e\u003ccode\u003e%P\u003c/code\u003e marker for \u003ccode\u003estrftime()\u003c/code\u003e doesn\u0026rsquo;t work as documented\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"learn-to-love-the-layer-cache\"\u003eLearn to love the layer cache\u003c/h3\u003e\n\u003cp\u003eDocker images use \u003cem\u003elayers\u003c/em\u003e to overlay newer changes over previous changes using a technology called \u003ca href=\"https://web.archive.org/web/20180816162752/https://en.wikipedia.org/wiki/UnionFS\"\u003eUnionFS\u003c/a\u003e. This works similarly to Git, where all of the changes that ever happened are still inside the repository, but when you pull the \u003ccode\u003emaster\u003c/code\u003e branch, you\u0026rsquo;re pulling down dozens (or \u003cem\u003ehundreds\u003c/em\u003e, or \u003cem\u003ethousands\u003c/em\u003e) of layers that all need to resolve into the current state of the branch.\u003c/p\u003e\n\u003cp\u003eWith Docker, each of these layers is introduced by the \u003ca href=\"https://web.archive.org/web/20180816162752/https://docs.docker.com/engine/reference/builder/#run\"\u003e\u003ccode\u003eRUN\u003c/code\u003e statement\u003c/a\u003e inside a \u003ccode\u003eDockerfile\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-Dockerfile\" data-lang=\"Dockerfile\"\u003e\u003cspan style=\"color:#66d9ef\"\u003eFROM\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e nginx:1.15.1-alpine\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e RUNTIME_DEPS ca-certificates curl\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://dl-cdn.alpinelinux.org/alpine/v3.7/main\u0026#34;\u003c/span\u003e \u0026gt;\u0026gt; /etc/apk/repositories\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk upgrade --no-cache --update\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPS\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e chmod -Rf \u003cspan style=\"color:#ae81ff\"\u003e0777\u003c/span\u003e /var/log\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e...\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf a change is made to an earlier layer, then the layer cache is invalidated for all of the later layers, and those later layers need to be rebuilt. (Because of this, it\u0026rsquo;s always a good idea to put \u003cem\u003einfrequently\u003c/em\u003e changed commands first, and \u003cem\u003emore frequently\u003c/em\u003e changed commands last.)\u003c/p\u003e\n\u003cp\u003eUnfortunately, many people (including myself) read that Docker image layers have filesize overhead built into them. In order to make your containers smaller, you should combine all of your commands into a single \u003ccode\u003eRUN\u003c/code\u003e statement. The side effect is that any time you need to change \u003cem\u003eanything\u003c/em\u003e inside that \u003ccode\u003eRUN\u003c/code\u003e statement, Docker needs to rebuild everything from scratch — since it\u0026rsquo;s all in the same layer (which changed).\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-Dockerfile\" data-lang=\"Dockerfile\"\u003e\u003cspan style=\"color:#66d9ef\"\u003eFROM\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e nginx:1.15.1-alpine\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e RUNTIME_DEPS ca-certificates curl\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://dl-cdn.alpinelinux.org/alpine/v3.7/main\u0026#34;\u003c/span\u003e \u0026gt;\u0026gt; /etc/apk/repositories \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    apk upgrade --no-cache --update \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPS \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    chmod -Rf \u003cspan style=\"color:#ae81ff\"\u003e0777\u003c/span\u003e /var/log\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e...\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBy leveraging the \u003ccode\u003eRUN\u003c/code\u003e statement as it was intended, you get to take advantage of faster re-build times by leveraging the \u003cem\u003elayer cache\u003c/em\u003e. This means that any layers (e.g., \u003ccode\u003eRUN\u003c/code\u003e statements) which haven\u0026rsquo;t changed since the last build do not need to be built again!\u003c/p\u003e\n\u003cp\u003eYes, your \u003cem\u003edevelopment\u003c/em\u003e Docker image may be a little larger, but we will address this later in this post.\u003c/p\u003e\n\u003ch3 id=\"installed-dependencies-should-be-runtime-only\"\u003eInstalled dependencies should be runtime-only\u003c/h3\u003e\n\u003cp\u003eThis is the one that kills me the most because it can be so wasteful, and it stems from not understanding how to use the tools in your toolbox.\u003c/p\u003e\n\u003cp\u003eFirstly, use a \u003ca href=\"https://web.archive.org/web/20180816162752/https://docs.docker.com/engine/reference/builder/#dockerignore-file\"\u003e\u003ccode\u003e.dockerignore\u003c/code\u003e file\u003c/a\u003e. Again, this is very similar to how a \u003ccode\u003e.gitignore\u003c/code\u003e file works — you don\u0026rsquo;t need everything you use for development to end up inside your Docker image, so use \u003ccode\u003e.dockerignore\u003c/code\u003e to avoid development dependencies.\u003c/p\u003e\n\u003cp\u003eYou never need your \u003ccode\u003e.git/\u003c/code\u003e directory to be copied into a Docker image. Once you\u0026rsquo;ve resolved your application dependencies, you also don\u0026rsquo;t need your \u003ccode\u003ecomposer.json\u003c/code\u003e, \u003ccode\u003epackage.json\u003c/code\u003e, \u003ccode\u003erequirements.txt\u003c/code\u003e, or other package manager definitions in there. You only need your vendored code. (Even then, you don\u0026rsquo;t need the tests for the vendored code either, most of the time. You should ignore those as well.)\u003c/p\u003e\n\u003cp\u003eSome dependencies need to build binaries for the OS they\u0026rsquo;re running inside of. For example, Node.js apps often rely on \u003cem\u003eOniguruma\u003c/em\u003e. Many Python applications rely on \u003cem\u003eMySQLdb\u003c/em\u003e. Both of these require that you install compilation tools and compile them on the OS that they run in.\u003c/p\u003e\n\u003cp\u003eSome companies solve this problem by \u003cem\u003einstalling GCC inside the Docker image\u003c/em\u003e.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"video/mp4\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/mcfly-confused.mp4\" alt=\"Marty McFly looking very confused.\" class=\"db fullimage\" decoding=\"async\"\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/mcfly-confused.webp\" alt=\"Marty McFly looking very confused.\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/mcfly-confused.gif\" alt=\"Marty McFly looking very confused.\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eA better solution is to have \u003cem\u003ebuild-time\u003c/em\u003e and \u003cem\u003erun-time\u003c/em\u003e dependencies, wherein you uninstall the build-time dependencies once you\u0026rsquo;re done with them.\u003c/p\u003e\n\u003cp\u003eHere is an example of a PHP app that includes Redis support and installs the New Relic agent extension.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-Dockerfile\" data-lang=\"Dockerfile\"\u003e\u003cspan style=\"color:#66d9ef\"\u003eFROM\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e php:7.2.8-fpm-alpine3.7\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Needed at build-time, then can be uninstalled.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e BUILD_DEPS alpine-sdk coreutils wget git autoconf re2c\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Should remain inside the container for runtime purposes.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e PERSISTENT_DEPS net-tools hiredis-dev gmp-dev\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# PHP extensions to install.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e INSTALL_EXTENSIONS gmp json opcache pdo pdo_mysql\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# New Relic values.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e NR_INSTALL_SILENT \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eENV\u003c/span\u003e NR_VERSION 8.0.0.204\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Update the packages in the container to their latest security patches.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk upgrade --no-cache --update\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Install your build-time and runtime dependencies.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Give these groups of dependencies names like `.build-deps`\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# and `.persistent-deps` that we can refer to later.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk add --no-cache --virtual .build-deps $BUILD_DEPS\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk add --no-cache --virtual .persistent-deps $PERSISTENT_DEPS\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Install the PHP extensions we need from the PHP repository.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# https://github.com/php/php-src/tree/master/ext\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e docker-php-ext-install $INSTALL_EXTENSIONS\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Install the New Relic agent extension for PHP.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e wget -O /tmp/newrelic-php5.tar.gz https://download.newrelic.com/php_agent/archive/$NR_VERSION/newrelic-php5-$NR_VERSION-linux-musl.tar.gz\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e tar -zxvf /tmp/newrelic-php5.tar.gz -C /usr/local/lib\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e /usr/local/lib/newrelic*/newrelic-install install\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e rm /usr/local/etc/php/conf.d/newrelic.ini\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Install the phpiredis extension for PHP.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e git clone https://github.com/nrk/phpiredis.git \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e cd phpiredis \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e phpize \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e ./configure --enable-phpiredis \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e make \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e make install\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e# Uninstall the grouping of dependencies called `.build-deps`.\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eRUN\u003c/span\u003e apk del .build-deps\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u003c/span\u003e...\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"flattening-your-base-images\"\u003eFlattening your (base) images\u003c/h3\u003e\n\u003cp\u003eFlattening your images is an extra step that you can take to make your images as small as possible. This is particularly useful if you are building/providing \u003cem\u003ebase\u003c/em\u003e images for other people to consume downstream.\u003c/p\u003e\n\u003cp\u003eAs Thomas Uhrig \u003ca href=\"https://web.archive.org/web/20180816162752/https://tuhrig.de/flatten-a-docker-container-or-image/\"\u003ewrites\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe can use this mechanism to flatten and shrink a Docker container. If we save an image to the disk, its whole history will be preserved, but if we export a container, its history gets lost and the resulting tarball will be much smaller.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"color:#75715e\"\u003e# Launch the container from a Docker image\u003c/span\u003e\ndocker run \u0026lt;image\u0026gt; --detach\n\n\u003cspan style=\"color:#75715e\"\u003e# Export the running container to a tarball\u003c/span\u003e\ndocker export \u0026lt;container\u0026gt; \u0026gt; /tmp/docker-image.tar\n \n\u003cspan style=\"color:#75715e\"\u003e# Import it back into Docker\u003c/span\u003e\ncat /tmp/docker-image.tar | docker import - php-fpm:without-layers\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBy running a container and exporting the data as a tarball, you can remove all of the intermediate layers and history from the final image, removing filesize overhead and reducing the overall image size.\u003c/p\u003e\n\u003cp\u003eAt the time of this writing, the latest version of \u003ca href=\"https://web.archive.org/web/20180816162752/https://store.docker.com/images/php\"\u003ePHP is 7.2.8\u003c/a\u003e (actually, 7.2.9 was cut yesterday, but the updated image hasn\u0026rsquo;t been released yet), which builds on top of the \u003ca href=\"https://web.archive.org/web/20180816162752/https://store.docker.com/images/alpine\"\u003eAlpine Linux 3.7\u003c/a\u003e image.\u003c/p\u003e\n\u003cp\u003eThe Alpine Linux image clocks in at just under \u003cstrong\u003e5 MB\u003c/strong\u003e. The PHP image adds a few layers, and brings things up to \u003cstrong\u003e78 MB\u003c/strong\u003e. So far, both of these are smaller than the base CentOS or Ubuntu images.\u003c/p\u003e\n\u003cp\u003eOur application includes the New Relic agent for PHP, a few extensions, our application code, and our Composer \u003ccode\u003evendor\u003c/code\u003e directory (without dev-dependencies).\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003ecomposer install --prefer-dist --no-dev\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWe \u003cem\u003eshould\u003c/em\u003e remove things like tests from our \u003ccode\u003evendor\u003c/code\u003e directory, but we haven\u0026rsquo;t done that yet at the time of this writing. With all of our (wonderfully cached) layers, this brings the decompressed image size to \u003cstrong\u003e408 MB\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAfter stripping out the history and removing all of the individual layers from the image (via a process called \u003cem\u003eflattening\u003c/em\u003e), our final \u003cem\u003edecompressed\u003c/em\u003e image size is a mere \u003cstrong\u003e197 MB\u003c/strong\u003e in size.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-plain\" data-lang=\"plain\"\u003e$ docker images\n\nREPOSITORY     TAG                   IMAGE ID       CREATED          SIZE\nalpine         3.7                   791c3e2ebfcb   5 weeks ago      4.2MB\nphp            7.2.7-fpm-alpine3.7   9cf17fea14c0   5 weeks ago      78.3MB\nphp-fpm        with-layers           94121f6a6537   29 seconds ago   408MB\nphp-fpm        without-layers        8468ea1ee874   4 seconds ago    197MB\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhen you push your image up to a Docker registry (e.g., Docker Hub, Amazon ECR, Google Container Registry, Quay.io, Artifactory), the images will be compressed. Our final Docker image, compressed-at-rest, is only \u003cstrong\u003e72 MB\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eA small, 72 MB Docker image for our application is small and easy enough to push into our CI/CD pipeline in only a few seconds, and puts very little network or storage strain on our internal systems. It\u0026rsquo;s fast to download into my local development environment, and every step of the development and build processes are automated.\u003c/p\u003e\n\u003ch2 id=\"reduced-security-vulnerabilities\"\u003eReduced security vulnerabilities\u003c/h2\u003e\n\u003cp\u003eOver my career, I\u0026rsquo;ve observed that engineers view the topic of \u0026ldquo;security\u0026rdquo; primarily through the lense of their job role.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eApplication engineers tend to view security as things like XSS vulnerabilities and SQL injections.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSystem engineers tend to view security as things like CVEs and intrusions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecurity engineers tend to see those things + TLS certificates + CIS Benchmarks + secrets management and rotation + user permissions + …\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this context, I\u0026rsquo;m referring primarily to \u003cem\u003esecurity vulnerabilties\u003c/em\u003e along the lines of \u003ca href=\"https://web.archive.org/web/20180816162752/http://heartbleed.com\"\u003eHeartbleed\u003c/a\u003e, \u003ca href=\"https://web.archive.org/web/20180816162752/https://nvd.nist.gov/vuln/detail/CVE-2014-6271\"\u003eShellShock\u003c/a\u003e, and \u003ca href=\"https://web.archive.org/web/20180816162752/https://httpoxy.org\"\u003ehttpoxy\u003c/a\u003e. Because there is so little software installed by default, the \u003cem\u003eattack surface\u003c/em\u003e is substantially reduced — oftentimes to the point where there are zero known vulnerabilities anywhere in your application container.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns pv4-l\"\u003e\n\t\u003camp-img src=\"https://cdn.ryanparman.com/hugo/posts/2018/heartbleed.png\" layout=\"responsive\" width=\"\" height=\"\"\u003e\u003c/amp-img\u003e\n    \u003cp class=\"f6 gray tc db\"\u003eLogo for the Heartbleed vulnerability.\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eThis is entirely unheard of in CentOS, Ubuntu, and other larger distributions. As a matter of fact, when our application went live and we underwent review with the security team, they scanned our hosts and containers with zero unpatched vulnerabilities and thought that the scan was bad or their software was broken.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eThe most important things to take away from this are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBig Docker images are a bad thing.\u003c/li\u003e\n\u003cli\u003eUse Alpine Linux. Seriously.\u003c/li\u003e\n\u003cli\u003eRemove your build-time dependencies.\u003c/li\u003e\n\u003cli\u003eFlatten your images if you\u0026rsquo;re sharing them.\u003c/li\u003e\n\u003cli\u003eThe less software that is installed, the fewer security vulnerabilities there will be.\u003c/li\u003e\n\u003cli\u003eMaking your images as small as possible can greatly reduce the burden on the rest of your infrastructure.\u003c/li\u003e\n\u003c/ol\u003e\n",
        "plain":"When it comes to Docker containers, the smaller, the better. Smaller containers are easier to work with, deploy faster, and tend to have fewer security vulnerabilities.   This piece is part of a larger series on Engineering for Site Reliability, specifically Docker.\n Big is Bad I worked at WePay during the transition from a monolithic application in the datacenter to a series of microservices running in the cloud. I spent a lot of time working on the Vagrant-based CentOS development environment for the monolith, and also started maintaining a custom CentOS base image in Google Cloud.\nAs we were all learning about Docker, images, containers, and how it all worked together, the director of DevOps declared (unilaterally) that we should create Docker base images for the various languages we were using (PHP, Python, Java, Go), and they should all be built on a core CentOS 7 Docker image.\nNow, many parts of this make sense:\n  Having a base disk image for our hosts that builds-in all of the shared functionality we needed.\n  Having a base Docker image that every application referenced with FROM in their Dockerfiles which included shared patterns for logging and metrics.\n  Having an optimized image for specific languages made it easier for developers using those languages to rapidly spin-up new application containers.\n  But there were also some major drawbacks to this approach.\n  Developers wanted to run composer install and pip install requirements.txt from inside the container. This often required development dependencies to be installed in the containers.\n  One of our Java micro-service applications (CentOS 7 + Oracle Java + application code + development dependencies) clocked in at 1.8 GB.\n  Our build system was frequently buckling under the weight of caching and transferring large Docker images between its cluster and our Artifactory installation.\n    Now, some of this can be chalked up to learning a new technology. Some of these are growing pains that were incurred at the same time as chunking apart our monolithic PHP app into Java/Python/Golang microservices. Some of this was hubris by people who made unilateral decisions. But we\u0026rsquo;d made it to the cloud. We\u0026rsquo;d made it to microservices. And I\u0026rsquo;m sure that WePay\u0026rsquo;s development practices have improved greatly over the last couple of years since I left.\nSmaller is better In my current gig, my team has gone all-in with Docker, the AWS cloud, Infrastructure-as-Code, CI/CD practices, and the SRE support model. I\u0026rsquo;ll spend some time talking about these other topics in a future post, but I do want to talk about some process magic that makes it nearly effortless to deploy to Production multiple times per day with exceptionally little stress.\nUse Alpine Linux Alpine Linux is the 5 MB successor to Busybox, which provides a few additional tools to Busybox’s 2 MB image size.\n  Generally speaking, you should always use Alpine Linux.\nI use the word \u0026ldquo;generally\u0026rdquo; because there are certain exceptions to this (otherwise) strong recommendation. The most important of which is that while larger Linux distributions which use the GNU’s glibc library for the C Standard Library implementation, Alpine, Busybox, and others use a different library called musl.\nYou can take a look at the differences between musl and glibc, but the part that matters to you is that there is some software that exists which depends on the non-standard parts of glibc that haven\u0026rsquo;t been implemented in musl yet. What this means, practically speaking, are that things like the %P marker for strftime() doesn\u0026rsquo;t work as documented.\nLearn to love the layer cache Docker images use layers to overlay newer changes over previous changes using a technology called UnionFS. This works similarly to Git, where all of the changes that ever happened are still inside the repository, but when you pull the master branch, you\u0026rsquo;re pulling down dozens (or hundreds, or thousands) of layers that all need to resolve into the current state of the branch.\nWith Docker, each of these layers is introduced by the RUN statement inside a Dockerfile.\nFROMnginx:1.15.1-alpineENV RUNTIME_DEPS ca-certificates curlRUN echo \u0026#34;http://dl-cdn.alpinelinux.org/alpine/v3.7/main\u0026#34; \u0026gt;\u0026gt; /etc/apk/repositoriesRUN apk upgrade --no-cache --updateRUN apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPSRUN chmod -Rf 0777 /var/log...If a change is made to an earlier layer, then the layer cache is invalidated for all of the later layers, and those later layers need to be rebuilt. (Because of this, it\u0026rsquo;s always a good idea to put infrequently changed commands first, and more frequently changed commands last.)\nUnfortunately, many people (including myself) read that Docker image layers have filesize overhead built into them. In order to make your containers smaller, you should combine all of your commands into a single RUN statement. The side effect is that any time you need to change anything inside that RUN statement, Docker needs to rebuild everything from scratch — since it\u0026rsquo;s all in the same layer (which changed).\nFROMnginx:1.15.1-alpineENV RUNTIME_DEPS ca-certificates curlRUN echo \u0026#34;http://dl-cdn.alpinelinux.org/alpine/v3.7/main\u0026#34; \u0026gt;\u0026gt; /etc/apk/repositories \u0026amp;\u0026amp; \\  apk upgrade --no-cache --update \u0026amp;\u0026amp; \\  apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPS \u0026amp;\u0026amp; \\  chmod -Rf 0777 /var/log...By leveraging the RUN statement as it was intended, you get to take advantage of faster re-build times by leveraging the layer cache. This means that any layers (e.g., RUN statements) which haven\u0026rsquo;t changed since the last build do not need to be built again!\nYes, your development Docker image may be a little larger, but we will address this later in this post.\nInstalled dependencies should be runtime-only This is the one that kills me the most because it can be so wasteful, and it stems from not understanding how to use the tools in your toolbox.\nFirstly, use a .dockerignore file. Again, this is very similar to how a .gitignore file works — you don\u0026rsquo;t need everything you use for development to end up inside your Docker image, so use .dockerignore to avoid development dependencies.\nYou never need your .git/ directory to be copied into a Docker image. Once you\u0026rsquo;ve resolved your application dependencies, you also don\u0026rsquo;t need your composer.json, package.json, requirements.txt, or other package manager definitions in there. You only need your vendored code. (Even then, you don\u0026rsquo;t need the tests for the vendored code either, most of the time. You should ignore those as well.)\nSome dependencies need to build binaries for the OS they\u0026rsquo;re running inside of. For example, Node.js apps often rely on Oniguruma. Many Python applications rely on MySQLdb. Both of these require that you install compilation tools and compile them on the OS that they run in.\nSome companies solve this problem by installing GCC inside the Docker image.\n  A better solution is to have build-time and run-time dependencies, wherein you uninstall the build-time dependencies once you\u0026rsquo;re done with them.\nHere is an example of a PHP app that includes Redis support and installs the New Relic agent extension.\nFROMphp:7.2.8-fpm-alpine3.7# Needed at build-time, then can be uninstalled.ENV BUILD_DEPS alpine-sdk coreutils wget git autoconf re2c# Should remain inside the container for runtime purposes.ENV PERSISTENT_DEPS net-tools hiredis-dev gmp-dev# PHP extensions to install.ENV INSTALL_EXTENSIONS gmp json opcache pdo pdo_mysql# New Relic values.ENV NR_INSTALL_SILENT 1ENV NR_VERSION 8.0.0.204# Update the packages in the container to their latest security patches.RUN apk upgrade --no-cache --update# Install your build-time and runtime dependencies.# Give these groups of dependencies names like `.build-deps`# and `.persistent-deps` that we can refer to later.RUN apk add --no-cache --virtual .build-deps $BUILD_DEPSRUN apk add --no-cache --virtual .persistent-deps $PERSISTENT_DEPS# Install the PHP extensions we need from the PHP repository.# https://github.com/php/php-src/tree/master/extRUN docker-php-ext-install $INSTALL_EXTENSIONS# Install the New Relic agent extension for PHP.RUN wget -O /tmp/newrelic-php5.tar.gz https://download.newrelic.com/php_agent/archive/$NR_VERSION/newrelic-php5-$NR_VERSION-linux-musl.tar.gzRUN tar -zxvf /tmp/newrelic-php5.tar.gz -C /usr/local/libRUN /usr/local/lib/newrelic*/newrelic-install installRUN rm /usr/local/etc/php/conf.d/newrelic.ini# Install the phpiredis extension for PHP.RUN git clone https://github.com/nrk/phpiredis.git \u0026amp;\u0026amp; cd phpiredis \u0026amp;\u0026amp; phpize \u0026amp;\u0026amp; ./configure --enable-phpiredis \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install# Uninstall the grouping of dependencies called `.build-deps`.RUN apk del .build-deps...Flattening your (base) images Flattening your images is an extra step that you can take to make your images as small as possible. This is particularly useful if you are building/providing base images for other people to consume downstream.\nAs Thomas Uhrig writes:\n We can use this mechanism to flatten and shrink a Docker container. If we save an image to the disk, its whole history will be preserved, but if we export a container, its history gets lost and the resulting tarball will be much smaller.\n # Launch the container from a Docker image docker run \u0026lt;image\u0026gt; --detach # Export the running container to a tarball docker export \u0026lt;container\u0026gt; \u0026gt; /tmp/docker-image.tar # Import it back into Docker cat /tmp/docker-image.tar | docker import - php-fpm:without-layers By running a container and exporting the data as a tarball, you can remove all of the intermediate layers and history from the final image, removing filesize overhead and reducing the overall image size.\nAt the time of this writing, the latest version of PHP is 7.2.8 (actually, 7.2.9 was cut yesterday, but the updated image hasn\u0026rsquo;t been released yet), which builds on top of the Alpine Linux 3.7 image.\nThe Alpine Linux image clocks in at just under 5 MB. The PHP image adds a few layers, and brings things up to 78 MB. So far, both of these are smaller than the base CentOS or Ubuntu images.\nOur application includes the New Relic agent for PHP, a few extensions, our application code, and our Composer vendor directory (without dev-dependencies).\ncomposer install --prefer-dist --no-dev We should remove things like tests from our vendor directory, but we haven\u0026rsquo;t done that yet at the time of this writing. With all of our (wonderfully cached) layers, this brings the decompressed image size to 408 MB.\nAfter stripping out the history and removing all of the individual layers from the image (via a process called flattening), our final decompressed image size is a mere 197 MB in size.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE alpine 3.7 791c3e2ebfcb 5 weeks ago 4.2MB php 7.2.7-fpm-alpine3.7 9cf17fea14c0 5 weeks ago 78.3MB php-fpm with-layers 94121f6a6537 29 seconds ago 408MB php-fpm without-layers 8468ea1ee874 4 seconds ago 197MB When you push your image up to a Docker registry (e.g., Docker Hub, Amazon ECR, Google Container Registry, Quay.io, Artifactory), the images will be compressed. Our final Docker image, compressed-at-rest, is only 72 MB.\nA small, 72 MB Docker image for our application is small and easy enough to push into our CI/CD pipeline in only a few seconds, and puts very little network or storage strain on our internal systems. It\u0026rsquo;s fast to download into my local development environment, and every step of the development and build processes are automated.\nReduced security vulnerabilities Over my career, I\u0026rsquo;ve observed that engineers view the topic of \u0026ldquo;security\u0026rdquo; primarily through the lense of their job role.\n  Application engineers tend to view security as things like XSS vulnerabilities and SQL injections.\n  System engineers tend to view security as things like CVEs and intrusions.\n  Security engineers tend to see those things + TLS certificates + CIS Benchmarks + secrets management and rotation + user permissions + …\n  In this context, I\u0026rsquo;m referring primarily to security vulnerabilties along the lines of Heartbleed, ShellShock, and httpoxy. Because there is so little software installed by default, the attack surface is substantially reduced — oftentimes to the point where there are zero known vulnerabilities anywhere in your application container.\n Logo for the Heartbleed vulnerability.\n This is entirely unheard of in CentOS, Ubuntu, and other larger distributions. As a matter of fact, when our application went live and we underwent review with the security team, they scanned our hosts and containers with zero unpatched vulnerabilities and thought that the scan was bad or their software was broken.\nConclusion The most important things to take away from this are:\n Big Docker images are a bad thing. Use Alpine Linux. Seriously. Remove your build-time dependencies. Flatten your images if you\u0026rsquo;re sharing them. The less software that is installed, the fewer security vulnerabilities there will be. Making your images as small as possible can greatly reduce the burden on the rest of your infrastructure.  ",
        "source":"\n{{\u003cdescription\u003e}}\nWhen it comes to Docker containers, the smaller, the better. Smaller containers are easier to work with, deploy faster, and tend to have fewer security vulnerabilities.\n{{\u003c/description\u003e}}\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/docker-logo.jpg\" alt=\"Docker Logo\"\u003e}}\n\n{{% aside %}}\nThis piece is part of a larger series on [Engineering for Site Reliability](/series/engineering-for-site-reliability/), specifically _Docker_.\n{{% /aside %}}\n\n## Big is Bad\n\nI worked at [WePay]({{\u003c wayback \"https://wepay.com\" \u003e}}) during the transition from a monolithic application in the datacenter to a series of microservices running in the cloud. I spent a lot of time working on the Vagrant-based CentOS development environment for the monolith, and also started maintaining a custom CentOS base image in Google Cloud.\n\nAs we were all learning about Docker, images, containers, and how it all worked together, the director of DevOps declared (unilaterally) that we should create Docker base images for the various languages we were using (PHP, Python, Java, Go), and they should all be built on a core CentOS 7 Docker image.\n\nNow, many parts of this make sense:\n\n1. Having a base disk image for our hosts that builds-in all of the shared functionality we needed.\n\n1. Having a base Docker image that every application referenced with `FROM` in their `Dockerfiles` which included shared patterns for logging and metrics.\n\n1. Having an optimized image for specific languages made it easier for developers using those languages to rapidly spin-up new application containers.\n\nBut there were also some major drawbacks to this approach.\n\n1. Developers wanted to run `composer install` and `pip install requirements.txt` from _inside the container_. This often required _development dependencies_ to be installed in the containers.\n\n1. One of our Java micro-service applications (CentOS 7 + Oracle Java + application code + development dependencies) clocked in at **1.8 GB**.\n\n1. Our build system was frequently buckling under the weight of caching and transferring large Docker images between its cluster and our Artifactory installation.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/fat-guy.jpg\" alt=\"Fat Guy eating a donut and cheese-whiz\"\u003e}}\n\nNow, some of this can be chalked up to learning a new technology. Some of these are growing pains that were incurred at the same time as chunking apart our monolithic PHP app into Java/Python/Golang microservices. Some of this was hubris by people who made unilateral decisions. But we'd made it to the cloud. We'd made it to microservices. And I'm sure that WePay's development practices have improved greatly over the last couple of years since I left.\n\n## Smaller is better\n\nIn my current gig, my team has gone all-in with Docker, the AWS cloud, Infrastructure-as-Code, CI/CD practices, and the SRE support model. I'll spend some time talking about these other topics in a future post, but I do want to talk about some process magic that makes it nearly effortless to deploy to Production multiple times per day with exceptionally little stress.\n\n### Use Alpine Linux\n\n[Alpine Linux]({{\u003c wayback \"https://alpinelinux.org\" \u003e}}) is the **5 MB** successor to [Busybox]({{\u003c wayback \"https://www.busybox.net\" \u003e}}), which provides a few additional tools to Busybox’s **2 MB** image size.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/alpine.png\" alt=\"Alpine Linux size compared to other Docker images.\"\u003e}}\n\nGenerally speaking, **you should always use Alpine Linux**.\n\nI use the word \"generally\" because there are certain exceptions to this (otherwise) strong recommendation. The most important of which is that while larger Linux distributions which use the GNU’s [glibc]({{\u003c wayback \"https://www.gnu.org/software/libc/\" \u003e}}) library for the C Standard Library implementation, Alpine, Busybox, and others use a different library called [musl]({{\u003c wayback \"https://musl-libc.org\" \u003e}}).\n\nYou can take a look at the [differences between musl and glibc]({{\u003c wayback \"https://wiki.musl-libc.org/functional-differences-from-glibc.html\" \u003e}}), but the part that matters to you is that there is _some_ software that exists which depends on the non-standard parts of glibc that haven't been implemented in musl yet. What this means, practically speaking, are that things like the [`%P` marker for `strftime()` doesn't work as documented]({{\u003c wayback \"https://bugs.php.net/74982\" \u003e}}).\n\n### Learn to love the layer cache\n\nDocker images use _layers_ to overlay newer changes over previous changes using a technology called [UnionFS]({{\u003c wayback \"https://en.wikipedia.org/wiki/UnionFS\" \u003e}}). This works similarly to Git, where all of the changes that ever happened are still inside the repository, but when you pull the `master` branch, you're pulling down dozens (or _hundreds_, or _thousands_) of layers that all need to resolve into the current state of the branch.\n\nWith Docker, each of these layers is introduced by the [`RUN` statement]({{\u003c wayback \"https://docs.docker.com/engine/reference/builder/#run\" \u003e}}) inside a `Dockerfile`.\n\n```Dockerfile\nFROM nginx:1.15.1-alpine\n\nENV RUNTIME_DEPS ca-certificates curl\n\nRUN echo \"http://dl-cdn.alpinelinux.org/alpine/v3.7/main\" \u003e\u003e /etc/apk/repositories\nRUN apk upgrade --no-cache --update\nRUN apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPS\nRUN chmod -Rf 0777 /var/log\n\n...\n```\n\nIf a change is made to an earlier layer, then the layer cache is invalidated for all of the later layers, and those later layers need to be rebuilt. (Because of this, it's always a good idea to put _infrequently_ changed commands first, and _more frequently_ changed commands last.)\n\nUnfortunately, many people (including myself) read that Docker image layers have filesize overhead built into them. In order to make your containers smaller, you should combine all of your commands into a single `RUN` statement. The side effect is that any time you need to change _anything_ inside that `RUN` statement, Docker needs to rebuild everything from scratch — since it's all in the same layer (which changed).\n\n```Dockerfile\nFROM nginx:1.15.1-alpine\n\nENV RUNTIME_DEPS ca-certificates curl\n\nRUN echo \"http://dl-cdn.alpinelinux.org/alpine/v3.7/main\" \u003e\u003e /etc/apk/repositories \u0026\u0026 \\\n    apk upgrade --no-cache --update \u0026\u0026 \\\n    apk add --no-cache --virtual .runtime-deps $RUNTIME_DEPS \u0026\u0026 \\\n    chmod -Rf 0777 /var/log\n\n...\n```\n\nBy leveraging the `RUN` statement as it was intended, you get to take advantage of faster re-build times by leveraging the _layer cache_. This means that any layers (e.g., `RUN` statements) which haven't changed since the last build do not need to be built again!\n\nYes, your _development_ Docker image may be a little larger, but we will address this later in this post.\n\n### Installed dependencies should be runtime-only\n\nThis is the one that kills me the most because it can be so wasteful, and it stems from not understanding how to use the tools in your toolbox.\n\nFirstly, use a [`.dockerignore` file]({{\u003c wayback \"https://docs.docker.com/engine/reference/builder/#dockerignore-file\" \u003e}}). Again, this is very similar to how a `.gitignore` file works — you don't need everything you use for development to end up inside your Docker image, so use `.dockerignore` to avoid development dependencies.\n\nYou never need your `.git/` directory to be copied into a Docker image. Once you've resolved your application dependencies, you also don't need your `composer.json`, `package.json`, `requirements.txt`, or other package manager definitions in there. You only need your vendored code. (Even then, you don't need the tests for the vendored code either, most of the time. You should ignore those as well.)\n\nSome dependencies need to build binaries for the OS they're running inside of. For example, Node.js apps often rely on _Oniguruma_. Many Python applications rely on _MySQLdb_. Both of these require that you install compilation tools and compile them on the OS that they run in.\n\nSome companies solve this problem by _installing GCC inside the Docker image_.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/mcfly-confused.gif\" mp4=true alt=\"Marty McFly looking very confused.\"\u003e}}\n\nA better solution is to have _build-time_ and _run-time_ dependencies, wherein you uninstall the build-time dependencies once you're done with them.\n\nHere is an example of a PHP app that includes Redis support and installs the New Relic agent extension.\n\n```Dockerfile\nFROM php:7.2.8-fpm-alpine3.7\n\n# Needed at build-time, then can be uninstalled.\nENV BUILD_DEPS alpine-sdk coreutils wget git autoconf re2c\n\n# Should remain inside the container for runtime purposes.\nENV PERSISTENT_DEPS net-tools hiredis-dev gmp-dev\n\n# PHP extensions to install.\nENV INSTALL_EXTENSIONS gmp json opcache pdo pdo_mysql\n\n# New Relic values.\nENV NR_INSTALL_SILENT 1\nENV NR_VERSION 8.0.0.204\n\n# Update the packages in the container to their latest security patches.\nRUN apk upgrade --no-cache --update\n\n# Install your build-time and runtime dependencies.\n# Give these groups of dependencies names like `.build-deps`\n# and `.persistent-deps` that we can refer to later.\nRUN apk add --no-cache --virtual .build-deps $BUILD_DEPS\nRUN apk add --no-cache --virtual .persistent-deps $PERSISTENT_DEPS\n\n# Install the PHP extensions we need from the PHP repository.\n# https://github.com/php/php-src/tree/master/ext\nRUN docker-php-ext-install $INSTALL_EXTENSIONS\n\n# Install the New Relic agent extension for PHP.\nRUN wget -O /tmp/newrelic-php5.tar.gz https://download.newrelic.com/php_agent/archive/$NR_VERSION/newrelic-php5-$NR_VERSION-linux-musl.tar.gz\nRUN tar -zxvf /tmp/newrelic-php5.tar.gz -C /usr/local/lib\nRUN /usr/local/lib/newrelic*/newrelic-install install\nRUN rm /usr/local/etc/php/conf.d/newrelic.ini\n\n# Install the phpiredis extension for PHP.\nRUN git clone https://github.com/nrk/phpiredis.git \u0026\u0026 cd phpiredis \u0026\u0026 phpize \u0026\u0026 ./configure --enable-phpiredis \u0026\u0026 make \u0026\u0026 make install\n\n# Uninstall the grouping of dependencies called `.build-deps`.\nRUN apk del .build-deps\n\n...\n```\n\n### Flattening your (base) images\n\nFlattening your images is an extra step that you can take to make your images as small as possible. This is particularly useful if you are building/providing _base_ images for other people to consume downstream.\n\nAs Thomas Uhrig [writes]({{\u003c wayback \"https://tuhrig.de/flatten-a-docker-container-or-image/\" \u003e}}):\n\n\u003e We can use this mechanism to flatten and shrink a Docker container. If we save an image to the disk, its whole history will be preserved, but if we export a container, its history gets lost and the resulting tarball will be much smaller.\n\n```bash\n# Launch the container from a Docker image\ndocker run \u003cimage\u003e --detach\n\n# Export the running container to a tarball\ndocker export \u003ccontainer\u003e \u003e /tmp/docker-image.tar\n \n# Import it back into Docker\ncat /tmp/docker-image.tar | docker import - php-fpm:without-layers\n```\n\nBy running a container and exporting the data as a tarball, you can remove all of the intermediate layers and history from the final image, removing filesize overhead and reducing the overall image size.\n\nAt the time of this writing, the latest version of [PHP is 7.2.8]({{\u003c wayback \"https://store.docker.com/images/php\" \u003e}}) (actually, 7.2.9 was cut yesterday, but the updated image hasn't been released yet), which builds on top of the [Alpine Linux 3.7]({{\u003c wayback \"https://store.docker.com/images/alpine\" \u003e}}) image.\n\nThe Alpine Linux image clocks in at just under **5 MB**. The PHP image adds a few layers, and brings things up to **78 MB**. So far, both of these are smaller than the base CentOS or Ubuntu images.\n\nOur application includes the New Relic agent for PHP, a few extensions, our application code, and our Composer `vendor` directory (without dev-dependencies).\n\n```bash\ncomposer install --prefer-dist --no-dev\n```\n\nWe _should_ remove things like tests from our `vendor` directory, but we haven't done that yet at the time of this writing. With all of our (wonderfully cached) layers, this brings the decompressed image size to **408 MB**.\n\nAfter stripping out the history and removing all of the individual layers from the image (via a process called _flattening_), our final _decompressed_ image size is a mere **197 MB** in size.\n\n```plain\n$ docker images\n\nREPOSITORY     TAG                   IMAGE ID       CREATED          SIZE\nalpine         3.7                   791c3e2ebfcb   5 weeks ago      4.2MB\nphp            7.2.7-fpm-alpine3.7   9cf17fea14c0   5 weeks ago      78.3MB\nphp-fpm        with-layers           94121f6a6537   29 seconds ago   408MB\nphp-fpm        without-layers        8468ea1ee874   4 seconds ago    197MB\n\n```\n\nWhen you push your image up to a Docker registry (e.g., Docker Hub, Amazon ECR, Google Container Registry, Quay.io, Artifactory), the images will be compressed. Our final Docker image, compressed-at-rest, is only **72 MB**.\n\nA small, 72 MB Docker image for our application is small and easy enough to push into our CI/CD pipeline in only a few seconds, and puts very little network or storage strain on our internal systems. It's fast to download into my local development environment, and every step of the development and build processes are automated.\n\n## Reduced security vulnerabilities\n\nOver my career, I've observed that engineers view the topic of \"security\" primarily through the lense of their job role.\n\n* Application engineers tend to view security as things like XSS vulnerabilities and SQL injections.\n\n* System engineers tend to view security as things like CVEs and intrusions.\n\n* Security engineers tend to see those things + TLS certificates + CIS Benchmarks + secrets management and rotation + user permissions + …\n\nIn this context, I'm referring primarily to _security vulnerabilties_ along the lines of [Heartbleed]({{\u003c wayback \"http://heartbleed.com\" \u003e}}), [ShellShock]({{\u003c wayback \"https://nvd.nist.gov/vuln/detail/CVE-2014-6271\" \u003e}}), and [httpoxy]({{\u003c wayback \"https://httpoxy.org\" \u003e}}). Because there is so little software installed by default, the _attack surface_ is substantially reduced — oftentimes to the point where there are zero known vulnerabilities anywhere in your application container.\n\n{{\u003cfullimage-noshadow src=\"https://cdn.ryanparman.com/hugo/posts/2018/heartbleed.png\" alt=\"Logo for the Heartbleed vulnerability.\" figure=\"Logo for the Heartbleed vulnerability.\"\u003e}}\n\nThis is entirely unheard of in CentOS, Ubuntu, and other larger distributions. As a matter of fact, when our application went live and we underwent review with the security team, they scanned our hosts and containers with zero unpatched vulnerabilities and thought that the scan was bad or their software was broken.\n\n## Conclusion\n\nThe most important things to take away from this are:\n\n1. Big Docker images are a bad thing.\n1. Use Alpine Linux. Seriously.\n1. Remove your build-time dependencies.\n1. Flatten your images if you're sharing them.\n1. The less software that is installed, the fewer security vulnerabilities there will be.\n1. Making your images as small as possible can greatly reduce the burden on the rest of your infrastructure.\n"},
    "links": {
        "prev": {"title": "Dear Nintendo, Part II", "permalink": "https://ryanparman.com/posts/2018/dear-nintendo-part-ii/"},
        "next": {"title": "Clueless Recruiters, Issue #8", "permalink": "https://ryanparman.com/posts/2018/clueless-recruiters-issue-8/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "Dear Nintendo, Part II",
    "description": "",
    "summary": {
        "content": "After reading Dear Nintendo…, a friend today asked me “How do you feel about the Switch?” Well, let's go down the list. Nintendo Account I can log into my account online, yes. Apparently, Nintendo Network ID is simply an OAuth provider for the rest of the Nintendo website. But you can also have a standalone Nintendo.com login. I deal with this stuff every day, and I still get confused on Nintendo\u0026rsquo;s website. I have a Switch, a Wii U, the original 3DS, the original 3DS XL, and now the new clamshell 2DS XL.",
        "isTruncated": true
    },
    "published": "2018-08-06T02:51:03Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/dear-nintendo-part-ii/",
    "relativePermalink": "/posts/2018/dear-nintendo-part-ii/",
    "aliases": ["/2018/08/05/dear-nintendo-part-ii"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2018/switch1.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/switch-pro-controller.jpg", "https://cdn.ryanparman.com/hugo/posts/2018/apple-tv-4k.jpg"],
    "videos": [],
    "categories": ["Tech Industry"],
    "tags": ["nintendo", "switch", "wii u", "3ds", "2ds", "apple", "microsoft", "sony"],
    "series": ["Editors Choice", "Dear Nintendo"],
    "keywords": [],
    "meta": {
        "wordCount": 892,
        "readingTime": "5 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20180805-dear-nintendo-part-ii.md",
        "logicalName": "20180805-dear-nintendo-part-ii.md",
        "translationBaseName": "20180805-dear-nintendo-part-ii",
        "baseFileName": "20180805-dear-nintendo-part-ii",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#nintendo-account\"\u003eNintendo Account\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#normal-controllers\"\u003eNormal Controllers\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#figure-out-the-first-vs-third-party-game-situation\"\u003eFigure out the first vs. third-party game situation\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#embrace-digital-downloads\"\u003eEmbrace Digital Downloads\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#pay-attention-to-your-competition\"\u003ePay Attention to your Competition\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nAfter reading \u003ca href=\"/2016/09/05/dear-nintendo\"\u003eDear Nintendo…\u003c/a\u003e, a friend today asked me “How do you feel about the Switch?” Well, let's go down the list.\n\u003c/p\u003e\n\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/switch1.webp\" alt=\"Nintendo Switch\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/switch1.jpg\" alt=\"Nintendo Switch\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"nintendo-account\"\u003eNintendo Account\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eI can log into my account online, yes. Apparently, \u003ca href=\"https://id.nintendo.net\"\u003eNintendo Network ID\u003c/a\u003e is simply an OAuth provider for the rest of the Nintendo website. But you can also have a standalone \u003ca href=\"https://www.nintendo.com\"\u003eNintendo.com login\u003c/a\u003e. I deal with this stuff every day, and I still get confused on Nintendo\u0026rsquo;s website.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI have a Switch, a Wii U, the original 3DS, the original 3DS XL, and now the new clamshell 2DS XL. I am still only allowed to log into one Switch, one Wii U, and one device from the 2DS/3DS mobile family. As such, this issue is still not resolved.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI still cannot log into my U.S. Nintendo account on a Canadian Wii U console. (I haven\u0026rsquo;t tried with the Switch yet.) As such, this issue is still not resolved (as far as I have been able to test).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThey finally joined the account balances across devices. This happened after the Switch shipped.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLastly, games are still tied to consoles instead of accounts. Save data is neither stored in the cloud with my Nintendo Network account, nor can I back it up onto an SD card. So my ideal future where I can throw my device in the ocean, buy a new one, and set it back up is still an impossibility — 18 months after the Switch shipped in the U.S.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUltimately, Nintendo still has the worst online account management system for games behind Apple, Microsoft, and Sony (in that order).\u003c/p\u003e\n\u003ch2 id=\"normal-controllers\"\u003eNormal Controllers\u003c/h2\u003e\n\u003cp\u003eI wasn\u0026rsquo;t sure what the “NX” was going to be, but it turned out to be a hybrid mobile/living room device. I\u0026rsquo;m not a big fan of mobile gaming overall, so my Switch is usually connected to one of the TVs in the house. In the case of the \u003cem\u003eNintendo Switch Pro Controller\u003c/em\u003e, they actually did a very good job. It feels like a quality controller, and uses standard Bluetooth (which is an improvement over the Xbox controller).\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/switch-pro-controller.webp\" alt=\"Nintendo Switch Pro Controller\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/switch-pro-controller.jpg\" alt=\"Nintendo Switch Pro Controller\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"figure-out-the-first-vs-third-party-game-situation\"\u003eFigure out the first vs. third-party game situation\u003c/h2\u003e\n\u003cp\u003eNintendo — with the Switch — has embraced third-party developers with such fervor, that they\u0026rsquo;ve cross into the \u003cem\u003ePerimeter of Wisdom\u003c/em\u003e, then back out the other side.\u003c/p\u003e\n\u003cp\u003eWith the Wii and Wii U, we had a handful of good games for the platform, then 95% \u003cem\u003eshovelware\u003c/em\u003e. With Switch, we\u0026rsquo;ve \u003cem\u003emostly\u003c/em\u003e seen good quality indie releases. But we\u0026rsquo;re also seeing things like \u003ca href=\"https://web.archive.org/web/20180806025103/http://www.ign.com/articles/2018/07/18/developers-publishers-react-to-nintendos-20-30-games-a-week-eshop-initiative\"\u003eNintendo\u0026rsquo;s 20-30 Games-a-Week Eshop Initiative\u003c/a\u003e. This is great for gamers, but terrible for developers. With that many titles coming in, many developers are likening it to the Steam store where there is virtually zero digital \u0026ldquo;shelf space\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eTo use American political figures as an example, I worry that they\u0026rsquo;ve jumped from \u003cem\u003eMitch McConnell\u003c/em\u003e (on the far-right), through the center, and all the way to \u003cem\u003eNanci Pelosi\u003c/em\u003e (on the far-left). Both places are bad places to be.\u003c/p\u003e\n\u003ch2 id=\"embrace-digital-downloads\"\u003eEmbrace Digital Downloads\u003c/h2\u003e\n\u003cp\u003eDuring the Xbox 360 and PS3 generation, I went about half downloads, and half disc for new games. In the time since then, I\u0026rsquo;ve stopped buying discs and still get free digital games, so my collection currently leans squarely on the \u003cem\u003emostly-digital\u003c/em\u003e side.\u003c/p\u003e\n\u003cp\u003eI wanted to do the same thing with the Wii and Wii U, but I couldn’t. I asked Nintendo to go all-in on digital, and allow us to have our Nintendo collections go all-digital. Did I get what I asked for? \u003cem\u003eSort of\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eJust about all Nintendo games are downloadable from the Nintendo Switch eShop, which is great. However… the Switch still only comes with 32 GB of (fast) internal storage, and offers (slower) micro-SD card support for expanded storage. Also, the Nintendo account situation (e.g., being able to re-download games to your second personal console) is still an unmitigated disaster.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s like we asked Nintendo to build us a motorcycle. Stage one should have been a usable, functional \u003cem\u003etricycle\u003c/em\u003e. Instead we got a full-size motorcycle chassis and no engine. In other words, we\u0026rsquo;re somewhere on the path, but none of it works well.\u003c/p\u003e\n\u003ch2 id=\"pay-attention-to-your-competition\"\u003ePay Attention to your Competition\u003c/h2\u003e\n\u003cp\u003eApple owns the largest video game platform in the world, but they don\u0026rsquo;t deserve it because they are \u003cem\u003eterrible\u003c/em\u003e caretakers of it.\u003c/p\u003e\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2018/apple-tv-4k.webp\" alt=\"Apple TV 4K\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2018/apple-tv-4k.jpg\" alt=\"Apple TV 4K\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eI have been predicting since 2011 that Apple could release an Apple TV which would support apps, and have all of the really fantastic account management and app re-downloads available. If they chose to, they could decimate Nintendo in the console space. Apple finally did ship the Apple TV 4 which had a more powerful processor, more RAM, and an SDK to write apps with. But they failed to see the opportunity for gaming sitting in front of their faces, and now they\u0026rsquo;ve done little better than bomb in the living room.\u003c/p\u003e\n\u003cp\u003eWith having ceded the \u003cem\u003egamer\u003c/em\u003e segment of the market to Sony and Microsoft, Nintendo is playing in the \u003cem\u003ecasual\u003c/em\u003e segment along with iOS developers (while Apple is off picking its nose in the corner somewhere).\u003c/p\u003e\n\u003cp\u003eIf Apple ever decides to get their act together on taking gaming seriously, I think that Nintendo would be toast. Apple is a very rich, very powerful competitor hanging out in the same segment as Nintendo is. If Nintendo doesn\u0026rsquo;t realize the precariousness of their market situation yet, they\u0026rsquo;re going to have a very painful ride ahead of them.\u003c/p\u003e\n",
        "plain":"After reading Dear Nintendo…, a friend today asked me “How do you feel about the Switch?” Well, let's go down the list.   Nintendo Account   I can log into my account online, yes. Apparently, Nintendo Network ID is simply an OAuth provider for the rest of the Nintendo website. But you can also have a standalone Nintendo.com login. I deal with this stuff every day, and I still get confused on Nintendo\u0026rsquo;s website.\n  I have a Switch, a Wii U, the original 3DS, the original 3DS XL, and now the new clamshell 2DS XL. I am still only allowed to log into one Switch, one Wii U, and one device from the 2DS/3DS mobile family. As such, this issue is still not resolved.\n  I still cannot log into my U.S. Nintendo account on a Canadian Wii U console. (I haven\u0026rsquo;t tried with the Switch yet.) As such, this issue is still not resolved (as far as I have been able to test).\n  They finally joined the account balances across devices. This happened after the Switch shipped.\n  Lastly, games are still tied to consoles instead of accounts. Save data is neither stored in the cloud with my Nintendo Network account, nor can I back it up onto an SD card. So my ideal future where I can throw my device in the ocean, buy a new one, and set it back up is still an impossibility — 18 months after the Switch shipped in the U.S.\n  Ultimately, Nintendo still has the worst online account management system for games behind Apple, Microsoft, and Sony (in that order).\nNormal Controllers I wasn\u0026rsquo;t sure what the “NX” was going to be, but it turned out to be a hybrid mobile/living room device. I\u0026rsquo;m not a big fan of mobile gaming overall, so my Switch is usually connected to one of the TVs in the house. In the case of the Nintendo Switch Pro Controller, they actually did a very good job. It feels like a quality controller, and uses standard Bluetooth (which is an improvement over the Xbox controller).\n  Figure out the first vs. third-party game situation Nintendo — with the Switch — has embraced third-party developers with such fervor, that they\u0026rsquo;ve cross into the Perimeter of Wisdom, then back out the other side.\nWith the Wii and Wii U, we had a handful of good games for the platform, then 95% shovelware. With Switch, we\u0026rsquo;ve mostly seen good quality indie releases. But we\u0026rsquo;re also seeing things like Nintendo\u0026rsquo;s 20-30 Games-a-Week Eshop Initiative. This is great for gamers, but terrible for developers. With that many titles coming in, many developers are likening it to the Steam store where there is virtually zero digital \u0026ldquo;shelf space\u0026rdquo;.\nTo use American political figures as an example, I worry that they\u0026rsquo;ve jumped from Mitch McConnell (on the far-right), through the center, and all the way to Nanci Pelosi (on the far-left). Both places are bad places to be.\nEmbrace Digital Downloads During the Xbox 360 and PS3 generation, I went about half downloads, and half disc for new games. In the time since then, I\u0026rsquo;ve stopped buying discs and still get free digital games, so my collection currently leans squarely on the mostly-digital side.\nI wanted to do the same thing with the Wii and Wii U, but I couldn’t. I asked Nintendo to go all-in on digital, and allow us to have our Nintendo collections go all-digital. Did I get what I asked for? Sort of.\nJust about all Nintendo games are downloadable from the Nintendo Switch eShop, which is great. However… the Switch still only comes with 32 GB of (fast) internal storage, and offers (slower) micro-SD card support for expanded storage. Also, the Nintendo account situation (e.g., being able to re-download games to your second personal console) is still an unmitigated disaster.\nIt\u0026rsquo;s like we asked Nintendo to build us a motorcycle. Stage one should have been a usable, functional tricycle. Instead we got a full-size motorcycle chassis and no engine. In other words, we\u0026rsquo;re somewhere on the path, but none of it works well.\nPay Attention to your Competition Apple owns the largest video game platform in the world, but they don\u0026rsquo;t deserve it because they are terrible caretakers of it.\n  I have been predicting since 2011 that Apple could release an Apple TV which would support apps, and have all of the really fantastic account management and app re-downloads available. If they chose to, they could decimate Nintendo in the console space. Apple finally did ship the Apple TV 4 which had a more powerful processor, more RAM, and an SDK to write apps with. But they failed to see the opportunity for gaming sitting in front of their faces, and now they\u0026rsquo;ve done little better than bomb in the living room.\nWith having ceded the gamer segment of the market to Sony and Microsoft, Nintendo is playing in the casual segment along with iOS developers (while Apple is off picking its nose in the corner somewhere).\nIf Apple ever decides to get their act together on taking gaming seriously, I think that Nintendo would be toast. Apple is a very rich, very powerful competitor hanging out in the same segment as Nintendo is. If Nintendo doesn\u0026rsquo;t realize the precariousness of their market situation yet, they\u0026rsquo;re going to have a very painful ride ahead of them.\n",
        "source":"\n{{\u003cdescription\u003e}}\nAfter reading \u003ca href=\"/2016/09/05/dear-nintendo\"\u003eDear Nintendo…\u003c/a\u003e, a friend today asked me “How do you feel about the Switch?” Well, let's go down the list.\n{{\u003c/description\u003e}}\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/switch1.jpg\" alt=\"Nintendo Switch\"\u003e}}\n\n## Nintendo Account\n\n1. I can log into my account online, yes. Apparently, [Nintendo Network ID](https://id.nintendo.net) is simply an OAuth provider for the rest of the Nintendo website. But you can also have a standalone [Nintendo.com login](https://www.nintendo.com). I deal with this stuff every day, and I still get confused on Nintendo's website.\n\n1. I have a Switch, a Wii U, the original 3DS, the original 3DS XL, and now the new clamshell 2DS XL. I am still only allowed to log into one Switch, one Wii U, and one device from the 2DS/3DS mobile family. As such, this issue is still not resolved.\n\n1. I still cannot log into my U.S. Nintendo account on a Canadian Wii U console. (I haven't tried with the Switch yet.) As such, this issue is still not resolved (as far as I have been able to test).\n\n1. They finally joined the account balances across devices. This happened after the Switch shipped.\n\n1. Lastly, games are still tied to consoles instead of accounts. Save data is neither stored in the cloud with my Nintendo Network account, nor can I back it up onto an SD card. So my ideal future where I can throw my device in the ocean, buy a new one, and set it back up is still an impossibility — 18 months after the Switch shipped in the U.S.\n\nUltimately, Nintendo still has the worst online account management system for games behind Apple, Microsoft, and Sony (in that order).\n\n## Normal Controllers\n\nI wasn't sure what the “NX” was going to be, but it turned out to be a hybrid mobile/living room device. I'm not a big fan of mobile gaming overall, so my Switch is usually connected to one of the TVs in the house. In the case of the _Nintendo Switch Pro Controller_, they actually did a very good job. It feels like a quality controller, and uses standard Bluetooth (which is an improvement over the Xbox controller).\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/switch-pro-controller.jpg\" alt=\"Nintendo Switch Pro Controller\"\u003e}}\n\n## Figure out the first vs. third-party game situation\n\nNintendo — with the Switch — has embraced third-party developers with such fervor, that they've cross into the _Perimeter of Wisdom_, then back out the other side.\n\nWith the Wii and Wii U, we had a handful of good games for the platform, then 95% _shovelware_. With Switch, we've _mostly_ seen good quality indie releases. But we're also seeing things like [Nintendo's 20-30 Games-a-Week Eshop Initiative]({{\u003c wayback \"http://www.ign.com/articles/2018/07/18/developers-publishers-react-to-nintendos-20-30-games-a-week-eshop-initiative\" \u003e}}). This is great for gamers, but terrible for developers. With that many titles coming in, many developers are likening it to the Steam store where there is virtually zero digital \"shelf space\".\n\nTo use American political figures as an example, I worry that they've jumped from _Mitch McConnell_ (on the far-right), through the center, and all the way to _Nanci Pelosi_ (on the far-left). Both places are bad places to be.\n\n## Embrace Digital Downloads\n\nDuring the Xbox 360 and PS3 generation, I went about half downloads, and half disc for new games. In the time since then, I've stopped buying discs and still get free digital games, so my collection currently leans squarely on the _mostly-digital_ side.\n\nI wanted to do the same thing with the Wii and Wii U, but I couldn’t. I asked Nintendo to go all-in on digital, and allow us to have our Nintendo collections go all-digital. Did I get what I asked for? _Sort of_.\n\nJust about all Nintendo games are downloadable from the Nintendo Switch eShop, which is great. However… the Switch still only comes with 32 GB of (fast) internal storage, and offers (slower) micro-SD card support for expanded storage. Also, the Nintendo account situation (e.g., being able to re-download games to your second personal console) is still an unmitigated disaster.\n\nIt's like we asked Nintendo to build us a motorcycle. Stage one should have been a usable, functional _tricycle_. Instead we got a full-size motorcycle chassis and no engine. In other words, we're somewhere on the path, but none of it works well.\n\n## Pay Attention to your Competition\n\nApple owns the largest video game platform in the world, but they don't deserve it because they are _terrible_ caretakers of it.\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2018/apple-tv-4k.jpg\" alt=\"Apple TV 4K\"\u003e}}\n\nI have been predicting since 2011 that Apple could release an Apple TV which would support apps, and have all of the really fantastic account management and app re-downloads available. If they chose to, they could decimate Nintendo in the console space. Apple finally did ship the Apple TV 4 which had a more powerful processor, more RAM, and an SDK to write apps with. But they failed to see the opportunity for gaming sitting in front of their faces, and now they've done little better than bomb in the living room.\n\nWith having ceded the _gamer_ segment of the market to Sony and Microsoft, Nintendo is playing in the _casual_ segment along with iOS developers (while Apple is off picking its nose in the corner somewhere).\n\nIf Apple ever decides to get their act together on taking gaming seriously, I think that Nintendo would be toast. Apple is a very rich, very powerful competitor hanging out in the same segment as Nintendo is. If Nintendo doesn't realize the precariousness of their market situation yet, they're going to have a very painful ride ahead of them.\n"},
    "links": {
        "prev": {"title": "PHP, DOMDocument, XPath 1.0, Case-Insensitivity, and Performance", "permalink": "https://ryanparman.com/posts/2018/php-domdocument-xpath-1-0-case-insensitivity-and-performance/"},
        "next": {"title": "Creating Smaller Docker Containers for Your Apps", "permalink": "https://ryanparman.com/posts/2018/creating-smaller-docker-containers-for-your-apps/"},
        "ignore": "me"
    }
}

            
            , {
    "kind": "page",
    "title": "PHP, DOMDocument, XPath 1.0, Case-Insensitivity, and Performance",
    "description": "",
    "summary": {
        "content": "TL;DR: How I improved the performance of case-insensitive XPath queries by 30–35%, reducing an 8× performance hit to only 4.5–5×. This was originally posted to the SimplePie NG blog. Parse-at-all-costs Most feeds are a mess. The old SimplePie “OG” took a parse-at-all-costs philosophy, and could handle many of the most broken feeds you could find — at a cost. While the early versions of SimplePie supported the letter of the RSS 2.0 specification, there were a surprising number of feeds which didn’t.",
        "isTruncated": true
    },
    "published": "2018-02-04T00:26:21Z",
    "updated": "2019-02-10T21:35:12-08:00",
    "permalink": "https://ryanparman.com/posts/2018/php-domdocument-xpath-1-0-case-insensitivity-and-performance/",
    "relativePermalink": "/posts/2018/php-domdocument-xpath-1-0-case-insensitivity-and-performance/",
    "aliases": ["/2018/02/04/php-domdocument-xpath-1-0-case-insensitivity-and-performance"],
    "images": ["https://cdn.ryanparman.com/hugo/posts/2017/hackerman.jpg"],
    "videos": [],
    "categories": ["Projects and Code"],
    "tags": ["simplepie", "simplepie-ng", "php", "xpath", "xslt", "rss", "domdocument", "performance"],
    "series": ["Editors Choice"],
    "keywords": [],
    "meta": {
        "wordCount": 1178,
        "readingTime": "6 minutes",
        "language": "en",
        "isDraft": false,
        "isHome": false,
        "isNode": false,
        "isPage": true,
        "isTranslated": false
    },
    "sourceFile": {
        "path": "posts/2018/20180204-php-domdocument-xpath-1-0-case-insensitivity-and-performance.md",
        "logicalName": "20180204-php-domdocument-xpath-1-0-case-insensitivity-and-performance.md",
        "translationBaseName": "20180204-php-domdocument-xpath-1-0-case-insensitivity-and-performance",
        "baseFileName": "20180204-php-domdocument-xpath-1-0-case-insensitivity-and-performance",
        "ext": "md",
        "lang": "en",
        "dir": "posts/2018/"
    },
    "content": {
        "tableOfContents": "\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#parse-at-all-costs\"\u003eParse-at-all-costs\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#introducing-xpath\"\u003eIntroducing XPath\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#performance-testing\"\u003ePerformance-testing\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#test-machine-and-environment\"\u003eTest machine and environment\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#first-pass-case-insensitive-with-xpath-translate\"\u003eFirst pass; Case-insensitive with XPath translate()\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#second-pass-normal-case-sensitive\"\u003eSecond pass; Normal, case-sensitive\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#experimentation\"\u003eExperimentation\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#xslt\"\u003eXSLT\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#enabling-php-functions-in-xpath\"\u003eEnabling PHP functions in XPath\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#simplify-translate\"\u003eSimplify translate()\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#wrapping-up\"\u003eWrapping-up\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e",
        "html":"\u003cp itemprop=\"description\" class=\"f5 f4-m f3-l mt0 lh-copy p-summary entry-summary\"\u003e\nTL;DR: How I improved the performance of case-insensitive XPath queries by 30–35%, reducing an 8× performance hit to only 4.5–5×.\n\u003c/p\u003e\n\n\n\u003caside class=\"age aside container flex\"\u003e\u003cp\u003eThis was originally posted to the \u003ca href=\"https://medium.com/simplepie-ng/php-domdocument-xpath-1-0-case-insensitivity-and-performance-ad962b98e71c\"\u003eSimplePie NG blog\u003c/a\u003e.\u003c/p\u003e\n\u003c/aside\u003e\n\n\u003cdiv class=\"pa2-ns\"\u003e\n    \u003cpicture\u003e\u003csource type=\"image/webp\" srcset=\"https://cdn.ryanparman.com/hugo/posts/2017/hackerman.webp\" alt=\"Title card for “Hackerman”, a character from the short film “Kung Fury”. http://www.kungfury.com\" class=\"db fullimage\" decoding=\"async\"\u003e\n        \u003cimg src=\"https://cdn.ryanparman.com/hugo/posts/2017/hackerman.jpg\" alt=\"Title card for “Hackerman”, a character from the short film “Kung Fury”. http://www.kungfury.com\" class=\"db fullimage\" decoding=\"async\"\u003e\n    \u003c/picture\u003e\n    \u003cp class=\"f6 gray tc db\"\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"parse-at-all-costs\"\u003eParse-at-all-costs\u003c/h2\u003e\n\u003cp\u003eMost feeds are a mess. The old SimplePie “OG” took a parse-at-all-costs philosophy, and could handle many of the most broken feeds you could find — at a cost. While the early versions of SimplePie supported the letter of the RSS 2.0 specification, there were a surprising number of feeds which didn’t.\u003c/p\u003e\n\u003cp\u003eOnce SimplePie started to get popular (2006–2008), we started getting bug reports from users who were working with RSS feeds containing elements such as \u003ccode\u003e\u0026lt;pubdate\u0026gt;\u003c/code\u003e (instead of \u003ca href=\"https://web.archive.org/web/20180204002621/http://www.rssboard.org/rss-specification\"\u003e\u003ccode\u003e\u0026lt;pubDate\u0026gt;\u003c/code\u003e\u003c/a\u003e) and \u003ccode\u003e\u0026lt;managingeditor\u0026gt;\u003c/code\u003e (instead of \u003ca href=\"https://web.archive.org/web/20180204002621/http://www.rssboard.org/rss-specification\"\u003e\u003ccode\u003e\u0026lt;managingEditor\u0026gt;\u003c/code\u003e\u003c/a\u003e). At first we told users that the feeds were broken — which they were. But then we started getting enough reports that we decided to do something about it.\u003c/p\u003e\n\u003ch2 id=\"introducing-xpath\"\u003eIntroducing XPath\u003c/h2\u003e\n\u003cp\u003eFast-forward to the summer of 2017 when I started work on SimplePie NG in earnest. There are a number of things I’m doing differently (read: better) this time around. The first is that the \u003cem\u003efastest\u003c/em\u003e approach is the \u003cem\u003edefault\u003c/em\u003e approach. A corollary to this principle is that if you want to do more things, you will pay for them with performance penalties.\u003c/p\u003e\n\u003cp\u003eDuring my time working at Amazon Web Services on the SDK for PHP, I discovered some substantial performance gains by moving a lot of the response-parsing code to XPath. As such, the core XML parsing in SimplePie NG is all built around \u003ca href=\"https://web.archive.org/web/20180204002621/https://secure.php.net/domdocument\"\u003eDOMDocument\u003c/a\u003e and \u003ca href=\"https://web.archive.org/web/20180204002621/https://secure.php.net/domxpath\"\u003eXPath\u003c/a\u003e queries.\u003c/p\u003e\n\u003cp\u003eTo solve this case-insensitivity problem, \u003ca href=\"https://web.archive.org/web/20180204002621/https://stackoverflow.com/questions/2893551/case-insensitive-matching-in-xpath\"\u003esearching Stack Overflow for “case insensitive xpath”\u003c/a\u003e tells you about the XPath 2.0 functions \u003ca href=\"https://web.archive.org/web/20180204002621/https://www.w3.org/TR/xpath-functions/#func-matches\"\u003e\u003ccode\u003ematches()\u003c/code\u003e\u003c/a\u003e and \u003ca href=\"https://web.archive.org/web/20180204002621/https://www.w3.org/TR/xpath-functions/#func-lower-case\"\u003e\u003ccode\u003elower-case()\u003c/code\u003e\u003c/a\u003e. However, I was surprised to learn that PHP only supports XPath 1.0. After doing some digging, the reason appears to be that the underlying \u003ca href=\"https://web.archive.org/web/20180204002621/http://xmlsoft.org\"\u003elibxml2\u003c/a\u003e library only supports XPath 1.0, with \u003ca href=\"https://web.archive.org/web/20180204002621/https://mail.gnome.org/archives/xml/2007-February/msg00077.html\"\u003eno updated support on the horizon\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe only alternative that Google and Stack Overflow had for me was the XPath 1.0 function, \u003ca href=\"https://web.archive.org/web/20180204002621/https://www.w3.org/TR/xpath/#function-translate\"\u003e\u003ccode\u003etranslate()\u003c/code\u003e\u003c/a\u003e. In PHP, the case-insensitive query for the \u003ccode\u003e\u0026lt;rss\u0026gt;\u003c/code\u003e element would be:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-php\" data-lang=\"php\"\u003e\u003cspan style=\"color:#f92672\"\u003e/*\u003c/span\u003e[\u003cspan style=\"color:#a6e22e\"\u003etranslate\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e(), \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;rss\u0026#39;\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt’s simple enough to turn this into a pattern inside of a function. Case-insensitive XML parsing. Done. Boo-yah.\u003c/p\u003e\n\u003ch2 id=\"performance-testing\"\u003ePerformance-testing\u003c/h2\u003e\n\u003cp\u003eA little while later, I started some early work on benchmarking SimplePie NG. I parsed a number of normal-sized feeds, and got back a bunch of perfectly reasonable results. But one thing that I wanted to test was memory usage to make sure there were no memory leaks.\u003c/p\u003e\n\u003cp\u003eI put together a quick and dirty test suite by starting with Tim Bray’s feed (one of the more nuanced and complex Atom 1.0 feeds), \u003ca href=\"https://web.archive.org/web/20180204002621/https://raw.githubusercontent.com/simplepie/simplepie-ng/master/tests/Integration/feeds/full/atom10/tim-bray-500.xml\"\u003eduplicating the entries to a total of 500\u003c/a\u003e (increasing the size to around 3 MB), and then \u003ca href=\"https://web.archive.org/web/20180204002621/https://github.com/simplepie/simplepie-ng/blob/master/tests/benchmarks/atom10.php\"\u003ewrote a test that flexed everything\u003c/a\u003e about SimplePie NG that I could think of. I then started running the test over and over again, collecting data about the timing and memory usage, and when the cache kicks-in and the engine warms up.\u003c/p\u003e\n\u003ch3 id=\"test-machine-and-environment\"\u003eTest machine and environment\u003c/h3\u003e\n\u003cp\u003eI’m running this on a \u003ca href=\"https://web.archive.org/web/20180204002621/https://everymac.com/systems/apple/macbook_pro/specs/macbook-pro-core-i7-2.2-17-early-2011-unibody-thunderbolt-specs.html\"\u003e2011 “Core i7” MacBook Pro\u003c/a\u003e, with 16 GB of RAM and an after-market SATA-III SSD. I have various background processes running, so it isn’t the same as running it on a fresh Linux web server. I also have \u003cem\u003eXDebug\u003c/em\u003e enabled, and I’m testing on the CLI where \u003cem\u003eZend OpCache\u003c/em\u003e is disabled.\u003c/p\u003e\n\u003ch3 id=\"first-pass-case-insensitive-with-xpath-translate\"\u003eFirst pass; Case-insensitive with XPath translate()\u003c/h3\u003e\n\u003cp\u003eThe intial results for this 3 MB, 500-entry feed — with case-insensitivity enabled by way of the XPath \u003ccode\u003etranslate()\u003c/code\u003e function — had an average runtime of \u003cstrong\u003e26 seconds\u003c/strong\u003e. That was \u003cem\u003equite\u003c/em\u003e a bit slower than I was hoping for (especially on PHP 7.2), but then again it \u003cem\u003ewas\u003c/em\u003e a big file with \u003cem\u003ea lot\u003c/em\u003e of entries.\u003c/p\u003e\n\u003cp\u003eLet’s compare to case-insensitivity turned off (i.e., case-sensitive XML parsing).\u003c/p\u003e\n\u003ch3 id=\"second-pass-normal-case-sensitive\"\u003eSecond pass; Normal, case-sensitive\u003c/h3\u003e\n\u003cp\u003eThe next round of results on the same 3 MB, 500-entry feed—with standard case-sensitive XPath queries—had an average runtime of \u003cstrong\u003e3.5 seconds\u003c/strong\u003e. That’s \u003cem\u003ea lot\u003c/em\u003e better.\u003c/p\u003e\n\u003cp\u003eTo do some quick math, the normal query took only \u003cstrong\u003e14%\u003c/strong\u003e of the amount of time it took to do a case-insensitive query. Or, put another way, the case-insensitive query took around \u003cstrong\u003e7.5×\u003c/strong\u003e longer than the normal query. \u003cem\u003eThat’s awful!\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"experimentation\"\u003eExperimentation\u003c/h2\u003e\n\u003cp\u003eI had to find a way to improve the performance of the case-insensitive XPath query. Could I reduce the number of times I had to call \u003ccode\u003etranslate()\u003c/code\u003e?\u003c/p\u003e\n\u003ch3 id=\"xslt\"\u003eXSLT\u003c/h3\u003e\n\u003cp\u003eI tried experimenting with XSLT for a few days. The goal was to transform the XML \u003cem\u003eonce\u003c/em\u003e with XSLT into a new XML document where all elements were lowercase, then I could just use regular XPath queries and avoid \u003ccode\u003etranslate()\u003c/code\u003e all-together.\u003c/p\u003e\n\u003cp\u003eOverall, I still think this is a fantastic idea if \u003cem\u003eyou know where your XML data is coming from\u003c/em\u003e. Unfortunately for me, I don’t, and I was completely unable to craft an appropriate XSLT template that would allow me to convert all tag names to lowercase without breaking a bunch of other things (e.g., entities). I ended up having to abandon this path.\u003c/p\u003e\n\u003ch3 id=\"enabling-php-functions-in-xpath\"\u003eEnabling PHP functions in XPath\u003c/h3\u003e\n\u003cp\u003eI only dabbled with this briefly, but there was no discernable performance improvement that I can recall. Also, the PHP documentation is lacking around this feature, so it was a lot of trial and error.\u003c/p\u003e\n\u003ch3 id=\"simplify-translate\"\u003eSimplify translate()\u003c/h3\u003e\n\u003cp\u003eFinally, I wondered if I could reduce the amount of time that \u003ccode\u003etranslate()\u003c/code\u003e takes if I simply gave it less work to do. Instead of giving it the entire alphabet, what if I only gave it the letters that were in the XML element name?\u003c/p\u003e\n\u003cp\u003ePHP has a function \u003ca href=\"https://web.archive.org/web/20180204002621/https://secure.php.net/manual/en/function.count-chars.php\"\u003e\u003ccode\u003ecount_chars()\u003c/code\u003e\u003c/a\u003e that can return the unique characters in a string. From here, we can create upper and lower-case versions of the string, and just use those in the \u003cem\u003etranslate()\u003c/em\u003e function.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-php\" data-lang=\"php\"\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026lt;?\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ephp\u003c/span\u003e\n\n$word           \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;rss\u0026#39;\u003c/span\u003e;\n$elementLetters \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e\\count_chars\u003c/span\u003e($word, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e);\n$lettersLower   \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e\\mb_strtolower\u003c/span\u003e($elementLetters);\n$lettersUpper   \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e\\mb_strtoupper\u003c/span\u003e($elementLetters);\n\n$query \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e\\sprintf\u003c/span\u003e(\n    \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/*[translate(name(), \\\u0026#39;%s\\\u0026#39;, \\\u0026#39;%s\\\u0026#39;) = \\\u0026#39;%s\\\u0026#39;]\u0026#39;\u003c/span\u003e,\n    $lettersUpper,\n    $lettersLower,\n    $word\n);\n\n\u003cspan style=\"color:#75715e\"\u003e# /*[translate(name(), \u0026#39;RS\u0026#39;, \u0026#39;rs\u0026#39;) = \u0026#39;rss\u0026#39;\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e$results \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e $domxpath\u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003equery\u003c/span\u003e($query);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTesting this approach on the same 3 MB, 500-entry feed — with case-insensitivity enabled by way of our smarter \u003ccode\u003etranslate()\u003c/code\u003e function — had an average runtime of \u003cstrong\u003e17 seconds\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eRunning the same benchmarks against my other test feeds consistently showed a \u003cstrong\u003e30–35% improvement\u003c/strong\u003e in performance when using only the required letters in the \u003ccode\u003etranslate()\u003c/code\u003e function instead of the entire alphabet.\u003c/p\u003e\n\u003ch2 id=\"wrapping-up\"\u003eWrapping-up\u003c/h2\u003e\n\u003cp\u003eEven with this technique (on this particular set of data, with this particular testing approach), case-insensitive queries are still \u003cstrong\u003e4.5–5×\u003c/strong\u003e slower than their case-sensitive counterparts. Using the \u003ccode\u003etranslate()\u003c/code\u003e XPath 1.0 function in PHP has a substantial impact on performance, so \u003cem\u003edon’t use it if you don’t have to\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eI still think that there is some viability in leveraging XSLT in a first pass, which I expect would substantially reduce the case-insensitive processing time, but someone with more XSLT experience than me would need to contribute that code.\u003c/p\u003e\n\u003cp\u003eLastly, SimplePie NG performs the faster case-sensitive queries by default. You are able to opt-in to case-insensitive mode on a per-feed basis. If you’re just processing a few average-sized feeds with this mode enabled, you probably won’t notice much of an impact.\u003c/p\u003e\n",
        "plain":"TL;DR: How I improved the performance of case-insensitive XPath queries by 30–35%, reducing an 8× performance hit to only 4.5–5×. This was originally posted to the SimplePie NG blog.\n   Parse-at-all-costs Most feeds are a mess. The old SimplePie “OG” took a parse-at-all-costs philosophy, and could handle many of the most broken feeds you could find — at a cost. While the early versions of SimplePie supported the letter of the RSS 2.0 specification, there were a surprising number of feeds which didn’t.\nOnce SimplePie started to get popular (2006–2008), we started getting bug reports from users who were working with RSS feeds containing elements such as \u0026lt;pubdate\u0026gt; (instead of \u0026lt;pubDate\u0026gt;) and \u0026lt;managingeditor\u0026gt; (instead of \u0026lt;managingEditor\u0026gt;). At first we told users that the feeds were broken — which they were. But then we started getting enough reports that we decided to do something about it.\nIntroducing XPath Fast-forward to the summer of 2017 when I started work on SimplePie NG in earnest. There are a number of things I’m doing differently (read: better) this time around. The first is that the fastest approach is the default approach. A corollary to this principle is that if you want to do more things, you will pay for them with performance penalties.\nDuring my time working at Amazon Web Services on the SDK for PHP, I discovered some substantial performance gains by moving a lot of the response-parsing code to XPath. As such, the core XML parsing in SimplePie NG is all built around DOMDocument and XPath queries.\nTo solve this case-insensitivity problem, searching Stack Overflow for “case insensitive xpath” tells you about the XPath 2.0 functions matches() and lower-case(). However, I was surprised to learn that PHP only supports XPath 1.0. After doing some digging, the reason appears to be that the underlying libxml2 library only supports XPath 1.0, with no updated support on the horizon.\nThe only alternative that Google and Stack Overflow had for me was the XPath 1.0 function, translate(). In PHP, the case-insensitive query for the \u0026lt;rss\u0026gt; element would be:\n/*[translate(name(), \u0026#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#39;, \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;) = \u0026#39;rss\u0026#39;] It’s simple enough to turn this into a pattern inside of a function. Case-insensitive XML parsing. Done. Boo-yah.\nPerformance-testing A little while later, I started some early work on benchmarking SimplePie NG. I parsed a number of normal-sized feeds, and got back a bunch of perfectly reasonable results. But one thing that I wanted to test was memory usage to make sure there were no memory leaks.\nI put together a quick and dirty test suite by starting with Tim Bray’s feed (one of the more nuanced and complex Atom 1.0 feeds), duplicating the entries to a total of 500 (increasing the size to around 3 MB), and then wrote a test that flexed everything about SimplePie NG that I could think of. I then started running the test over and over again, collecting data about the timing and memory usage, and when the cache kicks-in and the engine warms up.\nTest machine and environment I’m running this on a 2011 “Core i7” MacBook Pro, with 16 GB of RAM and an after-market SATA-III SSD. I have various background processes running, so it isn’t the same as running it on a fresh Linux web server. I also have XDebug enabled, and I’m testing on the CLI where Zend OpCache is disabled.\nFirst pass; Case-insensitive with XPath translate() The intial results for this 3 MB, 500-entry feed — with case-insensitivity enabled by way of the XPath translate() function — had an average runtime of 26 seconds. That was quite a bit slower than I was hoping for (especially on PHP 7.2), but then again it was a big file with a lot of entries.\nLet’s compare to case-insensitivity turned off (i.e., case-sensitive XML parsing).\nSecond pass; Normal, case-sensitive The next round of results on the same 3 MB, 500-entry feed—with standard case-sensitive XPath queries—had an average runtime of 3.5 seconds. That’s a lot better.\nTo do some quick math, the normal query took only 14% of the amount of time it took to do a case-insensitive query. Or, put another way, the case-insensitive query took around 7.5× longer than the normal query. That’s awful!\nExperimentation I had to find a way to improve the performance of the case-insensitive XPath query. Could I reduce the number of times I had to call translate()?\nXSLT I tried experimenting with XSLT for a few days. The goal was to transform the XML once with XSLT into a new XML document where all elements were lowercase, then I could just use regular XPath queries and avoid translate() all-together.\nOverall, I still think this is a fantastic idea if you know where your XML data is coming from. Unfortunately for me, I don’t, and I was completely unable to craft an appropriate XSLT template that would allow me to convert all tag names to lowercase without breaking a bunch of other things (e.g., entities). I ended up having to abandon this path.\nEnabling PHP functions in XPath I only dabbled with this briefly, but there was no discernable performance improvement that I can recall. Also, the PHP documentation is lacking around this feature, so it was a lot of trial and error.\nSimplify translate() Finally, I wondered if I could reduce the amount of time that translate() takes if I simply gave it less work to do. Instead of giving it the entire alphabet, what if I only gave it the letters that were in the XML element name?\nPHP has a function count_chars() that can return the unique characters in a string. From here, we can create upper and lower-case versions of the string, and just use those in the translate() function.\n\u0026lt;?php $word = \u0026#39;rss\u0026#39;; $elementLetters = \\count_chars($word, 3); $lettersLower = \\mb_strtolower($elementLetters); $lettersUpper = \\mb_strtoupper($elementLetters); $query = \\sprintf( \u0026#39;/*[translate(name(), \\\u0026#39;%s\\\u0026#39;, \\\u0026#39;%s\\\u0026#39;) = \\\u0026#39;%s\\\u0026#39;]\u0026#39;, $lettersUpper, $lettersLower, $word ); # /*[translate(name(), \u0026#39;RS\u0026#39;, \u0026#39;rs\u0026#39;) = \u0026#39;rss\u0026#39; $results = $domxpath-\u0026gt;query($query); Testing this approach on the same 3 MB, 500-entry feed — with case-insensitivity enabled by way of our smarter translate() function — had an average runtime of 17 seconds.\nRunning the same benchmarks against my other test feeds consistently showed a 30–35% improvement in performance when using only the required letters in the translate() function instead of the entire alphabet.\nWrapping-up Even with this technique (on this particular set of data, with this particular testing approach), case-insensitive queries are still 4.5–5× slower than their case-sensitive counterparts. Using the translate() XPath 1.0 function in PHP has a substantial impact on performance, so don’t use it if you don’t have to.\nI still think that there is some viability in leveraging XSLT in a first pass, which I expect would substantially reduce the case-insensitive processing time, but someone with more XSLT experience than me would need to contribute that code.\nLastly, SimplePie NG performs the faster case-sensitive queries by default. You are able to opt-in to case-insensitive mode on a per-feed basis. If you’re just processing a few average-sized feeds with this mode enabled, you probably won’t notice much of an impact.\n",
        "source":"\n{{\u003cdescription\u003e}}\nTL;DR: How I improved the performance of case-insensitive XPath queries by 30–35%, reducing an 8× performance hit to only 4.5–5×.\n{{\u003c/description\u003e}}\n\n{{% aside %}}\nThis was originally posted to the \u003ca href=\"https://medium.com/simplepie-ng/php-domdocument-xpath-1-0-case-insensitivity-and-performance-ad962b98e71c\"\u003eSimplePie NG blog\u003c/a\u003e.\n{{% /aside %}}\n\n{{\u003cfullimage src=\"https://cdn.ryanparman.com/hugo/posts/2017/hackerman.jpg\" alt=\"Title card for “Hackerman”, a character from the short film “Kung Fury”. http://www.kungfury.com\" \u003e}}\n\n## Parse-at-all-costs\n\nMost feeds are a mess. The old SimplePie “OG” took a parse-at-all-costs philosophy, and could handle many of the most broken feeds you could find — at a cost. While the early versions of SimplePie supported the letter of the RSS 2.0 specification, there were a surprising number of feeds which didn’t.\n\nOnce SimplePie started to get popular (2006–2008), we started getting bug reports from users who were working with RSS feeds containing elements such as `\u003cpubdate\u003e` (instead of [`\u003cpubDate\u003e`]({{\u003c wayback \"http://www.rssboard.org/rss-specification\" \u003e}})) and `\u003cmanagingeditor\u003e` (instead of [`\u003cmanagingEditor\u003e`]({{\u003c wayback \"http://www.rssboard.org/rss-specification\" \u003e}})). At first we told users that the feeds were broken — which they were. But then we started getting enough reports that we decided to do something about it.\n\n## Introducing XPath\n\nFast-forward to the summer of 2017 when I started work on SimplePie NG in earnest. There are a number of things I’m doing differently (read: better) this time around. The first is that the _fastest_ approach is the _default_ approach. A corollary to this principle is that if you want to do more things, you will pay for them with performance penalties.\n\nDuring my time working at Amazon Web Services on the SDK for PHP, I discovered some substantial performance gains by moving a lot of the response-parsing code to XPath. As such, the core XML parsing in SimplePie NG is all built around [DOMDocument]({{\u003c wayback \"https://secure.php.net/domdocument\" \u003e}}) and [XPath]({{\u003c wayback \"https://secure.php.net/domxpath\" \u003e}}) queries.\n\nTo solve this case-insensitivity problem, [searching Stack Overflow for “case insensitive xpath”]({{\u003c wayback \"https://stackoverflow.com/questions/2893551/case-insensitive-matching-in-xpath\" \u003e}}) tells you about the XPath 2.0 functions [`matches()`]({{\u003c wayback \"https://www.w3.org/TR/xpath-functions/#func-matches\" \u003e}}) and [`lower-case()`]({{\u003c wayback \"https://www.w3.org/TR/xpath-functions/#func-lower-case\" \u003e}}). However, I was surprised to learn that PHP only supports XPath 1.0. After doing some digging, the reason appears to be that the underlying [libxml2]({{\u003c wayback \"http://xmlsoft.org\" \u003e}}) library only supports XPath 1.0, with [no updated support on the horizon]({{\u003c wayback \"https://mail.gnome.org/archives/xml/2007-February/msg00077.html\" \u003e}}).\n\nThe only alternative that Google and Stack Overflow had for me was the XPath 1.0 function, [`translate()`]({{\u003c wayback \"https://www.w3.org/TR/xpath/#function-translate\" \u003e}}). In PHP, the case-insensitive query for the `\u003crss\u003e` element would be:\n\n```php\n/*[translate(name(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz') = 'rss']\n```\n\nIt’s simple enough to turn this into a pattern inside of a function. Case-insensitive XML parsing. Done. Boo-yah.\n\n## Performance-testing\n\nA little while later, I started some early work on benchmarking SimplePie NG. I parsed a number of normal-sized feeds, and got back a bunch of perfectly reasonable results. But one thing that I wanted to test was memory usage to make sure there were no memory leaks.\n\nI put together a quick and dirty test suite by starting with Tim Bray’s feed (one of the more nuanced and complex Atom 1.0 feeds), [duplicating the entries to a total of 500]({{\u003c wayback \"https://raw.githubusercontent.com/simplepie/simplepie-ng/master/tests/Integration/feeds/full/atom10/tim-bray-500.xml\" \u003e}}) (increasing the size to around 3 MB), and then [wrote a test that flexed everything]({{\u003c wayback \"https://github.com/simplepie/simplepie-ng/blob/master/tests/benchmarks/atom10.php\" \u003e}}) about SimplePie NG that I could think of. I then started running the test over and over again, collecting data about the timing and memory usage, and when the cache kicks-in and the engine warms up.\n\n### Test machine and environment\n\nI’m running this on a [2011 “Core i7” MacBook Pro]({{\u003c wayback \"https://everymac.com/systems/apple/macbook_pro/specs/macbook-pro-core-i7-2.2-17-early-2011-unibody-thunderbolt-specs.html\" \u003e}}), with 16 GB of RAM and an after-market SATA-III SSD. I have various background processes running, so it isn’t the same as running it on a fresh Linux web server. I also have _XDebug_ enabled, and I’m testing on the CLI where _Zend OpCache_ is disabled.\n\n### First pass; Case-insensitive with XPath translate()\n\nThe intial results for this 3 MB, 500-entry feed — with case-insensitivity enabled by way of the XPath `translate()` function — had an average runtime of **26 seconds**. That was _quite_ a bit slower than I was hoping for (especially on PHP 7.2), but then again it _was_ a big file with _a lot_ of entries.\n\nLet’s compare to case-insensitivity turned off (i.e., case-sensitive XML parsing).\n\n### Second pass; Normal, case-sensitive\n\nThe next round of results on the same 3 MB, 500-entry feed—with standard case-sensitive XPath queries—had an average runtime of **3.5 seconds**. That’s _a lot_ better.\n\nTo do some quick math, the normal query took only **14%** of the amount of time it took to do a case-insensitive query. Or, put another way, the case-insensitive query took around **7.5×** longer than the normal query. _That’s awful!_\n\n## Experimentation\n\nI had to find a way to improve the performance of the case-insensitive XPath query. Could I reduce the number of times I had to call `translate()`?\n\n### XSLT\n\nI tried experimenting with XSLT for a few days. The goal was to transform the XML _once_ with XSLT into a new XML document where all elements were lowercase, then I could just use regular XPath queries and avoid `translate()` all-together.\n\nOverall, I still think this is a fantastic idea if _you know where your XML data is coming from_. Unfortunately for me, I don’t, and I was completely unable to craft an appropriate XSLT template that would allow me to convert all tag names to lowercase without breaking a bunch of other things (e.g., entities). I ended up having to abandon this path.\n\n### Enabling PHP functions in XPath\n\nI only dabbled with this briefly, but there was no discernable performance improvement that I can recall. Also, the PHP documentation is lacking around this feature, so it was a lot of trial and error.\n\n### Simplify translate()\n\nFinally, I wondered if I could reduce the amount of time that `translate()` takes if I simply gave it less work to do. Instead of giving it the entire alphabet, what if I only gave it the letters that were in the XML element name?\n\nPHP has a function [`count_chars()`]({{\u003c wayback \"https://secure.php.net/manual/en/function.count-chars.php\" \u003e}}) that can return the unique characters in a string. From here, we can create upper and lower-case versions of the string, and just use those in the _translate()_ function.\n\n```php\n\u003c?php\n\n$word           = 'rss';\n$elementLetters = \\count_chars($word, 3);\n$lettersLower   = \\mb_strtolower($elementLetters);\n$lettersUpper   = \\mb_strtoupper($elementLetters);\n\n$query = \\sprintf(\n    '/*[translate(name(), \\'%s\\', \\'%s\\') = \\'%s\\']',\n    $lettersUpper,\n    $lettersLower,\n    $word\n);\n\n# /*[translate(name(), 'RS', 'rs') = 'rss'\n$results = $domxpath-\u003equery($query);\n```\n\nTesting this approach on the same 3 MB, 500-entry feed — with case-insensitivity enabled by way of our smarter `translate()` function — had an average runtime of **17 seconds**.\n\nRunning the same benchmarks against my other test feeds consistently showed a **30–35% improvement** in performance when using only the required letters in the `translate()` function instead of the entire alphabet.\n\n## Wrapping-up\n\nEven with this technique (on this particular set of data, with this particular testing approach), case-insensitive queries are still **4.5–5×** slower than their case-sensitive counterparts. Using the `translate()` XPath 1.0 function in PHP has a substantial impact on performance, so _don’t use it if you don’t have to_.\n\nI still think that there is some viability in leveraging XSLT in a first pass, which I expect would substantially reduce the case-insensitive processing time, but someone with more XSLT experience than me would need to contribute that code.\n\nLastly, SimplePie NG performs the faster case-sensitive queries by default. You are able to opt-in to case-insensitive mode on a per-feed basis. If you’re just processing a few average-sized feeds with this mode enabled, you probably won’t notice much of an impact.\n"},
    "links": {
        "prev": {"title": "A Refresh of the SimplePie Logo", "permalink": "https://ryanparman.com/posts/2017/a-refresh-of-the-simplepie-logo/"},
        "next": {"title": "Dear Nintendo, Part II", "permalink": "https://ryanparman.com/posts/2018/dear-nintendo-part-ii/"},
        "ignore": "me"
    }
}

            
        ]
    }
}
